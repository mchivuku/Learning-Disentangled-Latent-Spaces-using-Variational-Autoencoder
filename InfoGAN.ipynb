{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "InfoGAN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mchivuku/csb659-project/blob/master/InfoGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "ug3UJoX7mVYU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# InfoGAN\n",
        "\n",
        "Notebook contains implementation for InfoGAN - https://arxiv.org/abs/1606.03657\n",
        "\n",
        "Code adapted from: https://github.com/eriklindernoren/Pytorch-GAN/"
      ]
    },
    {
      "metadata": {
        "id": "isb8LS8-S1bR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Connect to google drive"
      ]
    },
    {
      "metadata": {
        "id": "527spdi5mjJL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install tqdm six"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fc6nEA_HmmCs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "fd9024ae-13ed-4b7a-b050-f8c7c8ff298e"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\")\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5RvxHBPamqk8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "269c3885-a666-4f18-fff5-fad63f1fd3cb"
      },
      "cell_type": "code",
      "source": [
        "%cd /content/drive/My\\ Drive/Masters-DS/CSCI-B659/project/examples/infoGAN\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Masters-DS/CSCI-B659/project/examples/infoGAN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2kaI4PuUm2r8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.makedirs(\"infoGAN\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bi77NZUim9SM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9afa8229-3829-48e0-fd56-fae843004335"
      },
      "cell_type": "code",
      "source": [
        "%cd infoGAN"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Masters-DS/CSCI-B659/project/examples/infoGAN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "QP6gJFhKm_e9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "os.makedirs(\"./results/static\", exist_ok=True)\n",
        "os.makedirs('./results/varying_c1/', exist_ok=True)\n",
        "os.makedirs('./results/varying_c2/', exist_ok=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LBaedON0nSw2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Torch Imports"
      ]
    },
    {
      "metadata": {
        "id": "beEwNN_fnGfM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.utils.data\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "import itertools\n",
        "from torch.autograd import Variable\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Pb9AxT4vnU6d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "7e2e0acd-88ea-49a8-e3f3-760875c6222f"
      },
      "cell_type": "code",
      "source": [
        "torch.manual_seed(5)\n",
        "\n",
        "print('Torch', torch.__version__, 'CUDA', torch.version.cuda)\n",
        "print('Device:', torch.device('cuda:0'))\n",
        "print(torch.cuda.is_available())\n",
        "\n",
        "is_cuda = torch.cuda.is_available()\n",
        "device = torch.device ( \"cuda:0\" if torch.cuda.is_available () else \"cpu\" )"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Torch 1.0.1.post2 CUDA 10.0.130\n",
            "Device: cuda:0\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KE9GmEw0o-24",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fd6c36d4-7a00-434c-b2f8-fae3daeae973"
      },
      "cell_type": "code",
      "source": [
        "class Params:\n",
        "    n_epochs = 200\n",
        "    batch_size = 128\n",
        "    lr = 0.0002\n",
        "    b1 = 0.5 #adam, decay of first order momentum of gradient\n",
        "    b2 = 0.999 #decay of first order momentum of gradient\n",
        "    n_cpu = 8\n",
        "    latent_dim = 62\n",
        "    code_dim = 2\n",
        "    n_classes = 10\n",
        "    img_size = 32\n",
        "    channels = 1\n",
        "    sample_interval = 400 # interval  between image sampling\n",
        "    \n",
        "    \n",
        "    \n",
        "Params.n_epochs"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "200"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "wN5i7fGgnZBN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## function to initialize weights\n",
        "def weights_init_normal(m):\n",
        "  classname= m.__class__.__name__\n",
        "  if classname.find(\"Conv\")!=-1:\n",
        "    torch.nn.init.normal_(m.weight.data,0.0,0.02)\n",
        "  elif classname.find(\"BatchNorm\")!=-1:\n",
        "    torch.nn.init.normal_(m.weight.data,1.0,0.02)\n",
        "    torch.nn.init.constant_(m.bias.data,0.0)\n",
        "    \n",
        "    \n",
        "\"\"\"\n",
        "One hot encoded vector for category\n",
        "\"\"\"\n",
        "def to_categorical(y, num_cols):\n",
        "  \"\"\"\n",
        "  \"\"\"\n",
        "  y_cat = np.zeros((y.shape[0],num_cols))\n",
        "  y_cat[range(y.shape[0]),y] =1.\n",
        "  \n",
        "  return Variable(FloatTensor(y_cat))\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lt5Y7HO3oeI-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Models\n",
        "class Generator(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Generator,self).__init__()\n",
        "    input_dim = Params.latent_dim + Params.n_classes + Params.code_dim\n",
        "    \n",
        "    self.init_size = Params.img_size // 4 #initial size before upsampling\n",
        "    self.l1 = nn.Sequential(nn.Linear(input_dim, 128*self.init_size **2))\n",
        "    \n",
        "    self.conv_blocks = nn.Sequential(\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.Upsample(scale_factor=2),\n",
        "            nn.Conv2d(128, 128, 3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(128, 0.8),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Upsample(scale_factor=2),\n",
        "            nn.Conv2d(128, 64, 3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(64, 0.8),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(64, Params.channels, 3, stride=1, padding=1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "    \n",
        "  def forward(self,noise, labels, code):\n",
        "    gen_input = torch.cat((noise, labels, code),-1) ## include to contains noise and code\n",
        "    out = self.l1(gen_input)\n",
        "    \n",
        "    out = out.view(out.shape[0],128,self.init_size, self.init_size)\n",
        "    img = self.conv_blocks(out)\n",
        "    return img\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tbRN8j-1qeJx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Discriminator(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Discriminator,self).__init__()\n",
        "    \n",
        "    def discriminator_block(in_filters, out_filters, bn=True):\n",
        "      \"\"\"Returns layers of each discriminator block\"\"\"\n",
        "      block = [   nn.Conv2d(in_filters, out_filters, 3, 2, 1),\n",
        "                nn.LeakyReLU(0.2, inplace=True),\n",
        "                nn.Dropout2d(0.25)]\n",
        "      if bn:\n",
        "        block.append(nn.BatchNorm2d(out_filters, 0.8))\n",
        "      return block\n",
        "          \n",
        "    self.conv_blocks = nn.Sequential(*discriminator_block(Params.channels,16, bn=False),\n",
        "                                    *discriminator_block(16,32),\n",
        "                                     *discriminator_block(32,64),\n",
        "                                     *discriminator_block(64,128)\n",
        "                                        \n",
        "                                    )    \n",
        "    \n",
        "    ## height and width of downsampled image\n",
        "    ds_size = Params.img_size // 2**4\n",
        "    self.adv_layer = nn.Sequential(nn.Linear(128*ds_size**2, 1))\n",
        "    self.aux_layer = nn.Sequential(\n",
        "            nn.Linear(128*ds_size**2, Params.n_classes),\n",
        "            nn.Softmax()\n",
        "        )\n",
        "    \n",
        "    self.latent_layer = nn.Sequential(nn.Linear(128*ds_size**2, Params.code_dim))\n",
        "    \n",
        "  def forward(self, img):\n",
        "      out = self.conv_blocks(img)\n",
        "      out = out.view(out.shape[0], -1)\n",
        "      validity = self.adv_layer(out)\n",
        "      label = self.aux_layer(out)\n",
        "      latent_code = self.latent_layer(out)\n",
        "\n",
        "      return validity, label, latent_code\n",
        "      \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iQA4Ar2Fr7Ll",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        },
        "outputId": "4532da99-9967-4877-9456-cea3b1076621"
      },
      "cell_type": "code",
      "source": [
        "## Loss Functions\n",
        "\n",
        "adversarial_loss = torch.nn.MSELoss()\n",
        "categorical_loss = torch.nn.CrossEntropyLoss() # cross entropy loss - discrete loss\n",
        "continuous_loss = torch.nn.MSELoss()\n",
        "\n",
        "\n",
        "## Loss weights\n",
        "lambda_cat = 1\n",
        "lambda_con = 0.1\n",
        "\n",
        "## Initialize generator and discriminator netowrk\n",
        "generator  = Generator()\n",
        "discriminator = Discriminator()\n",
        "\n",
        "\n",
        "generator = generator.cuda()\n",
        "discriminator = discriminator.cuda()\n",
        "\n",
        "print(generator)\n",
        "print()\n",
        "print(discriminator)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Generator(\n",
            "  (l1): Sequential(\n",
            "    (0): Linear(in_features=74, out_features=8192, bias=True)\n",
            "  )\n",
            "  (conv_blocks): Sequential(\n",
            "    (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (1): Upsample(scale_factor=2, mode=nearest)\n",
            "    (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): BatchNorm2d(128, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (4): LeakyReLU(negative_slope=0.2, inplace)\n",
            "    (5): Upsample(scale_factor=2, mode=nearest)\n",
            "    (6): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (7): BatchNorm2d(64, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (8): LeakyReLU(negative_slope=0.2, inplace)\n",
            "    (9): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (10): Tanh()\n",
            "  )\n",
            ")\n",
            "\n",
            "Discriminator(\n",
            "  (conv_blocks): Sequential(\n",
            "    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (1): LeakyReLU(negative_slope=0.2, inplace)\n",
            "    (2): Dropout2d(p=0.25)\n",
            "    (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (4): LeakyReLU(negative_slope=0.2, inplace)\n",
            "    (5): Dropout2d(p=0.25)\n",
            "    (6): BatchNorm2d(32, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (7): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (8): LeakyReLU(negative_slope=0.2, inplace)\n",
            "    (9): Dropout2d(p=0.25)\n",
            "    (10): BatchNorm2d(64, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (11): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "    (12): LeakyReLU(negative_slope=0.2, inplace)\n",
            "    (13): Dropout2d(p=0.25)\n",
            "    (14): BatchNorm2d(128, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  )\n",
            "  (adv_layer): Sequential(\n",
            "    (0): Linear(in_features=512, out_features=1, bias=True)\n",
            "  )\n",
            "  (aux_layer): Sequential(\n",
            "    (0): Linear(in_features=512, out_features=10, bias=True)\n",
            "    (1): Softmax()\n",
            "  )\n",
            "  (latent_layer): Sequential(\n",
            "    (0): Linear(in_features=512, out_features=2, bias=True)\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "FWnBsdkksx02",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        },
        "outputId": "25a6c2a7-82c9-4706-984d-56a73917761b"
      },
      "cell_type": "code",
      "source": [
        "## Initialize weights\n",
        "# Initialize weights\n",
        "generator.apply(weights_init_normal)\n",
        "discriminator.apply(weights_init_normal)\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Discriminator(\n",
              "  (conv_blocks): Sequential(\n",
              "    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "    (1): LeakyReLU(negative_slope=0.2, inplace)\n",
              "    (2): Dropout2d(p=0.25)\n",
              "    (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "    (4): LeakyReLU(negative_slope=0.2, inplace)\n",
              "    (5): Dropout2d(p=0.25)\n",
              "    (6): BatchNorm2d(32, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (7): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "    (8): LeakyReLU(negative_slope=0.2, inplace)\n",
              "    (9): Dropout2d(p=0.25)\n",
              "    (10): BatchNorm2d(64, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (11): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
              "    (12): LeakyReLU(negative_slope=0.2, inplace)\n",
              "    (13): Dropout2d(p=0.25)\n",
              "    (14): BatchNorm2d(128, eps=0.8, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  )\n",
              "  (adv_layer): Sequential(\n",
              "    (0): Linear(in_features=512, out_features=1, bias=True)\n",
              "  )\n",
              "  (aux_layer): Sequential(\n",
              "    (0): Linear(in_features=512, out_features=10, bias=True)\n",
              "    (1): Softmax()\n",
              "  )\n",
              "  (latent_layer): Sequential(\n",
              "    (0): Linear(in_features=512, out_features=2, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "metadata": {
        "id": "gqF9lDWztOlG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Data\n",
        "dataloader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('../vae/mnist/data', train=True, download=True,\n",
        "                   transform=transforms.Compose([\n",
        "                        transforms.Resize(Params.img_size),\n",
        "                        transforms.ToTensor(),\n",
        "                        transforms.Normalize((0.5, ), (0.5,))\n",
        "                   ])),\n",
        "    batch_size=Params.batch_size, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jqry-cH_tpKX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Define Optimizers\n",
        "# Optimizers\n",
        "optimizer_G = torch.optim.Adam(generator.parameters(), lr=Params.lr, betas=(Params.b1, Params.b2))\n",
        "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=Params.lr, betas=(Params.b1, Params.b2))\n",
        "optimizer_info = torch.optim.Adam(itertools.chain(generator.parameters(), discriminator.parameters()),\n",
        "                                    lr=Params.lr, betas=(Params.b1, Params.b2))\n",
        "\n",
        "FloatTensor = torch.cuda.FloatTensor  \n",
        "LongTensor = torch.cuda.LongTensor  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Nu6k9nDWt5GX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Static generator inputs for sampling\n",
        "# Static generator inputs for sampling\n",
        "import numpy as np\n",
        "\n",
        "static_z = Variable(FloatTensor(np.zeros((Params.n_classes**2, Params.latent_dim))))\n",
        "static_label = to_categorical(np.array([num for _ in range(Params.n_classes) for num in range(Params.n_classes)]),\n",
        "                                 Params.n_classes)\n",
        "static_code = Variable(FloatTensor(np.zeros((Params.n_classes**2, Params.code_dim))))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "g54BVXUKuW2I",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def sample_image(n_row, batches_done):\n",
        "    \"\"\"Saves a grid of generated digits ranging from 0 to n_classes\"\"\"\n",
        "    # Static sample\n",
        "    z = Variable(FloatTensor(np.random.normal(0, 1, (n_row**2, Params.latent_dim))))\n",
        "    static_sample = generator(z, static_label, static_code)\n",
        "    save_image(static_sample.data, './results/static/%d.png' % batches_done, nrow=n_row, normalize=True)\n",
        "\n",
        "    # Get varied c1 and c2\n",
        "    zeros = np.zeros((n_row**2, 1))\n",
        "    c_varied = np.repeat(np.linspace(-1, 1, n_row)[:, np.newaxis], n_row, 0)\n",
        "    c1 = Variable(FloatTensor(np.concatenate((c_varied, zeros), -1)))\n",
        "    c2 = Variable(FloatTensor(np.concatenate((zeros, c_varied), -1)))\n",
        "    sample1 = generator(static_z, static_label, c1)\n",
        "    sample2 = generator(static_z, static_label, c2)\n",
        "    save_image(sample1.data, './results/varying_c1/%d.png' % batches_done, nrow=n_row, normalize=True)\n",
        "    save_image(sample2.data, './results/varying_c2/%d.png' % batches_done, nrow=n_row, normalize=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FmFAf5GJvH6c",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training"
      ]
    },
    {
      "metadata": {
        "id": "G60CT3uyux_p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5086
        },
        "outputId": "6befd0c8-13ac-4fcb-ff88-53de2734309a"
      },
      "cell_type": "code",
      "source": [
        "for epoch in range(Params.n_epochs):\n",
        "  for i, (images,labels) in enumerate(dataloader):\n",
        "    batch_size = images.size(0)\n",
        "    \n",
        "    ## real and fake ground truth\n",
        "    real = Variable(FloatTensor(batch_size,1).fill_(1.0),requires_grad=False)\n",
        "    fake = Variable(FloatTensor(batch_size,1).fill_(0.0),requires_grad=False)\n",
        "    \n",
        "    ## configure input\n",
        "    real_imgs = Variable(images.type(FloatTensor))\n",
        "    labels = Variable(labels.type(FloatTensor))\n",
        "    \n",
        "    ### Train Generator\n",
        "    optimizer_G.zero_grad()\n",
        "    \n",
        "    ## sample noise and labels as generator input\n",
        "    z = Variable(FloatTensor(np.random.normal(0,1,(batch_size,Params.latent_dim))))\n",
        "    label_input = to_categorical(np.random.randint(0,Params.n_classes, batch_size),Params.n_classes)\n",
        "    \n",
        "    code_input = Variable(FloatTensor(np.random.uniform(-1,1,(batch_size,Params.code_dim))))\n",
        "    \n",
        "    # Generate a batch of images - labels, code and z\n",
        "    gen_imgs = generator(z, label_input, code_input)\n",
        "    \n",
        "    \n",
        "    # discriminator\n",
        "    validity, _, _ = discriminator(gen_imgs)\n",
        "    \n",
        "    g_loss = adversarial_loss(validity,real)\n",
        "    \n",
        "    g_loss.backward()\n",
        "    optimizer_G.step()\n",
        "    \n",
        "    \n",
        "    ##\n",
        "    # Train Discriminator\n",
        "    \n",
        "    optimizer_D.zero_grad()\n",
        "    \n",
        "    ## Loss on real images\n",
        "    real_pred, _, _ = discriminator(real_imgs)\n",
        "    d_real_loss = adversarial_loss(real_pred, real)\n",
        "    \n",
        "    ## Loss on fake images\n",
        "    fake_pred, _, _ = discriminator(gen_imgs.detach())\n",
        "    d_fake_loss = adversarial_loss(fake_pred, fake)\n",
        "    \n",
        "    ## Total discriminator loss\n",
        "    d_loss = (d_real_loss + d_fake_loss)/2\n",
        "    \n",
        "    d_loss.backward()\n",
        "    optimizer_D.step()\n",
        "    \n",
        "    \n",
        "    \n",
        "    ### MutualInformation Loss\n",
        "    ####\n",
        "    \n",
        "    optimizer_info.zero_grad()\n",
        "    \n",
        "    ## sample labels\n",
        "    sampled_labels = np.random.randint(0,Params.n_classes, batch_size)\n",
        "    \n",
        "    # ground truth labels\n",
        "    gt_labels = Variable(LongTensor(sampled_labels),requires_grad = False)\n",
        "    \n",
        "    \n",
        "    # Sample noise, labels and code as generator input\n",
        "    z = Variable(FloatTensor(np.random.normal(0, 1, (batch_size, Params.latent_dim))))\n",
        "    label_input = to_categorical(sampled_labels, Params.n_classes)\n",
        "    code_input = Variable(FloatTensor(np.random.normal(-1, 1, (batch_size, Params.code_dim))))\n",
        "\n",
        "    gen_imgs = generator(z, label_input, code_input)\n",
        "    _, pred_label, pred_code = discriminator(gen_imgs)\n",
        "    \n",
        "    info_loss = lambda_cat * categorical_loss(pred_label, gt_labels) + \\\n",
        "                lambda_con * continuous_loss(pred_code, code_input)\n",
        "    \n",
        "    info_loss.backward()\n",
        "    \n",
        "    optimizer_info.step()\n",
        "    \n",
        "    #--------------\n",
        "    # Log Progress\n",
        "    #--------------\n",
        "\n",
        "    print (\"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f] [info loss: %f]\" % (epoch, Params.n_epochs, i, len(dataloader),\n",
        "                                                            d_loss.item(), g_loss.item(), info_loss.item()))\n",
        "    \n",
        "    \n",
        "    batches_done = epoch * len(dataloader) + i\n",
        "    if batches_done % Params.sample_interval == 0:\n",
        "            sample_image(n_row=10, batches_done=batches_done)\n",
        "\n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/upsampling.py:129: UserWarning: nn.Upsample is deprecated. Use nn.functional.interpolate instead.\n",
            "  warnings.warn(\"nn.{} is deprecated. Use nn.functional.interpolate instead.\".format(self.name))\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch 0/200] [Batch 0/469] [D loss: 0.488959] [G loss: 0.978922] [info loss: 2.485431]\n",
            "[Epoch 0/200] [Batch 1/469] [D loss: 0.486341] [G loss: 0.974166] [info loss: 2.495313]\n",
            "[Epoch 0/200] [Batch 2/469] [D loss: 0.483580] [G loss: 0.968782] [info loss: 2.466264]\n",
            "[Epoch 0/200] [Batch 3/469] [D loss: 0.479737] [G loss: 0.962913] [info loss: 2.481699]\n",
            "[Epoch 0/200] [Batch 4/469] [D loss: 0.475852] [G loss: 0.956066] [info loss: 2.484247]\n",
            "[Epoch 0/200] [Batch 5/469] [D loss: 0.470917] [G loss: 0.946706] [info loss: 2.498102]\n",
            "[Epoch 0/200] [Batch 6/469] [D loss: 0.464695] [G loss: 0.936057] [info loss: 2.487242]\n",
            "[Epoch 0/200] [Batch 7/469] [D loss: 0.457368] [G loss: 0.923186] [info loss: 2.488725]\n",
            "[Epoch 0/200] [Batch 8/469] [D loss: 0.447540] [G loss: 0.905998] [info loss: 2.494567]\n",
            "[Epoch 0/200] [Batch 9/469] [D loss: 0.436703] [G loss: 0.880101] [info loss: 2.495739]\n",
            "[Epoch 0/200] [Batch 10/469] [D loss: 0.418810] [G loss: 0.845121] [info loss: 2.496710]\n",
            "[Epoch 0/200] [Batch 11/469] [D loss: 0.400655] [G loss: 0.802012] [info loss: 2.477338]\n",
            "[Epoch 0/200] [Batch 12/469] [D loss: 0.376918] [G loss: 0.749609] [info loss: 2.464515]\n",
            "[Epoch 0/200] [Batch 13/469] [D loss: 0.350804] [G loss: 0.681752] [info loss: 2.465601]\n",
            "[Epoch 0/200] [Batch 14/469] [D loss: 0.328092] [G loss: 0.585106] [info loss: 2.469678]\n",
            "[Epoch 0/200] [Batch 15/469] [D loss: 0.306105] [G loss: 0.475442] [info loss: 2.446889]\n",
            "[Epoch 0/200] [Batch 16/469] [D loss: 0.292449] [G loss: 0.379506] [info loss: 2.446990]\n",
            "[Epoch 0/200] [Batch 17/469] [D loss: 0.281976] [G loss: 0.276083] [info loss: 2.427967]\n",
            "[Epoch 0/200] [Batch 18/469] [D loss: 0.288649] [G loss: 0.240190] [info loss: 2.449394]\n",
            "[Epoch 0/200] [Batch 19/469] [D loss: 0.296941] [G loss: 0.200764] [info loss: 2.418822]\n",
            "[Epoch 0/200] [Batch 20/469] [D loss: 0.288710] [G loss: 0.202644] [info loss: 2.426629]\n",
            "[Epoch 0/200] [Batch 21/469] [D loss: 0.272450] [G loss: 0.223021] [info loss: 2.410709]\n",
            "[Epoch 0/200] [Batch 22/469] [D loss: 0.264975] [G loss: 0.221639] [info loss: 2.406542]\n",
            "[Epoch 0/200] [Batch 23/469] [D loss: 0.264667] [G loss: 0.251293] [info loss: 2.404109]\n",
            "[Epoch 0/200] [Batch 24/469] [D loss: 0.264887] [G loss: 0.259002] [info loss: 2.426310]\n",
            "[Epoch 0/200] [Batch 25/469] [D loss: 0.252350] [G loss: 0.280041] [info loss: 2.410087]\n",
            "[Epoch 0/200] [Batch 26/469] [D loss: 0.235519] [G loss: 0.280206] [info loss: 2.399833]\n",
            "[Epoch 0/200] [Batch 27/469] [D loss: 0.224143] [G loss: 0.281910] [info loss: 2.399488]\n",
            "[Epoch 0/200] [Batch 28/469] [D loss: 0.224397] [G loss: 0.301648] [info loss: 2.400028]\n",
            "[Epoch 0/200] [Batch 29/469] [D loss: 0.203827] [G loss: 0.311647] [info loss: 2.391034]\n",
            "[Epoch 0/200] [Batch 30/469] [D loss: 0.199618] [G loss: 0.332293] [info loss: 2.390260]\n",
            "[Epoch 0/200] [Batch 31/469] [D loss: 0.183712] [G loss: 0.327932] [info loss: 2.397252]\n",
            "[Epoch 0/200] [Batch 32/469] [D loss: 0.186719] [G loss: 0.340060] [info loss: 2.380980]\n",
            "[Epoch 0/200] [Batch 33/469] [D loss: 0.197325] [G loss: 0.323229] [info loss: 2.372070]\n",
            "[Epoch 0/200] [Batch 34/469] [D loss: 0.177821] [G loss: 0.337417] [info loss: 2.374271]\n",
            "[Epoch 0/200] [Batch 35/469] [D loss: 0.180213] [G loss: 0.310793] [info loss: 2.377701]\n",
            "[Epoch 0/200] [Batch 36/469] [D loss: 0.180163] [G loss: 0.302937] [info loss: 2.361986]\n",
            "[Epoch 0/200] [Batch 37/469] [D loss: 0.178968] [G loss: 0.317632] [info loss: 2.378692]\n",
            "[Epoch 0/200] [Batch 38/469] [D loss: 0.178145] [G loss: 0.339794] [info loss: 2.375561]\n",
            "[Epoch 0/200] [Batch 39/469] [D loss: 0.167262] [G loss: 0.329901] [info loss: 2.357082]\n",
            "[Epoch 0/200] [Batch 40/469] [D loss: 0.172011] [G loss: 0.326199] [info loss: 2.356569]\n",
            "[Epoch 0/200] [Batch 41/469] [D loss: 0.183473] [G loss: 0.277180] [info loss: 2.364410]\n",
            "[Epoch 0/200] [Batch 42/469] [D loss: 0.182628] [G loss: 0.288805] [info loss: 2.366400]\n",
            "[Epoch 0/200] [Batch 43/469] [D loss: 0.193803] [G loss: 0.254272] [info loss: 2.361350]\n",
            "[Epoch 0/200] [Batch 44/469] [D loss: 0.190437] [G loss: 0.294894] [info loss: 2.363148]\n",
            "[Epoch 0/200] [Batch 45/469] [D loss: 0.197914] [G loss: 0.286293] [info loss: 2.350964]\n",
            "[Epoch 0/200] [Batch 46/469] [D loss: 0.192101] [G loss: 0.294996] [info loss: 2.349474]\n",
            "[Epoch 0/200] [Batch 47/469] [D loss: 0.183442] [G loss: 0.304956] [info loss: 2.348971]\n",
            "[Epoch 0/200] [Batch 48/469] [D loss: 0.176712] [G loss: 0.319067] [info loss: 2.347739]\n",
            "[Epoch 0/200] [Batch 49/469] [D loss: 0.164324] [G loss: 0.342206] [info loss: 2.346621]\n",
            "[Epoch 0/200] [Batch 50/469] [D loss: 0.173574] [G loss: 0.302497] [info loss: 2.347405]\n",
            "[Epoch 0/200] [Batch 51/469] [D loss: 0.170166] [G loss: 0.366027] [info loss: 2.340612]\n",
            "[Epoch 0/200] [Batch 52/469] [D loss: 0.159523] [G loss: 0.367535] [info loss: 2.340090]\n",
            "[Epoch 0/200] [Batch 53/469] [D loss: 0.135206] [G loss: 0.380340] [info loss: 2.343626]\n",
            "[Epoch 0/200] [Batch 54/469] [D loss: 0.157573] [G loss: 0.356680] [info loss: 2.332403]\n",
            "[Epoch 0/200] [Batch 55/469] [D loss: 0.135238] [G loss: 0.400822] [info loss: 2.338345]\n",
            "[Epoch 0/200] [Batch 56/469] [D loss: 0.139811] [G loss: 0.349259] [info loss: 2.337793]\n",
            "[Epoch 0/200] [Batch 57/469] [D loss: 0.136622] [G loss: 0.387965] [info loss: 2.343231]\n",
            "[Epoch 0/200] [Batch 58/469] [D loss: 0.145063] [G loss: 0.414984] [info loss: 2.330451]\n",
            "[Epoch 0/200] [Batch 59/469] [D loss: 0.147857] [G loss: 0.408761] [info loss: 2.324842]\n",
            "[Epoch 0/200] [Batch 60/469] [D loss: 0.157458] [G loss: 0.333074] [info loss: 2.323636]\n",
            "[Epoch 0/200] [Batch 61/469] [D loss: 0.160081] [G loss: 0.336954] [info loss: 2.338166]\n",
            "[Epoch 0/200] [Batch 62/469] [D loss: 0.159264] [G loss: 0.353400] [info loss: 2.328327]\n",
            "[Epoch 0/200] [Batch 63/469] [D loss: 0.156463] [G loss: 0.362776] [info loss: 2.321646]\n",
            "[Epoch 0/200] [Batch 64/469] [D loss: 0.160080] [G loss: 0.299573] [info loss: 2.327868]\n",
            "[Epoch 0/200] [Batch 65/469] [D loss: 0.157487] [G loss: 0.310982] [info loss: 2.320853]\n",
            "[Epoch 0/200] [Batch 66/469] [D loss: 0.172564] [G loss: 0.336977] [info loss: 2.323651]\n",
            "[Epoch 0/200] [Batch 67/469] [D loss: 0.170226] [G loss: 0.341989] [info loss: 2.322065]\n",
            "[Epoch 0/200] [Batch 68/469] [D loss: 0.177280] [G loss: 0.346140] [info loss: 2.330730]\n",
            "[Epoch 0/200] [Batch 69/469] [D loss: 0.206206] [G loss: 0.334722] [info loss: 2.324718]\n",
            "[Epoch 0/200] [Batch 70/469] [D loss: 0.208149] [G loss: 0.283054] [info loss: 2.326365]\n",
            "[Epoch 0/200] [Batch 71/469] [D loss: 0.197819] [G loss: 0.320892] [info loss: 2.331542]\n",
            "[Epoch 0/200] [Batch 72/469] [D loss: 0.204657] [G loss: 0.304717] [info loss: 2.318264]\n",
            "[Epoch 0/200] [Batch 73/469] [D loss: 0.202255] [G loss: 0.322149] [info loss: 2.324393]\n",
            "[Epoch 0/200] [Batch 74/469] [D loss: 0.221437] [G loss: 0.283940] [info loss: 2.326447]\n",
            "[Epoch 0/200] [Batch 75/469] [D loss: 0.223842] [G loss: 0.254300] [info loss: 2.317086]\n",
            "[Epoch 0/200] [Batch 76/469] [D loss: 0.230330] [G loss: 0.275122] [info loss: 2.315480]\n",
            "[Epoch 0/200] [Batch 77/469] [D loss: 0.216687] [G loss: 0.298791] [info loss: 2.312564]\n",
            "[Epoch 0/200] [Batch 78/469] [D loss: 0.232591] [G loss: 0.260886] [info loss: 2.325040]\n",
            "[Epoch 0/200] [Batch 79/469] [D loss: 0.221969] [G loss: 0.259394] [info loss: 2.320584]\n",
            "[Epoch 0/200] [Batch 80/469] [D loss: 0.224098] [G loss: 0.305529] [info loss: 2.307364]\n",
            "[Epoch 0/200] [Batch 81/469] [D loss: 0.219811] [G loss: 0.293270] [info loss: 2.313895]\n",
            "[Epoch 0/200] [Batch 82/469] [D loss: 0.219119] [G loss: 0.285791] [info loss: 2.304043]\n",
            "[Epoch 0/200] [Batch 83/469] [D loss: 0.217885] [G loss: 0.277972] [info loss: 2.304917]\n",
            "[Epoch 0/200] [Batch 84/469] [D loss: 0.215776] [G loss: 0.301095] [info loss: 2.311631]\n",
            "[Epoch 0/200] [Batch 85/469] [D loss: 0.214580] [G loss: 0.271635] [info loss: 2.307346]\n",
            "[Epoch 0/200] [Batch 86/469] [D loss: 0.225093] [G loss: 0.294114] [info loss: 2.308388]\n",
            "[Epoch 0/200] [Batch 87/469] [D loss: 0.222106] [G loss: 0.313812] [info loss: 2.304984]\n",
            "[Epoch 0/200] [Batch 88/469] [D loss: 0.228749] [G loss: 0.304687] [info loss: 2.305592]\n",
            "[Epoch 0/200] [Batch 89/469] [D loss: 0.237366] [G loss: 0.297352] [info loss: 2.293456]\n",
            "[Epoch 0/200] [Batch 90/469] [D loss: 0.223200] [G loss: 0.293787] [info loss: 2.292763]\n",
            "[Epoch 0/200] [Batch 91/469] [D loss: 0.210289] [G loss: 0.300074] [info loss: 2.298177]\n",
            "[Epoch 0/200] [Batch 92/469] [D loss: 0.229900] [G loss: 0.280026] [info loss: 2.290816]\n",
            "[Epoch 0/200] [Batch 93/469] [D loss: 0.235702] [G loss: 0.313708] [info loss: 2.301737]\n",
            "[Epoch 0/200] [Batch 94/469] [D loss: 0.218030] [G loss: 0.294199] [info loss: 2.273126]\n",
            "[Epoch 0/200] [Batch 95/469] [D loss: 0.200457] [G loss: 0.355627] [info loss: 2.281008]\n",
            "[Epoch 0/200] [Batch 96/469] [D loss: 0.198759] [G loss: 0.361322] [info loss: 2.275989]\n",
            "[Epoch 0/200] [Batch 97/469] [D loss: 0.217618] [G loss: 0.353812] [info loss: 2.280624]\n",
            "[Epoch 0/200] [Batch 98/469] [D loss: 0.229708] [G loss: 0.264396] [info loss: 2.269112]\n",
            "[Epoch 0/200] [Batch 99/469] [D loss: 0.212429] [G loss: 0.340563] [info loss: 2.276265]\n",
            "[Epoch 0/200] [Batch 100/469] [D loss: 0.208203] [G loss: 0.308602] [info loss: 2.237443]\n",
            "[Epoch 0/200] [Batch 101/469] [D loss: 0.204386] [G loss: 0.347078] [info loss: 2.243618]\n",
            "[Epoch 0/200] [Batch 102/469] [D loss: 0.198733] [G loss: 0.336370] [info loss: 2.221579]\n",
            "[Epoch 0/200] [Batch 103/469] [D loss: 0.220593] [G loss: 0.317164] [info loss: 2.232153]\n",
            "[Epoch 0/200] [Batch 104/469] [D loss: 0.204894] [G loss: 0.356326] [info loss: 2.217388]\n",
            "[Epoch 0/200] [Batch 105/469] [D loss: 0.195415] [G loss: 0.361951] [info loss: 2.206370]\n",
            "[Epoch 0/200] [Batch 106/469] [D loss: 0.207673] [G loss: 0.282061] [info loss: 2.205416]\n",
            "[Epoch 0/200] [Batch 107/469] [D loss: 0.189533] [G loss: 0.345667] [info loss: 2.185464]\n",
            "[Epoch 0/200] [Batch 108/469] [D loss: 0.189205] [G loss: 0.438349] [info loss: 2.205265]\n",
            "[Epoch 0/200] [Batch 109/469] [D loss: 0.194105] [G loss: 0.340575] [info loss: 2.190601]\n",
            "[Epoch 0/200] [Batch 110/469] [D loss: 0.149936] [G loss: 0.383130] [info loss: 2.161780]\n",
            "[Epoch 0/200] [Batch 111/469] [D loss: 0.188091] [G loss: 0.444013] [info loss: 2.164547]\n",
            "[Epoch 0/200] [Batch 112/469] [D loss: 0.185619] [G loss: 0.344707] [info loss: 2.140939]\n",
            "[Epoch 0/200] [Batch 113/469] [D loss: 0.172364] [G loss: 0.367521] [info loss: 2.145493]\n",
            "[Epoch 0/200] [Batch 114/469] [D loss: 0.195160] [G loss: 0.413576] [info loss: 2.168121]\n",
            "[Epoch 0/200] [Batch 115/469] [D loss: 0.176284] [G loss: 0.387881] [info loss: 2.169036]\n",
            "[Epoch 0/200] [Batch 116/469] [D loss: 0.217563] [G loss: 0.321304] [info loss: 2.163704]\n",
            "[Epoch 0/200] [Batch 117/469] [D loss: 0.194109] [G loss: 0.430815] [info loss: 2.072074]\n",
            "[Epoch 0/200] [Batch 118/469] [D loss: 0.205240] [G loss: 0.301590] [info loss: 2.112517]\n",
            "[Epoch 0/200] [Batch 119/469] [D loss: 0.198970] [G loss: 0.424769] [info loss: 2.139652]\n",
            "[Epoch 0/200] [Batch 120/469] [D loss: 0.182971] [G loss: 0.386150] [info loss: 2.099738]\n",
            "[Epoch 0/200] [Batch 121/469] [D loss: 0.159969] [G loss: 0.376399] [info loss: 2.104408]\n",
            "[Epoch 0/200] [Batch 122/469] [D loss: 0.189902] [G loss: 0.381998] [info loss: 2.097924]\n",
            "[Epoch 0/200] [Batch 123/469] [D loss: 0.166022] [G loss: 0.366720] [info loss: 2.079644]\n",
            "[Epoch 0/200] [Batch 124/469] [D loss: 0.203626] [G loss: 0.303524] [info loss: 2.107331]\n",
            "[Epoch 0/200] [Batch 125/469] [D loss: 0.191833] [G loss: 0.520260] [info loss: 2.056019]\n",
            "[Epoch 0/200] [Batch 126/469] [D loss: 0.190073] [G loss: 0.411968] [info loss: 2.054400]\n",
            "[Epoch 0/200] [Batch 127/469] [D loss: 0.192003] [G loss: 0.326333] [info loss: 2.041875]\n",
            "[Epoch 0/200] [Batch 128/469] [D loss: 0.165750] [G loss: 0.407425] [info loss: 2.054143]\n",
            "[Epoch 0/200] [Batch 129/469] [D loss: 0.189621] [G loss: 0.412729] [info loss: 2.062034]\n",
            "[Epoch 0/200] [Batch 130/469] [D loss: 0.192425] [G loss: 0.405298] [info loss: 2.002489]\n",
            "[Epoch 0/200] [Batch 131/469] [D loss: 0.170439] [G loss: 0.412675] [info loss: 2.053898]\n",
            "[Epoch 0/200] [Batch 132/469] [D loss: 0.187017] [G loss: 0.388141] [info loss: 1.999003]\n",
            "[Epoch 0/200] [Batch 133/469] [D loss: 0.189584] [G loss: 0.379828] [info loss: 1.995480]\n",
            "[Epoch 0/200] [Batch 134/469] [D loss: 0.186449] [G loss: 0.517319] [info loss: 1.968074]\n",
            "[Epoch 0/200] [Batch 135/469] [D loss: 0.200120] [G loss: 0.468917] [info loss: 1.969555]\n",
            "[Epoch 0/200] [Batch 136/469] [D loss: 0.190377] [G loss: 0.384754] [info loss: 1.959608]\n",
            "[Epoch 0/200] [Batch 137/469] [D loss: 0.188909] [G loss: 0.363084] [info loss: 1.947771]\n",
            "[Epoch 0/200] [Batch 138/469] [D loss: 0.190955] [G loss: 0.403244] [info loss: 1.923539]\n",
            "[Epoch 0/200] [Batch 139/469] [D loss: 0.149723] [G loss: 0.466522] [info loss: 1.970914]\n",
            "[Epoch 0/200] [Batch 140/469] [D loss: 0.187567] [G loss: 0.441364] [info loss: 1.924174]\n",
            "[Epoch 0/200] [Batch 141/469] [D loss: 0.166042] [G loss: 0.395826] [info loss: 1.877554]\n",
            "[Epoch 0/200] [Batch 142/469] [D loss: 0.185123] [G loss: 0.377287] [info loss: 1.941728]\n",
            "[Epoch 0/200] [Batch 143/469] [D loss: 0.197172] [G loss: 0.385106] [info loss: 1.932513]\n",
            "[Epoch 0/200] [Batch 144/469] [D loss: 0.172568] [G loss: 0.464605] [info loss: 1.945828]\n",
            "[Epoch 0/200] [Batch 145/469] [D loss: 0.204331] [G loss: 0.347162] [info loss: 1.960436]\n",
            "[Epoch 0/200] [Batch 146/469] [D loss: 0.245963] [G loss: 0.294795] [info loss: 1.879972]\n",
            "[Epoch 0/200] [Batch 147/469] [D loss: 0.210958] [G loss: 0.416655] [info loss: 1.892682]\n",
            "[Epoch 0/200] [Batch 148/469] [D loss: 0.202758] [G loss: 0.501045] [info loss: 1.836610]\n",
            "[Epoch 0/200] [Batch 149/469] [D loss: 0.199507] [G loss: 0.438453] [info loss: 1.885255]\n",
            "[Epoch 0/200] [Batch 150/469] [D loss: 0.187231] [G loss: 0.354413] [info loss: 1.871941]\n",
            "[Epoch 0/200] [Batch 151/469] [D loss: 0.214340] [G loss: 0.255328] [info loss: 1.907533]\n",
            "[Epoch 0/200] [Batch 152/469] [D loss: 0.201236] [G loss: 0.394799] [info loss: 1.820216]\n",
            "[Epoch 0/200] [Batch 153/469] [D loss: 0.211803] [G loss: 0.501432] [info loss: 1.828711]\n",
            "[Epoch 0/200] [Batch 154/469] [D loss: 0.204786] [G loss: 0.378337] [info loss: 1.885338]\n",
            "[Epoch 0/200] [Batch 155/469] [D loss: 0.205265] [G loss: 0.447027] [info loss: 1.868852]\n",
            "[Epoch 0/200] [Batch 156/469] [D loss: 0.224659] [G loss: 0.362331] [info loss: 1.815996]\n",
            "[Epoch 0/200] [Batch 157/469] [D loss: 0.216806] [G loss: 0.491527] [info loss: 1.848076]\n",
            "[Epoch 0/200] [Batch 158/469] [D loss: 0.201223] [G loss: 0.392072] [info loss: 1.804022]\n",
            "[Epoch 0/200] [Batch 159/469] [D loss: 0.217411] [G loss: 0.350970] [info loss: 1.854394]\n",
            "[Epoch 0/200] [Batch 160/469] [D loss: 0.194927] [G loss: 0.371445] [info loss: 1.808398]\n",
            "[Epoch 0/200] [Batch 161/469] [D loss: 0.199960] [G loss: 0.373096] [info loss: 1.837307]\n",
            "[Epoch 0/200] [Batch 162/469] [D loss: 0.221679] [G loss: 0.360972] [info loss: 1.812214]\n",
            "[Epoch 0/200] [Batch 163/469] [D loss: 0.204534] [G loss: 0.381620] [info loss: 1.783540]\n",
            "[Epoch 0/200] [Batch 164/469] [D loss: 0.223675] [G loss: 0.458202] [info loss: 1.795748]\n",
            "[Epoch 0/200] [Batch 165/469] [D loss: 0.208971] [G loss: 0.305533] [info loss: 1.848044]\n",
            "[Epoch 0/200] [Batch 166/469] [D loss: 0.203163] [G loss: 0.403936] [info loss: 1.789976]\n",
            "[Epoch 0/200] [Batch 167/469] [D loss: 0.230838] [G loss: 0.395599] [info loss: 1.741465]\n",
            "[Epoch 0/200] [Batch 168/469] [D loss: 0.223904] [G loss: 0.383483] [info loss: 1.715428]\n",
            "[Epoch 0/200] [Batch 169/469] [D loss: 0.212631] [G loss: 0.482373] [info loss: 1.754240]\n",
            "[Epoch 0/200] [Batch 170/469] [D loss: 0.232221] [G loss: 0.356242] [info loss: 1.786086]\n",
            "[Epoch 0/200] [Batch 171/469] [D loss: 0.245242] [G loss: 0.473705] [info loss: 1.749615]\n",
            "[Epoch 0/200] [Batch 172/469] [D loss: 0.240161] [G loss: 0.299716] [info loss: 1.751655]\n",
            "[Epoch 0/200] [Batch 173/469] [D loss: 0.229015] [G loss: 0.336544] [info loss: 1.745858]\n",
            "[Epoch 0/200] [Batch 174/469] [D loss: 0.234464] [G loss: 0.455296] [info loss: 1.708932]\n",
            "[Epoch 0/200] [Batch 175/469] [D loss: 0.227802] [G loss: 0.374440] [info loss: 1.719075]\n",
            "[Epoch 0/200] [Batch 176/469] [D loss: 0.233812] [G loss: 0.411930] [info loss: 1.686517]\n",
            "[Epoch 0/200] [Batch 177/469] [D loss: 0.224270] [G loss: 0.333842] [info loss: 1.731292]\n",
            "[Epoch 0/200] [Batch 178/469] [D loss: 0.214209] [G loss: 0.304940] [info loss: 1.756032]\n",
            "[Epoch 0/200] [Batch 179/469] [D loss: 0.220026] [G loss: 0.431436] [info loss: 1.681222]\n",
            "[Epoch 0/200] [Batch 180/469] [D loss: 0.215172] [G loss: 0.368256] [info loss: 1.670610]\n",
            "[Epoch 0/200] [Batch 181/469] [D loss: 0.226577] [G loss: 0.338521] [info loss: 1.694489]\n",
            "[Epoch 0/200] [Batch 182/469] [D loss: 0.225686] [G loss: 0.424856] [info loss: 1.690998]\n",
            "[Epoch 0/200] [Batch 183/469] [D loss: 0.243141] [G loss: 0.422199] [info loss: 1.694860]\n",
            "[Epoch 0/200] [Batch 184/469] [D loss: 0.229932] [G loss: 0.292369] [info loss: 1.675163]\n",
            "[Epoch 0/200] [Batch 185/469] [D loss: 0.216816] [G loss: 0.389102] [info loss: 1.650363]\n",
            "[Epoch 0/200] [Batch 186/469] [D loss: 0.230051] [G loss: 0.437056] [info loss: 1.675736]\n",
            "[Epoch 0/200] [Batch 187/469] [D loss: 0.233857] [G loss: 0.360472] [info loss: 1.626092]\n",
            "[Epoch 0/200] [Batch 188/469] [D loss: 0.243699] [G loss: 0.393224] [info loss: 1.607527]\n",
            "[Epoch 0/200] [Batch 189/469] [D loss: 0.244625] [G loss: 0.336376] [info loss: 1.609607]\n",
            "[Epoch 0/200] [Batch 190/469] [D loss: 0.233200] [G loss: 0.447770] [info loss: 1.607613]\n",
            "[Epoch 0/200] [Batch 191/469] [D loss: 0.205326] [G loss: 0.387521] [info loss: 1.631725]\n",
            "[Epoch 0/200] [Batch 192/469] [D loss: 0.230591] [G loss: 0.407461] [info loss: 1.639228]\n",
            "[Epoch 0/200] [Batch 193/469] [D loss: 0.196067] [G loss: 0.421010] [info loss: 1.654087]\n",
            "[Epoch 0/200] [Batch 194/469] [D loss: 0.236872] [G loss: 0.310768] [info loss: 1.602798]\n",
            "[Epoch 0/200] [Batch 195/469] [D loss: 0.249193] [G loss: 0.281753] [info loss: 1.626840]\n",
            "[Epoch 0/200] [Batch 196/469] [D loss: 0.214248] [G loss: 0.477894] [info loss: 1.606882]\n",
            "[Epoch 0/200] [Batch 197/469] [D loss: 0.211248] [G loss: 0.443476] [info loss: 1.605321]\n",
            "[Epoch 0/200] [Batch 198/469] [D loss: 0.245011] [G loss: 0.257016] [info loss: 1.571331]\n",
            "[Epoch 0/200] [Batch 199/469] [D loss: 0.221974] [G loss: 0.394344] [info loss: 1.569483]\n",
            "[Epoch 0/200] [Batch 200/469] [D loss: 0.236413] [G loss: 0.413369] [info loss: 1.595726]\n",
            "[Epoch 0/200] [Batch 201/469] [D loss: 0.242373] [G loss: 0.349212] [info loss: 1.602150]\n",
            "[Epoch 0/200] [Batch 202/469] [D loss: 0.214275] [G loss: 0.368069] [info loss: 1.602808]\n",
            "[Epoch 0/200] [Batch 203/469] [D loss: 0.214153] [G loss: 0.467799] [info loss: 1.574655]\n",
            "[Epoch 0/200] [Batch 204/469] [D loss: 0.234635] [G loss: 0.405699] [info loss: 1.589438]\n",
            "[Epoch 0/200] [Batch 205/469] [D loss: 0.245148] [G loss: 0.296326] [info loss: 1.584271]\n",
            "[Epoch 0/200] [Batch 206/469] [D loss: 0.220173] [G loss: 0.438270] [info loss: 1.565177]\n",
            "[Epoch 0/200] [Batch 207/469] [D loss: 0.214514] [G loss: 0.413231] [info loss: 1.571463]\n",
            "[Epoch 0/200] [Batch 208/469] [D loss: 0.219956] [G loss: 0.357565] [info loss: 1.578187]\n",
            "[Epoch 0/200] [Batch 209/469] [D loss: 0.225668] [G loss: 0.371865] [info loss: 1.557725]\n",
            "[Epoch 0/200] [Batch 210/469] [D loss: 0.243361] [G loss: 0.385284] [info loss: 1.614682]\n",
            "[Epoch 0/200] [Batch 211/469] [D loss: 0.216390] [G loss: 0.417617] [info loss: 1.557362]\n",
            "[Epoch 0/200] [Batch 212/469] [D loss: 0.183218] [G loss: 0.386586] [info loss: 1.568627]\n",
            "[Epoch 0/200] [Batch 213/469] [D loss: 0.231370] [G loss: 0.349616] [info loss: 1.547642]\n",
            "[Epoch 0/200] [Batch 214/469] [D loss: 0.243591] [G loss: 0.375925] [info loss: 1.556944]\n",
            "[Epoch 0/200] [Batch 215/469] [D loss: 0.228396] [G loss: 0.431497] [info loss: 1.578131]\n",
            "[Epoch 0/200] [Batch 216/469] [D loss: 0.209318] [G loss: 0.365417] [info loss: 1.566258]\n",
            "[Epoch 0/200] [Batch 217/469] [D loss: 0.235883] [G loss: 0.337429] [info loss: 1.560614]\n",
            "[Epoch 0/200] [Batch 218/469] [D loss: 0.216288] [G loss: 0.342090] [info loss: 1.561882]\n",
            "[Epoch 0/200] [Batch 219/469] [D loss: 0.229668] [G loss: 0.402482] [info loss: 1.556674]\n",
            "[Epoch 0/200] [Batch 220/469] [D loss: 0.227422] [G loss: 0.336250] [info loss: 1.564703]\n",
            "[Epoch 0/200] [Batch 221/469] [D loss: 0.238500] [G loss: 0.372218] [info loss: 1.563368]\n",
            "[Epoch 0/200] [Batch 222/469] [D loss: 0.232849] [G loss: 0.334319] [info loss: 1.537914]\n",
            "[Epoch 0/200] [Batch 223/469] [D loss: 0.239965] [G loss: 0.304692] [info loss: 1.547330]\n",
            "[Epoch 0/200] [Batch 224/469] [D loss: 0.246124] [G loss: 0.382885] [info loss: 1.537346]\n",
            "[Epoch 0/200] [Batch 225/469] [D loss: 0.234685] [G loss: 0.338616] [info loss: 1.548675]\n",
            "[Epoch 0/200] [Batch 226/469] [D loss: 0.256928] [G loss: 0.384739] [info loss: 1.548726]\n",
            "[Epoch 0/200] [Batch 227/469] [D loss: 0.214520] [G loss: 0.414886] [info loss: 1.557421]\n",
            "[Epoch 0/200] [Batch 228/469] [D loss: 0.243660] [G loss: 0.305558] [info loss: 1.551443]\n",
            "[Epoch 0/200] [Batch 229/469] [D loss: 0.221341] [G loss: 0.351216] [info loss: 1.549215]\n",
            "[Epoch 0/200] [Batch 230/469] [D loss: 0.197557] [G loss: 0.420738] [info loss: 1.547950]\n",
            "[Epoch 0/200] [Batch 231/469] [D loss: 0.222150] [G loss: 0.419882] [info loss: 1.533650]\n",
            "[Epoch 0/200] [Batch 232/469] [D loss: 0.208589] [G loss: 0.366972] [info loss: 1.533271]\n",
            "[Epoch 0/200] [Batch 233/469] [D loss: 0.237305] [G loss: 0.401916] [info loss: 1.528075]\n",
            "[Epoch 0/200] [Batch 234/469] [D loss: 0.253482] [G loss: 0.326679] [info loss: 1.539876]\n",
            "[Epoch 0/200] [Batch 235/469] [D loss: 0.246925] [G loss: 0.386065] [info loss: 1.545017]\n",
            "[Epoch 0/200] [Batch 236/469] [D loss: 0.212027] [G loss: 0.381617] [info loss: 1.541887]\n",
            "[Epoch 0/200] [Batch 237/469] [D loss: 0.235088] [G loss: 0.277146] [info loss: 1.533558]\n",
            "[Epoch 0/200] [Batch 238/469] [D loss: 0.226065] [G loss: 0.428829] [info loss: 1.538476]\n",
            "[Epoch 0/200] [Batch 239/469] [D loss: 0.220522] [G loss: 0.402975] [info loss: 1.560963]\n",
            "[Epoch 0/200] [Batch 240/469] [D loss: 0.217657] [G loss: 0.362751] [info loss: 1.542964]\n",
            "[Epoch 0/200] [Batch 241/469] [D loss: 0.253948] [G loss: 0.353746] [info loss: 1.531558]\n",
            "[Epoch 0/200] [Batch 242/469] [D loss: 0.246954] [G loss: 0.365062] [info loss: 1.520068]\n",
            "[Epoch 0/200] [Batch 243/469] [D loss: 0.229672] [G loss: 0.346379] [info loss: 1.539647]\n",
            "[Epoch 0/200] [Batch 244/469] [D loss: 0.226710] [G loss: 0.415129] [info loss: 1.568951]\n",
            "[Epoch 0/200] [Batch 245/469] [D loss: 0.233641] [G loss: 0.461761] [info loss: 1.572690]\n",
            "[Epoch 0/200] [Batch 246/469] [D loss: 0.240606] [G loss: 0.272596] [info loss: 1.541832]\n",
            "[Epoch 0/200] [Batch 247/469] [D loss: 0.227042] [G loss: 0.416736] [info loss: 1.543129]\n",
            "[Epoch 0/200] [Batch 248/469] [D loss: 0.257919] [G loss: 0.403790] [info loss: 1.546513]\n",
            "[Epoch 0/200] [Batch 249/469] [D loss: 0.237584] [G loss: 0.373208] [info loss: 1.548578]\n",
            "[Epoch 0/200] [Batch 250/469] [D loss: 0.220673] [G loss: 0.379568] [info loss: 1.542862]\n",
            "[Epoch 0/200] [Batch 251/469] [D loss: 0.231919] [G loss: 0.336372] [info loss: 1.542289]\n",
            "[Epoch 0/200] [Batch 252/469] [D loss: 0.244334] [G loss: 0.374345] [info loss: 1.519226]\n",
            "[Epoch 0/200] [Batch 253/469] [D loss: 0.205063] [G loss: 0.405538] [info loss: 1.536631]\n",
            "[Epoch 0/200] [Batch 254/469] [D loss: 0.234833] [G loss: 0.388417] [info loss: 1.517532]\n",
            "[Epoch 0/200] [Batch 255/469] [D loss: 0.250481] [G loss: 0.313429] [info loss: 1.545203]\n",
            "[Epoch 0/200] [Batch 256/469] [D loss: 0.243147] [G loss: 0.357009] [info loss: 1.532266]\n",
            "[Epoch 0/200] [Batch 257/469] [D loss: 0.255849] [G loss: 0.323992] [info loss: 1.524128]\n",
            "[Epoch 0/200] [Batch 258/469] [D loss: 0.263229] [G loss: 0.289513] [info loss: 1.523433]\n",
            "[Epoch 0/200] [Batch 259/469] [D loss: 0.253664] [G loss: 0.408670] [info loss: 1.520789]\n",
            "[Epoch 0/200] [Batch 260/469] [D loss: 0.230980] [G loss: 0.395609] [info loss: 1.527204]\n",
            "[Epoch 0/200] [Batch 261/469] [D loss: 0.244604] [G loss: 0.329766] [info loss: 1.515794]\n",
            "[Epoch 0/200] [Batch 262/469] [D loss: 0.223202] [G loss: 0.333412] [info loss: 1.510628]\n",
            "[Epoch 0/200] [Batch 263/469] [D loss: 0.240814] [G loss: 0.290449] [info loss: 1.531209]\n",
            "[Epoch 0/200] [Batch 264/469] [D loss: 0.243755] [G loss: 0.353154] [info loss: 1.515164]\n",
            "[Epoch 0/200] [Batch 265/469] [D loss: 0.263864] [G loss: 0.346731] [info loss: 1.529544]\n",
            "[Epoch 0/200] [Batch 266/469] [D loss: 0.255037] [G loss: 0.316677] [info loss: 1.534449]\n",
            "[Epoch 0/200] [Batch 267/469] [D loss: 0.278675] [G loss: 0.339873] [info loss: 1.519300]\n",
            "[Epoch 0/200] [Batch 268/469] [D loss: 0.253148] [G loss: 0.382317] [info loss: 1.529225]\n",
            "[Epoch 0/200] [Batch 269/469] [D loss: 0.228566] [G loss: 0.339438] [info loss: 1.525144]\n",
            "[Epoch 0/200] [Batch 270/469] [D loss: 0.233982] [G loss: 0.377826] [info loss: 1.513088]\n",
            "[Epoch 0/200] [Batch 271/469] [D loss: 0.231947] [G loss: 0.400251] [info loss: 1.514789]\n",
            "[Epoch 0/200] [Batch 272/469] [D loss: 0.279360] [G loss: 0.280031] [info loss: 1.520295]\n",
            "[Epoch 0/200] [Batch 273/469] [D loss: 0.251881] [G loss: 0.351844] [info loss: 1.521506]\n",
            "[Epoch 0/200] [Batch 274/469] [D loss: 0.223624] [G loss: 0.348350] [info loss: 1.518917]\n",
            "[Epoch 0/200] [Batch 275/469] [D loss: 0.245857] [G loss: 0.369964] [info loss: 1.522856]\n",
            "[Epoch 0/200] [Batch 276/469] [D loss: 0.219641] [G loss: 0.314430] [info loss: 1.549917]\n",
            "[Epoch 0/200] [Batch 277/469] [D loss: 0.221471] [G loss: 0.318796] [info loss: 1.517463]\n",
            "[Epoch 0/200] [Batch 278/469] [D loss: 0.211765] [G loss: 0.373001] [info loss: 1.503880]\n",
            "[Epoch 0/200] [Batch 279/469] [D loss: 0.259889] [G loss: 0.343043] [info loss: 1.525641]\n",
            "[Epoch 0/200] [Batch 280/469] [D loss: 0.247195] [G loss: 0.359505] [info loss: 1.507193]\n",
            "[Epoch 0/200] [Batch 281/469] [D loss: 0.248204] [G loss: 0.351380] [info loss: 1.513671]\n",
            "[Epoch 0/200] [Batch 282/469] [D loss: 0.246294] [G loss: 0.342602] [info loss: 1.520416]\n",
            "[Epoch 0/200] [Batch 283/469] [D loss: 0.224928] [G loss: 0.351800] [info loss: 1.515068]\n",
            "[Epoch 0/200] [Batch 284/469] [D loss: 0.242574] [G loss: 0.303835] [info loss: 1.524016]\n",
            "[Epoch 0/200] [Batch 285/469] [D loss: 0.221913] [G loss: 0.418876] [info loss: 1.515188]\n",
            "[Epoch 0/200] [Batch 286/469] [D loss: 0.236864] [G loss: 0.377343] [info loss: 1.508302]\n",
            "[Epoch 0/200] [Batch 287/469] [D loss: 0.211772] [G loss: 0.313243] [info loss: 1.519887]\n",
            "[Epoch 0/200] [Batch 288/469] [D loss: 0.236320] [G loss: 0.348815] [info loss: 1.533552]\n",
            "[Epoch 0/200] [Batch 289/469] [D loss: 0.239366] [G loss: 0.383912] [info loss: 1.530769]\n",
            "[Epoch 0/200] [Batch 290/469] [D loss: 0.241837] [G loss: 0.400523] [info loss: 1.520619]\n",
            "[Epoch 0/200] [Batch 291/469] [D loss: 0.228834] [G loss: 0.325605] [info loss: 1.507690]\n",
            "[Epoch 0/200] [Batch 292/469] [D loss: 0.272444] [G loss: 0.281446] [info loss: 1.539887]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6ekdSG-IyHr9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}