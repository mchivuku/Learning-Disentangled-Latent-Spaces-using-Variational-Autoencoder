{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VAE + Classifier.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mchivuku/csb659-project/blob/master/VAE_%2B_Classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "NTSMmyaDT5F4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install tqdm six"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pkb7Xqt0cAbC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "83ffea5f-8a1b-45ae-ca84-0f31c07e0988"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "\n",
        "%cd /content/drive/My\\ Drive/Masters-DS/CSCI-B659/project/examples/vae/\n",
        "%ls"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n",
            "/content/drive/My Drive/Masters-DS/CSCI-B659/project/examples/vae\n",
            "\u001b[0m\u001b[01;34mMNIST\u001b[0m/  \u001b[01;34mresults\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ovIkb7hJbtnS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Classification Accuracy of VAE"
      ]
    },
    {
      "metadata": {
        "id": "w61zgwWJbwrK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "fe36fc8b-081b-4d6f-ae29-739dab37e3f0"
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.utils.data\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "\n",
        "params = {\n",
        "    \"batch_size\":128,\n",
        "    \"epochs\" : 10,\n",
        "    \"log_interval\":10\n",
        "    \n",
        "}\n",
        "\n",
        "torch.manual_seed(5)\n",
        "\n",
        "print('Torch', torch.__version__, 'CUDA', torch.version.cuda)\n",
        "print('Device:', torch.device('cuda:0'))\n",
        "print(torch.cuda.is_available())\n",
        "\n",
        "is_cuda = torch.cuda.is_available()\n",
        "device = torch.device ( \"cuda:0\" if torch.cuda.is_available () else \"cpu\" )\n",
        "\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Torch 1.0.1.post2 CUDA 10.0.130\n",
            "Device: cuda:0\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "L1pS8KL-chlZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class RunningAverage ():\n",
        "    \"\"\"A simple class that maintains the running average of a quantity\n",
        "    Example:\n",
        "    ```\n",
        "    loss_avg = RunningAverage()\n",
        "    loss_avg.update(2)\n",
        "    loss_avg.update(4)\n",
        "    loss_avg() = 3\n",
        "    ```\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__( self ):\n",
        "        self.steps = 0\n",
        "        self.total = 0\n",
        "\n",
        "    def update( self, val ):\n",
        "        self.total += val\n",
        "        self.steps += 1\n",
        "    \n",
        "    def reset(self):\n",
        "        self.steps = 0.\n",
        "        self.total = 0.\n",
        "        \n",
        "        \n",
        "        \n",
        "    def __call__( self ):\n",
        "        return self.total / float ( self.steps )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PnmqxKJpeNQ-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "outputId": "334449bc-68ef-423a-bc44-9efe511160f1"
      },
      "cell_type": "code",
      "source": [
        "## Data Loaders\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True}\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('MNIST/data', train=True, download=True,\n",
        "                   transform=transforms.ToTensor()),\n",
        "    batch_size=params.get(\"batch_size\"), shuffle=True, **kwargs)\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('MNIST/data', train=False, transform=transforms.ToTensor()),\n",
        "    batch_size=params.get(\"batch_size\"), shuffle=True, **kwargs)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  0%|          | 0/9912422 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to MNIST/data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "9920512it [00:00, 19829563.97it/s]                            \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting MNIST/data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "32768it [00:00, 299560.49it/s]                           \n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to MNIST/data/MNIST/raw/train-labels-idx1-ubyte.gz\n",
            "Extracting MNIST/data/MNIST/raw/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to MNIST/data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1654784it [00:00, 5204865.37it/s]                           \n",
            "8192it [00:00, 124379.14it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting MNIST/data/MNIST/raw/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to MNIST/data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
            "Extracting MNIST/data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n",
            "Processing...\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Elui0JjVcJ0I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "outputId": "097e5473-ef70-42b1-9156-574163ff79ee"
      },
      "cell_type": "code",
      "source": [
        "from torch.nn import init\n",
        "\n",
        "## Classifier\n",
        "class Classifier(nn.Module):\n",
        "  def __init__(self,dims):\n",
        "    \"\"\"\n",
        "    Single hidden layer with softmax\n",
        "    \"\"\"\n",
        "    super(Classifier,self).__init__()\n",
        "    [x_dim, h_dim, y_dim] = dims\n",
        "    \n",
        "    self.fc1 = nn.Linear(x_dim, h_dim)\n",
        "    self.fc2 = nn.Linear(h_dim, y_dim)\n",
        "    \n",
        "  def forward(self,x):\n",
        "    ## flatten x input\n",
        "    x = x.view(-1,self.num_flat_features(x))\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.softmax(self.fc2(x),dim=-1)\n",
        "    \n",
        "    return x\n",
        "  \n",
        "  \n",
        "  def num_flat_features(self,x):\n",
        "    size = x.size()[1:] # all dimensions except the batch dimension\n",
        "    num_features = 1\n",
        "    for s in size:\n",
        "      num_features *=s\n",
        "    return num_features\n",
        "  \n",
        "\"\"\"\n",
        "Gaussian Sample Layer\n",
        "\"\"\"\n",
        "class GaussianSample(nn.Module):\n",
        "  def __init__(self,in_features, out_features):\n",
        "    super(GaussianSample, self).__init__()\n",
        "    self.in_features = in_features\n",
        "    self.out_features = out_features\n",
        "    \n",
        "    self.mu = nn.Linear(in_features, out_features)\n",
        "    self.log_var = nn.Linear(in_features, out_features)\n",
        "    \n",
        "  def forward(self,x):\n",
        "    mu = self.mu(x)\n",
        "    log_var = F.softplus(self.logvar(x))\n",
        "    \n",
        "    return self.reparametrize(mu,log_var), mu, log_var\n",
        "    \n",
        "  def reparametrize(self, mu, log_var):\n",
        "    epsilon = Variable(torch.randn(mu.size()), requires_grad=False)\n",
        "\n",
        "    if mu.is_cuda:\n",
        "      epsilon = epsilon.cuda()\n",
        "    # log_std = 0.5 * log_var\n",
        "    # std = exp(log_std)\n",
        "    std = log_var.mul(0.5).exp_()\n",
        "\n",
        "    # z = std * epsilon + mu\n",
        "    z = mu.addcmul(std, epsilon)\n",
        "    return z\n",
        "      \n",
        "\n",
        "\"\"\"\n",
        "VAE Encoder\n",
        "\"\"\"\n",
        "class Encoder(nn.Module):\n",
        "  def __init__(self,dims):\n",
        "    super(Encoder,self).__init__()\n",
        "    [x_dim, h_dim, z_dim] = dims\n",
        "    ##linear layers\n",
        "    neurons = [x_dim, *h_dim]\n",
        "    linear_layers = [nn.Linear(neurons[i-1],neurons[i]) for i in range(1,len(neurons))]\n",
        "    \n",
        "    self.hidden=nn.ModuleList(linear_layers)\n",
        "    \n",
        "    self.sample = GaussianSample(h_dim[-1],z_dim)\n",
        "    \n",
        "    \n",
        "  def forward(self,x):\n",
        "    for layer in self.hidden:\n",
        "      x = F.relu(layer(x))\n",
        "    return self.sample(x)\n",
        "  \n",
        "\"\"\"\n",
        "VAE Decoder\n",
        "\"\"\"\n",
        "class Decoder(nn.Module):\n",
        "  def __init__(self,dims):\n",
        "    super(Decoder,self).__init__()\n",
        "    [z_dim, h_dim, x_dim] = dims\n",
        "    ##linear layers\n",
        "    neurons = [z_dim, *h_dim]\n",
        "    linear_layers = [nn.Linear(neurons[i-1],neurons[i]) for i in range(1,len(neurons))]\n",
        "    \n",
        "    self.hidden=nn.ModuleList(linear_layers)\n",
        "    \n",
        "    self.reconstruction = nn.Linear(h_dim[-1],x_dim)\n",
        "    self.output_activation = torch.sigmoid\n",
        "    \n",
        "    \n",
        "  def forward(self,x):\n",
        "    for layer in self.hidden:\n",
        "      x = F.relu(layer(x))\n",
        "    return self.output_activation(self.reconstruction(x))\n",
        "  \n",
        "\n",
        "\n",
        "\"\"\"\n",
        "VAE object\n",
        "\"\"\"\n",
        "class VAE(nn.Module):\n",
        "  def __init__(self,dims):\n",
        "    super(VAE,self).__init__()\n",
        "    \n",
        "    [x_dim,z_dim,h_dim] = dims\n",
        "    self.z_dim = z_dim\n",
        "    self.h_dim = h_dim\n",
        "    self.x_dim = x_dim\n",
        "    \n",
        "    \n",
        "    ## Encoder\n",
        "    self.encoder = Encoder([x_dim,h_dim,z_dim])\n",
        "    ## Decoder\n",
        "    self.decoder = Decoder([z_dim,h_dim,x_dim])\n",
        "    \n",
        "    ## xaview initialization for weights\n",
        "    for m in self.modules():\n",
        "      if isinstance(m, nn.Linear):\n",
        "        init.xavier_normal_(m.weight.data)\n",
        "        if m.bias is not None:\n",
        "          m.bias.data.zero_()\n",
        "    \n",
        " \n",
        "  \"\"\"\n",
        "  Forward\n",
        "  \"\"\"\n",
        "  def forward(self,x):\n",
        "    z, z_mu, z_log_var = self.encoder(x)\n",
        "    return self.decoder(z), z_mu, z_log_var\n",
        "  \n",
        "  \n",
        "  \n",
        "\"\"\"\n",
        "Deep Generative Model \n",
        "\"\"\"\n",
        "class DeepGenerativeModel(VAE):\n",
        "    def __init__(self, dims):\n",
        "        \"\"\"\n",
        "        M2 code replication from the paper\n",
        "        'Semi-Supervised Learning with Deep Generative Models'\n",
        "        (Kingma 2014) in PyTorch.\n",
        "        The \"Generative semi-supervised model\" is a probabilistic\n",
        "        model that incorporates label information in both\n",
        "        inference and generation.\n",
        "        Initialise a new generative model\n",
        "        :param dims: dimensions of x, y, z and hidden layers.\n",
        "        \"\"\"\n",
        "        super(DeepGenerativeModel, self).__init__([dims[0], dims[2], dims[3]])\n",
        "        [x_dim, self.y_dim, z_dim, h_dim] = dims\n",
        "        \n",
        "\n",
        "        self.encoder = Encoder([x_dim + self.y_dim, h_dim, z_dim])\n",
        "        self.decoder = Decoder([z_dim + self.y_dim, list(reversed(h_dim)), x_dim])\n",
        "        self.classifier = Classifier([x_dim, h_dim[0], self.y_dim])\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "              init.xavier_normal_(m.weight.data)\n",
        "              if m.bias is not None:\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    \"\"\"\n",
        "    Compute KLD on gaussian sample - inference\n",
        "    \"\"\"\n",
        "    def _kld(self,x,q_param):\n",
        "      (mu, log_var) = q_param\n",
        "      log_pdf = - 0.5 * math.log(2 * math.pi) - log_var / 2 - (x - mu)**2 / (2 * torch.exp(log_var))\n",
        "      return torch.sum(log_pdf, dim=-1)\n",
        "      \n",
        "    def forward(self, x, y):\n",
        "        # Add label and data and generate latent variable\n",
        "        z, z_mu, z_log_var = self.encoder(torch.cat([x, y], dim=1))\n",
        "\n",
        "        self.kl_divergence = self._kld(z, (z_mu, z_log_var))\n",
        "\n",
        "        # Reconstruct data point from latent data and label\n",
        "        x_mu = self.decoder(torch.cat([z, y], dim=1))\n",
        "\n",
        "        return x_mu\n",
        "\n",
        "    def classify(self, x):\n",
        "        logits = self.classifier(x)\n",
        "        return logits\n",
        "\n",
        "    def sample(self, z, y):\n",
        "        \"\"\"\n",
        "        Samples from the Decoder to generate an x.\n",
        "        :param z: latent normal variable\n",
        "        :param y: label (one-hot encoded)\n",
        "        :return: x\n",
        "        \"\"\"\n",
        "        y = y.float()\n",
        "        x = self.decoder(torch.cat([z, y], dim=1))\n",
        "        return x\n",
        "\n",
        "\"\"\"\n",
        "Joint loss over all the parameters\n",
        "\"\"\"\n",
        "def loss_function(recon_x, x, mu, logvar):\n",
        "  BCE = F.binary_cross_entropy(recon_x,x.view(-1,784),reduction=\"sum\")\n",
        "  # https://arxiv.org/abs/1312.6114\n",
        "  # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
        "  KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "  \n",
        "  return BCE + KLD\n",
        "  \n",
        "y_dim = 10\n",
        "z_dim = 32\n",
        "h_dim = [256, 128]\n",
        "\n",
        "model = DeepGenerativeModel([784, y_dim, z_dim, h_dim]).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "\n",
        "model\n",
        "    "
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeepGenerativeModel(\n",
              "  (encoder): Encoder(\n",
              "    (hidden): ModuleList(\n",
              "      (0): Linear(in_features=794, out_features=256, bias=True)\n",
              "      (1): Linear(in_features=256, out_features=128, bias=True)\n",
              "    )\n",
              "    (sample): GaussianSample(\n",
              "      (mu): Linear(in_features=128, out_features=32, bias=True)\n",
              "      (log_var): Linear(in_features=128, out_features=32, bias=True)\n",
              "    )\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (hidden): ModuleList(\n",
              "      (0): Linear(in_features=42, out_features=128, bias=True)\n",
              "      (1): Linear(in_features=128, out_features=256, bias=True)\n",
              "    )\n",
              "    (reconstruction): Linear(in_features=256, out_features=784, bias=True)\n",
              "  )\n",
              "  (classifier): Classifier(\n",
              "    (fc1): Linear(in_features=784, out_features=256, bias=True)\n",
              "    (fc2): Linear(in_features=256, out_features=10, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "metadata": {
        "id": "rJdaHuhhcSOI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Train\n",
        "\"\"\"\n",
        "def train(epoch):\n",
        "  model.train()\n",
        "  train_loss = 0\n",
        "  accuracy = RunningAverage()\n",
        "  for batch_idx, (data, labels) in enumerate(train_loader):\n",
        "    data = data.to(device)\n",
        "    labels = labels.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    batch_size = data.size(0)\n",
        "    recon_batch, mu, logvar = model(data)\n",
        "    loss = loss_function(recon_batch, data, mu, logvar)\n",
        "    loss.backward()\n",
        "    \n",
        "    train_loss+=loss.item()\n",
        "    \n",
        "    \n",
        "    ## accuracy\n",
        "    _, idx = recon_batch.topk(1,dim=1)\n",
        "    accuracy.update(torch.sum(idx.view(batch_size) == labels).item())\n",
        "    print(idx)    \n",
        "    \n",
        "    optimizer.step()\n",
        "    if batch_idx % params.get(\"log_interval\") == 0:\n",
        "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader),\n",
        "                loss.item() / len(data)))\n",
        "    tl = train_loss/len(train_loader.dataset)\n",
        "    print(\"==> Epoch {}, average loss: {:.4f},average accuracy: {:.4f}\".format(epoch, tl,accuracy()))\n",
        "    \n",
        "    return (tl,accuracy)\n",
        "\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qfQLtE5kdPYY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Test\n",
        "\"\"\"\n",
        "def test(epoch):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    test_accuracy = RunningAverage()\n",
        "    with torch.no_grad():\n",
        "        for i, (data,labels) in enumerate(test_loader):\n",
        "            data = data.to(device)\n",
        "            labels = labels.to(device)\n",
        "            recon_batch, mu, logvar = model(data)\n",
        "            batch_size = data.size(0)\n",
        "            test_loss += loss_function(recon_batch, data, mu, logvar).item()\n",
        "            ## accuracy\n",
        "            _, idx = recon_batch.topk(1,dim=1)\n",
        "            test_accuracy.update(torch.sum(idx.view(batch_size) == labels).item())\n",
        "        \n",
        "            \n",
        "            if i == 0:\n",
        "                n = min(data.size(0), 8)\n",
        "                comparison = torch.cat([data[:n],\n",
        "                                      recon_batch.view(params.get(\"batch_size\"), 1, 28, 28)[:n]])\n",
        "                save_image(comparison.cpu(),\n",
        "                         './results/reconstruction_' + str(epoch) + '.png', nrow=n)\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print('====> Test set loss: {:.4f}, test accuracy: {:.4f}'.format(test_loss,test_accuracy()))\n",
        "    return (test_loss,test_accuracy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mXS9GUuSdyy-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2244
        },
        "outputId": "06d9abc9-95e6-447a-858b-fbc5e3780091"
      },
      "cell_type": "code",
      "source": [
        "train_losses = []\n",
        "test_losses = []\n",
        "train_acc_list = []\n",
        "test_acc_list = []\n",
        "for epoch in range(1, 2):\n",
        "        trainloss,train_acc = train(epoch)\n",
        "        testloss,test_acc = test(epoch)\n",
        "        train_losses.append(trainloss)\n",
        "        test_losses.append(testloss)\n",
        "        train_acc_list.append(train_acc)\n",
        "        test_acc_list.append(test_acc)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            sample = torch.randn(64, 20).to(device)\n",
        "            sample = model.decode(sample).cpu()\n",
        "            save_image(sample.view(64, 1, 28, 28),\n",
        "                       'results/sample_' + str(epoch) + '.png')"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[407],\n",
            "        [408],\n",
            "        [408],\n",
            "        [407],\n",
            "        [408],\n",
            "        [434],\n",
            "        [407],\n",
            "        [407],\n",
            "        [408],\n",
            "        [407],\n",
            "        [407],\n",
            "        [407],\n",
            "        [407],\n",
            "        [407],\n",
            "        [408],\n",
            "        [434],\n",
            "        [407],\n",
            "        [407],\n",
            "        [408],\n",
            "        [407],\n",
            "        [408],\n",
            "        [407],\n",
            "        [407],\n",
            "        [407],\n",
            "        [407],\n",
            "        [407],\n",
            "        [408],\n",
            "        [407],\n",
            "        [407],\n",
            "        [408],\n",
            "        [408],\n",
            "        [407],\n",
            "        [408],\n",
            "        [407],\n",
            "        [408],\n",
            "        [434],\n",
            "        [407],\n",
            "        [407],\n",
            "        [408],\n",
            "        [408],\n",
            "        [407],\n",
            "        [407],\n",
            "        [407],\n",
            "        [407],\n",
            "        [407],\n",
            "        [407],\n",
            "        [408],\n",
            "        [407],\n",
            "        [407],\n",
            "        [407],\n",
            "        [407],\n",
            "        [408],\n",
            "        [407],\n",
            "        [408],\n",
            "        [407],\n",
            "        [407],\n",
            "        [408],\n",
            "        [407],\n",
            "        [407],\n",
            "        [407],\n",
            "        [407],\n",
            "        [408],\n",
            "        [407],\n",
            "        [407],\n",
            "        [407],\n",
            "        [407],\n",
            "        [407],\n",
            "        [407],\n",
            "        [408],\n",
            "        [407],\n",
            "        [407],\n",
            "        [407],\n",
            "        [408],\n",
            "        [407],\n",
            "        [407],\n",
            "        [407],\n",
            "        [407],\n",
            "        [408],\n",
            "        [407],\n",
            "        [407],\n",
            "        [407],\n",
            "        [407],\n",
            "        [408],\n",
            "        [407],\n",
            "        [408],\n",
            "        [407],\n",
            "        [407],\n",
            "        [434],\n",
            "        [408],\n",
            "        [407],\n",
            "        [407],\n",
            "        [407],\n",
            "        [408],\n",
            "        [408],\n",
            "        [407],\n",
            "        [408],\n",
            "        [407],\n",
            "        [407],\n",
            "        [407],\n",
            "        [408],\n",
            "        [407],\n",
            "        [408],\n",
            "        [407],\n",
            "        [407],\n",
            "        [408],\n",
            "        [407],\n",
            "        [407],\n",
            "        [407],\n",
            "        [407],\n",
            "        [407],\n",
            "        [407],\n",
            "        [408],\n",
            "        [407],\n",
            "        [407],\n",
            "        [408],\n",
            "        [408],\n",
            "        [408],\n",
            "        [407],\n",
            "        [407],\n",
            "        [407],\n",
            "        [407],\n",
            "        [407],\n",
            "        [407],\n",
            "        [408],\n",
            "        [408],\n",
            "        [407],\n",
            "        [407],\n",
            "        [407]], device='cuda:0')\n",
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 214.258728\n",
            "==> Epoch 1, average loss: 0.4571,average accuracy: 0.0000\n",
            "====> Test set loss: 216.8492, test accuracy: 0.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3x7BxjQGeGSu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.makedirs(\"./results\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iaqMNHkzestE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}