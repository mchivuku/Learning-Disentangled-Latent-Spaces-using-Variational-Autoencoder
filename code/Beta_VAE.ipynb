{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Beta-VAE.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mchivuku/csb659-project/blob/master/Beta_VAE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "irybfVlgCOOw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Beta Variational Auto-Encoder:\n",
        "https://openreview.net/forum?id=Sy2fzU9gl"
      ]
    },
    {
      "metadata": {
        "id": "mQx0i7KACiD3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Installations"
      ]
    },
    {
      "metadata": {
        "id": "jnOpjjPLCWlw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install tqdm six"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0r6vIjMlCokA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Connect to Google Drive"
      ]
    },
    {
      "metadata": {
        "id": "5CgKXRqKCjXo",
        "colab_type": "code",
        "outputId": "22944433-caf5-4433-82ae-0f5003c12b0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "\n",
        "%cd /content/drive/My\\ Drive/Masters-DS/CSCI-B659/project/\n",
        "%ls"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/My Drive/Masters-DS/CSCI-B659/project\n",
            "\u001b[0m\u001b[01;34mdata\u001b[0m/  \u001b[01;34mdsprites-dataset\u001b[0m/  \u001b[01;34mexamples\u001b[0m/  \u001b[01;34mresults\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "OObtlmpIC11I",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Import"
      ]
    },
    {
      "metadata": {
        "id": "riHsQru5C1Hx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.utils.data\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0YrYqQtgDeGS",
        "colab_type": "code",
        "outputId": "ac57462e-3cab-41ec-a523-45c8e71e95b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/dawnJ/dsprites-dataset.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'dsprites-dataset'...\n",
            "remote: Enumerating objects: 16, done.\u001b[K\n",
            "remote: Total 16 (delta 0), reused 0 (delta 0), pack-reused 16\u001b[K\n",
            "Unpacking objects: 100% (16/16), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ihCazNpJC6wY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Data"
      ]
    },
    {
      "metadata": {
        "id": "Jdf0eAd8CqQo",
        "colab_type": "code",
        "outputId": "7c778dd2-2d24-4e3e-97fd-aac5c54fd8ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        }
      },
      "cell_type": "code",
      "source": [
        "# Load dataset\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        " \n",
        "\n",
        "# Change figure aesthetics\n",
        "%matplotlib inline\n",
        "sns.set_context('talk', font_scale=1.2, rc={'lines.linewidth': 1.5})\n",
        "\n",
        "# Load dataset\n",
        "dataset_zip = np.load('dsprites-dataset/dsprites_ndarray_co1sh3sc6or40x32y32_64x64.npz', encoding='latin1')\n",
        "\n",
        "print('Keys in the dataset:', dataset_zip.keys())\n",
        "imgs = dataset_zip['imgs']\n",
        "latents_values = dataset_zip['latents_values']\n",
        "latents_classes = dataset_zip['latents_classes']\n",
        "metadata = dataset_zip['metadata'][()]\n",
        "\n",
        "print('Metadata: \\n', metadata)\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Keys in the dataset: ['metadata', 'imgs', 'latents_classes', 'latents_values']\n",
            "Metadata: \n",
            " {'date': 'April 2017', 'description': 'Disentanglement test Sprites dataset.Procedurally generated 2D shapes, from 6 disentangled latent factors.This dataset uses 6 latents, controlling the color, shape, scale, rotation and position of a sprite. All possible variations of the latents are present. Ordering along dimension 1 is fixed and can be mapped back to the exact latent values that generated that image.We made sure that the pixel outputs are different. No noise added.', 'version': 1, 'latents_names': ('color', 'shape', 'scale', 'orientation', 'posX', 'posY'), 'latents_possible_values': {'orientation': array([0.        , 0.16110732, 0.32221463, 0.48332195, 0.64442926,\n",
            "       0.80553658, 0.96664389, 1.12775121, 1.28885852, 1.44996584,\n",
            "       1.61107316, 1.77218047, 1.93328779, 2.0943951 , 2.25550242,\n",
            "       2.41660973, 2.57771705, 2.73882436, 2.89993168, 3.061039  ,\n",
            "       3.22214631, 3.38325363, 3.54436094, 3.70546826, 3.86657557,\n",
            "       4.02768289, 4.1887902 , 4.34989752, 4.51100484, 4.67211215,\n",
            "       4.83321947, 4.99432678, 5.1554341 , 5.31654141, 5.47764873,\n",
            "       5.63875604, 5.79986336, 5.96097068, 6.12207799, 6.28318531]), 'posX': array([0.        , 0.03225806, 0.06451613, 0.09677419, 0.12903226,\n",
            "       0.16129032, 0.19354839, 0.22580645, 0.25806452, 0.29032258,\n",
            "       0.32258065, 0.35483871, 0.38709677, 0.41935484, 0.4516129 ,\n",
            "       0.48387097, 0.51612903, 0.5483871 , 0.58064516, 0.61290323,\n",
            "       0.64516129, 0.67741935, 0.70967742, 0.74193548, 0.77419355,\n",
            "       0.80645161, 0.83870968, 0.87096774, 0.90322581, 0.93548387,\n",
            "       0.96774194, 1.        ]), 'posY': array([0.        , 0.03225806, 0.06451613, 0.09677419, 0.12903226,\n",
            "       0.16129032, 0.19354839, 0.22580645, 0.25806452, 0.29032258,\n",
            "       0.32258065, 0.35483871, 0.38709677, 0.41935484, 0.4516129 ,\n",
            "       0.48387097, 0.51612903, 0.5483871 , 0.58064516, 0.61290323,\n",
            "       0.64516129, 0.67741935, 0.70967742, 0.74193548, 0.77419355,\n",
            "       0.80645161, 0.83870968, 0.87096774, 0.90322581, 0.93548387,\n",
            "       0.96774194, 1.        ]), 'scale': array([0.5, 0.6, 0.7, 0.8, 0.9, 1. ]), 'shape': array([1., 2., 3.]), 'color': array([1.])}, 'latents_sizes': array([ 1,  3,  6, 40, 32, 32]), 'author': 'lmatthey@google.com', 'title': 'dSprites dataset'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tYXksExFC_oo",
        "colab_type": "code",
        "outputId": "81be751e-9ede-4475-aae4-50b0f7854386",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 846
        }
      },
      "cell_type": "code",
      "source": [
        "# Helper function to show images\n",
        "\n",
        "# Define number of values per latents and functions to convert to indices\n",
        "latents_sizes = metadata['latents_sizes']\n",
        "latents_bases = np.concatenate((latents_sizes[::-1].cumprod()[::-1][1:],\n",
        "                                np.array([1,])))\n",
        "\n",
        "def latent_to_index(latents):\n",
        "  return np.dot(latents, latents_bases).astype(int)\n",
        "\n",
        "\n",
        "def show_images_grid(imgs_, num_images=25):\n",
        "  ncols = int(np.ceil(num_images**0.5))\n",
        "  nrows = int(np.ceil(num_images / ncols))\n",
        "  _, axes = plt.subplots(ncols, nrows, figsize=(nrows * 3, ncols * 3))\n",
        "  axes = axes.flatten()\n",
        "\n",
        "  for ax_i, ax in enumerate(axes):\n",
        "    if ax_i < num_images:\n",
        "      ax.imshow(imgs_[ax_i], cmap='Greys_r',  interpolation='nearest')\n",
        "      ax.set_xticks([])\n",
        "      ax.set_yticks([])\n",
        "    else:\n",
        "      ax.axis('off')\n",
        "      \n",
        "def sample_latent(size=1):\n",
        "  samples = np.zeros((size, latents_sizes.size))\n",
        "  for lat_i, lat_size in enumerate(latents_sizes):\n",
        "    samples[:, lat_i] = np.random.randint(lat_size, size=size)\n",
        "\n",
        "  return samples\n",
        "\n",
        "# Sample latents randomly\n",
        "latents_sampled = sample_latent(size=200)\n",
        "\n",
        "# Select images\n",
        "indices_sampled = latent_to_index(latents_sampled)\n",
        "imgs_sampled = imgs[indices_sampled]\n",
        "\n",
        "# Show images\n",
        "show_images_grid(imgs_sampled)\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA08AAAM9CAYAAABE1EjQAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEX7l4N8AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 1080x1080 with 25 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "1AJV3b0UFG3L",
        "colab_type": "code",
        "outputId": "55de44b1-3bcc-4308-be91-7298a90fdbe6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "print(\"Total number of images\",len(imgs))\n",
        "\n",
        "imgs = imgs[:2000]"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total number of images 737280\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PWQdjzaqGJd6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#print(\"Latent\",latents_sizes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NqF4qsXPG54k",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## DataLoader"
      ]
    },
    {
      "metadata": {
        "id": "FQdmk2jvG5NE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9mQyHo5HGVby",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "params = {\n",
        "    \"batch_size\":128,\n",
        "    \"eta\":1e-2, # learning rate for adagrad\n",
        "    \"beta\":4,\n",
        "    \"nb_latents\":10,\n",
        "    \"epochs\":100,\n",
        "    \"seed\":1,\n",
        "    \"log_interval\":100,\n",
        "    \"save_interval\":1000\n",
        "\n",
        "}\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "\n",
        "class SpritesDataset(Dataset):\n",
        "  def __init__(self):\n",
        "    dataset_zip = np.load('dsprites-dataset/dsprites_ndarray_co1sh3sc6or40x32y32_64x64.npz', encoding='latin1')\n",
        "    self.imgs = dataset_zip['imgs']\n",
        "    \n",
        "  def __len__(self):\n",
        "    return self.imgs.shape[0]\n",
        "  \n",
        "  def __getitem__(self,idx):\n",
        "    return self.imgs[idx].astype(np.float32)\n",
        "  \n",
        "\"\"\"\n",
        "Construct dataloader to return images\n",
        "\"\"\"\n",
        "def get_dataloader(batchsize,shuffle=True):\n",
        "  sprites_dataset = SpritesDataset()\n",
        "  return DataLoader(sprites_dataset, batch_size=params.get(\"batch_size\"), shuffle=shuffle)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "c6wI4_mgHxV8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Models"
      ]
    },
    {
      "metadata": {
        "id": "AJfhiKHXHwtD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## DenseVAE\n",
        "\n",
        "from torch import nn\n",
        "from torch.autograd import Variable\n",
        "\n",
        "class DenseVAE(nn.Module):\n",
        "  def __init__(self,nb_latents):\n",
        "    super(DenseVAE,self).__init__()\n",
        "    \n",
        "    ## Define network\n",
        "    self.fc1 = nn.Linear(4096,1200)\n",
        "    self.fc2 = nn.Linear(1200,1200)\n",
        "    \n",
        "    ## encode\n",
        "    self.fc_mean = nn.Linear(1200, nb_latents)\n",
        "    self.fc_std = nn.Linear(1200,nb_latents)\n",
        "    \n",
        "    # decode\n",
        "    self.fc3 = nn.Linear(nb_latents,1200)\n",
        "    self.fc4 = nn.Linear(1200, 1200)\n",
        "    self.fc5 = nn.Linear(1200,1200)\n",
        "    self.fc6 = nn.Linear(1200,4096)\n",
        "    \n",
        "    self.relu = nn.ReLU()\n",
        "    self.tanh = nn.Tanh()\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "  \n",
        "  \"\"\"\n",
        "  ENCODE INTO TWO LATENT VECTORS - MEAN AND STD\n",
        "  \"\"\"\n",
        "  def encode(self,x):\n",
        "    x = self.relu(self.fc1(x))\n",
        "    x = self.relu(self.fc2(x))\n",
        "    \n",
        "    \n",
        "    return self.fc_mean(x), self.fc_std(x)\n",
        "  \n",
        "  \"\"\"\n",
        "  Reparametrize \n",
        "  \"\"\"\n",
        "  def reparametrize(self, mu, logvar):\n",
        "    if self.training:\n",
        "      std = logvar.mul(0.5).exp_()\n",
        "      eps = Variable(std.data.new(std.size()).normal_())\n",
        "      return eps.mul(std).add_(mu)\n",
        "    \n",
        "    else:\n",
        "      return mu\n",
        "    \n",
        "    \n",
        "  \"\"\"\n",
        "  Decode\n",
        "  \"\"\"\n",
        "  def decode(self,z):\n",
        "    x = self.tanh(self.fc3(z))\n",
        "    x = self.tanh(self.fc4(x))\n",
        "    x = self.tanh(self.fc5(x))\n",
        "    return self.sigmoid(self.fc6(x))\n",
        "  \n",
        "  \n",
        "  \"\"\"\n",
        "  Forward\n",
        "  \"\"\"\n",
        "  def forward(self,x):\n",
        "    mu, logvar = self.encode(x.view(-1,64*64))\n",
        "    z = self.reparametrize(mu,logvar)\n",
        "    return self.decode(z),mu, logvar\n",
        "  \n",
        "  \n",
        "  \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Hz7K_pw4HrtT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class ConvVAE(nn.Module):\n",
        "    def __init__(self, nb_latents):\n",
        "        super(ConvVAE, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=4, stride=2, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 32, kernel_size=4, stride=2, padding=1)\n",
        "        self.conv3 = nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1)\n",
        "        self.conv4 = nn.Conv2d(64, 64, kernel_size=4, stride=2, padding=1)\n",
        "        self.fc1 = nn.Linear(64*4*4, 256)\n",
        "        \n",
        "        self.fc_mean = nn.Linear(256, nb_latents)\n",
        "        self.fc_std = nn.Linear(256, nb_latents)\n",
        "        \n",
        "        self.fc2 = nn.Linear(nb_latents, 256)\n",
        "        self.fc3 = nn.Linear(256, 64*4*4)\n",
        "        self.deconv1 = nn.ConvTranspose2d(64, 64, kernel_size=4, stride=2, padding=1)\n",
        "        self.deconv2 = nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1)\n",
        "        self.deconv3 = nn.ConvTranspose2d(32, 32, kernel_size=4, stride=2, padding=1)\n",
        "        self.deconv4 = nn.ConvTranspose2d(32, 1, kernel_size=4, stride=2, padding=1)\n",
        "    \n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "    \n",
        "    def encode(self, x):\n",
        "        x = self.relu(self.conv1(x))\n",
        "        x = self.relu(self.conv2(x))\n",
        "        x = self.relu(self.conv3(x))\n",
        "        x = self.relu(self.conv4(x))\n",
        "        x = self.relu(self.fc1(x.view(-1, 64*4*4)))\n",
        "        return self.fc_mean(x), self.fc_std(x)\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        if self.training:\n",
        "            std = logvar.mul(0.5).exp_()\n",
        "            eps = Variable(std.data.new(std.size()).normal_())\n",
        "            return eps.mul(std).add_(mu)\n",
        "        else:\n",
        "            return mu\n",
        "    \n",
        "    def decode(self, z):\n",
        "        x = self.relu(self.fc2(z))\n",
        "        x = self.relu(self.fc3(x))\n",
        "        x = self.relu(self.deconv1(x.view(-1, 64, 4, 4)))\n",
        "        x = self.relu(self.deconv2(x))\n",
        "        x = self.relu(self.deconv3(x))\n",
        "        return self.sigmoid(self.deconv4(x))\n",
        "        \n",
        "    def forward(self, x):\n",
        "        mu, logvar = self.encode(x.unsqueeze(1))\n",
        "        z = self.reparameterize(mu, logvar)\n",
        "        return self.decode(z), mu, logvar"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "n2ew4BFOKDBn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training"
      ]
    },
    {
      "metadata": {
        "id": "oQLK5tm5JwzH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## make directory results if it doesnt exist\n",
        "import os \n",
        "import shutil\n",
        "\n",
        "import time\n",
        "\n",
        "\n",
        "\n",
        "def make_results():\n",
        "  if os.path.isdir(\"results\"):\n",
        "    shutil.rmtree(\"results\")\n",
        "  os.makedirs(\"results\")\n",
        "  \n",
        "  \n",
        "def loss_function(recon_x, x, mu, logvar,beta):\n",
        "  \"\"\"\n",
        "  Reconstruction loss  + KL divergence loss over all elements of the batch\n",
        "  \"\"\"\n",
        "  bce = F.binary_cross_entropy(recon_x, x.view(-1, 64*64), size_average=False)\n",
        "  \n",
        "  kld = -0.5* (1+ logvar -mu.pow(2) -  logvar.exp())\n",
        "  return kld.mean(dim = 0), bce + beta*kld.sum()\n",
        "  \n",
        "\"\"\"\n",
        "Traverse Latents\n",
        "\"\"\"\n",
        "def traverse_latents(model, datapoint, nb_latents, epoch, batch_idx, dirpath=\"results\"):\n",
        "  model.eval()\n",
        "  \n",
        "  if isinstance(model,ConvVAE):\n",
        "    datapoint = datapoint.unsqueeze(0).unsqueeze(1)\n",
        "    mu, _ = model.encode(datapoint)\n",
        "  else:\n",
        "    mu, _ = model.encode(datapoint.view(-1))\n",
        "  \n",
        "  recons = torch.zeros((7, nb_latents, 64, 64))\n",
        "  for zi in range(nb_latents):\n",
        "    muc = mu.squeeze().clone()\n",
        "    for i, val in enumerate(np.linspace(-3, 3, 7)):\n",
        "      muc[zi] = val\n",
        "      recon = model.decode(muc).cpu()\n",
        "      recons[i, zi] = recon.view(64, 64)\n",
        "\n",
        "  filename = os.path.join(dirpath, 'traversal_' + str(epoch) + '_' + str(batch_idx) + '.png')\n",
        "  save_image(recons.view(-1, 1, 64, 64), filename, nrow=nb_latents, pad_value=1)\n",
        "  \n",
        "  \n",
        "  \n",
        "def train():\n",
        "  ## make results dir - if it doesnt exist\n",
        "  make_results()\n",
        "  \n",
        "  ## dataloader\n",
        "  dataloader = get_dataloader(params.get(\"batch_size\"))\n",
        "  testpoint = torch.Tensor(dataloader.dataset[0])\n",
        "  \n",
        "  \n",
        "  model = DenseVAE(params.get(\"nb_latents\"))\n",
        "  optimizer = optim.Adagrad(model.parameters(), lr=params.get(\"eta\"))\n",
        "\n",
        "  model.train()\n",
        "  \n",
        "  runningloss, runningkld = None, np.array([])\n",
        "  start_time = time.time()\n",
        "  \n",
        "  for epoch in range(1, params.get(\"epochs\")+1):\n",
        "    for batch_idx, data in enumerate(dataloader):\n",
        "      recon_batch, mu, logvar = model(data)\n",
        "      \n",
        "      kld, loss = loss_function(recon_batch, data, mu, logvar,params.get(\"beta\"))\n",
        "      \n",
        "      \n",
        "      ## parameter update\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      \n",
        "      loss /=len(data)\n",
        "      runningloss = loss if not runningloss else runningloss*0.99 + loss*0.01\n",
        "      \n",
        "      runningkld =  np.zeros(params.get(\"nb_latents\")) if not len(runningkld) else runningkld*0.99 + kld.data.cpu().numpy()*0.01\n",
        "      if not batch_idx % params.get(\"log_interval\"):\n",
        "                print(\"Epoch {}, batch: {}/{} ({:.2f} s), loss: {:.2f}, kl: [{}]\".format(\n",
        "                    epoch, batch_idx, len(dataloader), time.time() - start_time, runningloss, \n",
        "                    \", \".join(\"{:.2f}\".format(kl) for kl in runningkld)))\n",
        "                start_time = time.time()\n",
        "\n",
        "      if not batch_idx % params.get(\"save_interval\"):\n",
        "          traverse_latents(model, testpoint, params.get(\"nb_latents\"), epoch, batch_idx)\n",
        "          model.train()\n",
        "\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oszo8GvmN3Nh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "9c08cd94-ca0e-4b8d-8098-257e236c20da"
      },
      "cell_type": "code",
      "source": [
        "params"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'batch_size': 128,\n",
              " 'beta': 4,\n",
              " 'epochs': 100,\n",
              " 'eta': 0.01,\n",
              " 'log_interval': 100,\n",
              " 'nb_latents': 10,\n",
              " 'save_interval': 1000,\n",
              " 'seed': 1}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "id": "NdeBsVvkOJSR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 21508
        },
        "outputId": "a883ed49-258d-4da5-c567-019846ec65b4"
      },
      "cell_type": "code",
      "source": [
        "train()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1, batch: 0/5760 (0.45 s), loss: 2842.03, kl: [0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00]\n",
            "Epoch 1, batch: 100/5760 (36.12 s), loss: 1600.24, kl: [5.82, 4.64, 9.43, 5.10, 3.91, 5.19, 2.71, 6.21, 5.94, 4.21]\n",
            "Epoch 1, batch: 200/5760 (35.40 s), loss: 790.05, kl: [2.96, 2.40, 4.06, 2.23, 1.79, 2.60, 1.37, 3.34, 2.41, 2.23]\n",
            "Epoch 1, batch: 300/5760 (35.24 s), loss: 461.20, kl: [2.10, 1.77, 2.14, 1.22, 0.93, 1.71, 0.79, 2.57, 1.10, 1.69]\n",
            "Epoch 1, batch: 400/5760 (35.32 s), loss: 319.81, kl: [1.83, 1.79, 1.49, 0.86, 0.48, 1.43, 0.45, 2.48, 0.58, 1.58]\n",
            "Epoch 1, batch: 500/5760 (35.60 s), loss: 253.97, kl: [1.72, 1.95, 1.24, 0.70, 0.24, 1.24, 0.24, 2.58, 0.33, 1.62]\n",
            "Epoch 1, batch: 600/5760 (35.42 s), loss: 221.65, kl: [1.68, 2.04, 1.11, 0.61, 0.12, 1.12, 0.14, 2.67, 0.19, 1.68]\n",
            "Epoch 1, batch: 700/5760 (34.99 s), loss: 203.91, kl: [1.67, 2.10, 1.03, 0.49, 0.07, 0.99, 0.09, 2.75, 0.10, 1.77]\n",
            "Epoch 1, batch: 800/5760 (35.13 s), loss: 193.26, kl: [1.66, 2.16, 0.96, 0.37, 0.04, 0.90, 0.06, 2.81, 0.06, 1.82]\n",
            "Epoch 1, batch: 900/5760 (35.24 s), loss: 185.54, kl: [1.66, 2.19, 0.90, 0.25, 0.03, 0.81, 0.04, 2.86, 0.04, 1.87]\n",
            "Epoch 1, batch: 1000/5760 (34.94 s), loss: 179.49, kl: [1.66, 2.21, 0.82, 0.16, 0.03, 0.73, 0.03, 2.90, 0.02, 1.88]\n",
            "Epoch 1, batch: 1100/5760 (35.60 s), loss: 175.45, kl: [1.66, 2.24, 0.72, 0.10, 0.02, 0.67, 0.03, 2.91, 0.02, 1.92]\n",
            "Epoch 1, batch: 1200/5760 (35.31 s), loss: 172.02, kl: [1.67, 2.27, 0.62, 0.06, 0.02, 0.59, 0.02, 2.93, 0.01, 1.97]\n",
            "Epoch 1, batch: 1300/5760 (35.02 s), loss: 168.89, kl: [1.68, 2.27, 0.53, 0.04, 0.02, 0.55, 0.02, 2.92, 0.01, 1.99]\n",
            "Epoch 1, batch: 1400/5760 (35.47 s), loss: 166.60, kl: [1.70, 2.29, 0.45, 0.03, 0.01, 0.49, 0.01, 2.94, 0.01, 1.98]\n",
            "Epoch 1, batch: 1500/5760 (34.95 s), loss: 164.59, kl: [1.70, 2.29, 0.40, 0.02, 0.01, 0.43, 0.01, 2.95, 0.01, 2.01]\n",
            "Epoch 1, batch: 1600/5760 (34.92 s), loss: 162.79, kl: [1.70, 2.31, 0.34, 0.02, 0.01, 0.37, 0.01, 2.96, 0.01, 2.04]\n",
            "Epoch 1, batch: 1700/5760 (34.90 s), loss: 161.57, kl: [1.73, 2.32, 0.29, 0.01, 0.01, 0.33, 0.01, 2.95, 0.00, 2.05]\n",
            "Epoch 1, batch: 1800/5760 (35.01 s), loss: 159.86, kl: [1.72, 2.33, 0.24, 0.01, 0.01, 0.29, 0.01, 2.95, 0.00, 2.06]\n",
            "Epoch 1, batch: 1900/5760 (35.51 s), loss: 159.33, kl: [1.70, 2.33, 0.20, 0.01, 0.01, 0.26, 0.01, 2.97, 0.00, 2.07]\n",
            "Epoch 1, batch: 2000/5760 (35.17 s), loss: 157.21, kl: [1.70, 2.33, 0.16, 0.01, 0.01, 0.24, 0.00, 2.97, 0.00, 2.07]\n",
            "Epoch 1, batch: 2100/5760 (35.71 s), loss: 156.29, kl: [1.71, 2.33, 0.12, 0.01, 0.00, 0.21, 0.00, 2.96, 0.00, 2.08]\n",
            "Epoch 1, batch: 2200/5760 (35.13 s), loss: 155.55, kl: [1.69, 2.34, 0.09, 0.01, 0.00, 0.19, 0.00, 2.96, 0.00, 2.10]\n",
            "Epoch 1, batch: 2300/5760 (35.47 s), loss: 155.15, kl: [1.68, 2.35, 0.06, 0.01, 0.00, 0.18, 0.00, 2.97, 0.00, 2.09]\n",
            "Epoch 1, batch: 2400/5760 (34.98 s), loss: 154.18, kl: [1.66, 2.35, 0.05, 0.01, 0.00, 0.15, 0.00, 2.98, 0.00, 2.09]\n",
            "Epoch 1, batch: 2500/5760 (35.09 s), loss: 153.09, kl: [1.63, 2.35, 0.03, 0.01, 0.00, 0.13, 0.00, 2.98, 0.00, 2.10]\n",
            "Epoch 1, batch: 2600/5760 (34.90 s), loss: 152.88, kl: [1.63, 2.36, 0.02, 0.00, 0.00, 0.12, 0.00, 2.98, 0.00, 2.10]\n",
            "Epoch 1, batch: 2700/5760 (35.10 s), loss: 152.58, kl: [1.62, 2.37, 0.02, 0.00, 0.00, 0.11, 0.00, 2.98, 0.00, 2.11]\n",
            "Epoch 1, batch: 2800/5760 (35.34 s), loss: 152.04, kl: [1.61, 2.36, 0.02, 0.00, 0.00, 0.10, 0.00, 2.99, 0.00, 2.10]\n",
            "Epoch 1, batch: 2900/5760 (34.97 s), loss: 151.42, kl: [1.60, 2.38, 0.01, 0.00, 0.00, 0.09, 0.00, 2.98, 0.00, 2.10]\n",
            "Epoch 1, batch: 3000/5760 (35.01 s), loss: 151.13, kl: [1.59, 2.36, 0.01, 0.00, 0.00, 0.08, 0.00, 2.98, 0.00, 2.09]\n",
            "Epoch 1, batch: 3100/5760 (35.56 s), loss: 150.31, kl: [1.58, 2.37, 0.01, 0.00, 0.00, 0.08, 0.00, 2.98, 0.00, 2.09]\n",
            "Epoch 1, batch: 3200/5760 (35.56 s), loss: 150.35, kl: [1.59, 2.36, 0.01, 0.00, 0.00, 0.07, 0.00, 2.98, 0.00, 2.09]\n",
            "Epoch 1, batch: 3300/5760 (35.07 s), loss: 149.91, kl: [1.59, 2.37, 0.01, 0.00, 0.00, 0.07, 0.00, 2.98, 0.00, 2.09]\n",
            "Epoch 1, batch: 3400/5760 (34.77 s), loss: 149.51, kl: [1.57, 2.38, 0.01, 0.00, 0.00, 0.06, 0.00, 2.99, 0.00, 2.09]\n",
            "Epoch 1, batch: 3500/5760 (35.28 s), loss: 149.02, kl: [1.57, 2.38, 0.01, 0.00, 0.00, 0.06, 0.00, 3.00, 0.00, 2.08]\n",
            "Epoch 1, batch: 3600/5760 (35.11 s), loss: 149.14, kl: [1.58, 2.37, 0.01, 0.00, 0.00, 0.05, 0.00, 2.99, 0.00, 2.07]\n",
            "Epoch 1, batch: 3700/5760 (35.33 s), loss: 149.30, kl: [1.57, 2.39, 0.01, 0.00, 0.00, 0.05, 0.00, 2.99, 0.00, 2.08]\n",
            "Epoch 1, batch: 3800/5760 (35.21 s), loss: 148.95, kl: [1.59, 2.39, 0.01, 0.00, 0.00, 0.05, 0.00, 2.99, 0.00, 2.08]\n",
            "Epoch 1, batch: 3900/5760 (35.09 s), loss: 148.84, kl: [1.57, 2.39, 0.00, 0.00, 0.00, 0.04, 0.00, 2.99, 0.00, 2.08]\n",
            "Epoch 1, batch: 4000/5760 (35.42 s), loss: 148.29, kl: [1.57, 2.39, 0.00, 0.00, 0.00, 0.04, 0.00, 2.99, 0.00, 2.07]\n",
            "Epoch 1, batch: 4100/5760 (35.82 s), loss: 148.18, kl: [1.60, 2.39, 0.00, 0.00, 0.00, 0.04, 0.00, 2.99, 0.00, 2.07]\n",
            "Epoch 1, batch: 4200/5760 (35.34 s), loss: 147.58, kl: [1.58, 2.39, 0.00, 0.00, 0.00, 0.04, 0.00, 3.00, 0.00, 2.07]\n",
            "Epoch 1, batch: 4300/5760 (35.01 s), loss: 147.50, kl: [1.58, 2.39, 0.00, 0.00, 0.00, 0.04, 0.00, 3.01, 0.00, 2.08]\n",
            "Epoch 1, batch: 4400/5760 (34.74 s), loss: 147.48, kl: [1.59, 2.39, 0.00, 0.00, 0.00, 0.04, 0.00, 3.01, 0.00, 2.07]\n",
            "Epoch 1, batch: 4500/5760 (35.13 s), loss: 147.34, kl: [1.59, 2.39, 0.00, 0.00, 0.00, 0.03, 0.00, 3.00, 0.00, 2.08]\n",
            "Epoch 1, batch: 4600/5760 (35.32 s), loss: 147.06, kl: [1.58, 2.39, 0.00, 0.00, 0.00, 0.03, 0.00, 3.01, 0.00, 2.08]\n",
            "Epoch 1, batch: 4700/5760 (35.52 s), loss: 147.00, kl: [1.58, 2.39, 0.00, 0.00, 0.00, 0.03, 0.00, 3.01, 0.00, 2.08]\n",
            "Epoch 1, batch: 4800/5760 (35.15 s), loss: 146.67, kl: [1.57, 2.40, 0.00, 0.00, 0.00, 0.03, 0.00, 3.01, 0.00, 2.08]\n",
            "Epoch 1, batch: 4900/5760 (35.09 s), loss: 146.45, kl: [1.59, 2.40, 0.00, 0.00, 0.00, 0.03, 0.00, 3.00, 0.00, 2.07]\n",
            "Epoch 1, batch: 5000/5760 (35.59 s), loss: 145.97, kl: [1.58, 2.40, 0.00, 0.00, 0.00, 0.03, 0.00, 3.00, 0.00, 2.08]\n",
            "Epoch 1, batch: 5100/5760 (35.50 s), loss: 145.66, kl: [1.59, 2.41, 0.00, 0.00, 0.00, 0.03, 0.00, 3.00, 0.00, 2.08]\n",
            "Epoch 1, batch: 5200/5760 (35.03 s), loss: 145.21, kl: [1.58, 2.41, 0.00, 0.00, 0.00, 0.03, 0.00, 3.01, 0.00, 2.08]\n",
            "Epoch 1, batch: 5300/5760 (35.06 s), loss: 145.83, kl: [1.59, 2.40, 0.00, 0.00, 0.00, 0.03, 0.00, 3.00, 0.00, 2.07]\n",
            "Epoch 1, batch: 5400/5760 (35.49 s), loss: 145.06, kl: [1.58, 2.41, 0.00, 0.00, 0.00, 0.02, 0.00, 3.01, 0.00, 2.09]\n",
            "Epoch 1, batch: 5500/5760 (35.05 s), loss: 145.20, kl: [1.58, 2.39, 0.00, 0.00, 0.00, 0.02, 0.00, 3.00, 0.00, 2.10]\n",
            "Epoch 1, batch: 5600/5760 (35.66 s), loss: 144.54, kl: [1.58, 2.40, 0.00, 0.00, 0.00, 0.02, 0.00, 3.00, 0.00, 2.10]\n",
            "Epoch 1, batch: 5700/5760 (35.08 s), loss: 144.77, kl: [1.60, 2.41, 0.00, 0.00, 0.00, 0.02, 0.00, 3.01, 0.00, 2.11]\n",
            "Epoch 2, batch: 0/5760 (21.09 s), loss: 144.96, kl: [1.59, 2.41, 0.00, 0.00, 0.00, 0.02, 0.00, 3.01, 0.00, 2.10]\n",
            "Epoch 2, batch: 100/5760 (35.86 s), loss: 144.83, kl: [1.60, 2.41, 0.00, 0.00, 0.00, 0.02, 0.00, 3.00, 0.00, 2.10]\n",
            "Epoch 2, batch: 200/5760 (35.14 s), loss: 144.66, kl: [1.60, 2.40, 0.00, 0.00, 0.00, 0.02, 0.00, 3.00, 0.00, 2.11]\n",
            "Epoch 2, batch: 300/5760 (35.19 s), loss: 144.06, kl: [1.60, 2.40, 0.00, 0.00, 0.00, 0.02, 0.00, 3.01, 0.00, 2.10]\n",
            "Epoch 2, batch: 400/5760 (35.20 s), loss: 143.79, kl: [1.62, 2.40, 0.00, 0.00, 0.00, 0.02, 0.00, 3.00, 0.00, 2.11]\n",
            "Epoch 2, batch: 500/5760 (35.37 s), loss: 143.91, kl: [1.61, 2.40, 0.00, 0.00, 0.00, 0.02, 0.00, 3.01, 0.00, 2.11]\n",
            "Epoch 2, batch: 600/5760 (34.83 s), loss: 143.70, kl: [1.60, 2.40, 0.00, 0.00, 0.00, 0.02, 0.00, 3.00, 0.00, 2.13]\n",
            "Epoch 2, batch: 700/5760 (35.68 s), loss: 143.68, kl: [1.60, 2.41, 0.00, 0.00, 0.00, 0.02, 0.00, 3.00, 0.00, 2.13]\n",
            "Epoch 2, batch: 800/5760 (35.28 s), loss: 143.65, kl: [1.61, 2.41, 0.00, 0.00, 0.00, 0.02, 0.00, 3.01, 0.00, 2.13]\n",
            "Epoch 2, batch: 900/5760 (35.44 s), loss: 142.99, kl: [1.62, 2.41, 0.00, 0.00, 0.00, 0.02, 0.00, 3.00, 0.00, 2.12]\n",
            "Epoch 2, batch: 1000/5760 (35.57 s), loss: 143.13, kl: [1.65, 2.41, 0.00, 0.00, 0.00, 0.01, 0.00, 3.01, 0.00, 2.14]\n",
            "Epoch 2, batch: 1100/5760 (35.69 s), loss: 143.04, kl: [1.63, 2.41, 0.00, 0.00, 0.00, 0.01, 0.00, 3.01, 0.00, 2.15]\n",
            "Epoch 2, batch: 1200/5760 (35.31 s), loss: 142.66, kl: [1.61, 2.41, 0.00, 0.00, 0.00, 0.01, 0.00, 3.01, 0.00, 2.16]\n",
            "Epoch 2, batch: 1300/5760 (35.54 s), loss: 142.56, kl: [1.63, 2.41, 0.00, 0.00, 0.00, 0.01, 0.00, 3.01, 0.00, 2.15]\n",
            "Epoch 2, batch: 1400/5760 (35.54 s), loss: 142.09, kl: [1.63, 2.40, 0.00, 0.00, 0.00, 0.01, 0.00, 3.00, 0.00, 2.15]\n",
            "Epoch 2, batch: 1500/5760 (35.24 s), loss: 142.20, kl: [1.65, 2.40, 0.00, 0.00, 0.00, 0.01, 0.00, 3.01, 0.00, 2.15]\n",
            "Epoch 2, batch: 1600/5760 (35.72 s), loss: 141.71, kl: [1.68, 2.40, 0.00, 0.00, 0.00, 0.01, 0.00, 3.00, 0.00, 2.14]\n",
            "Epoch 2, batch: 1700/5760 (35.56 s), loss: 141.47, kl: [1.69, 2.40, 0.00, 0.00, 0.00, 0.01, 0.00, 3.01, 0.00, 2.15]\n",
            "Epoch 2, batch: 1800/5760 (34.84 s), loss: 141.48, kl: [1.68, 2.41, 0.00, 0.00, 0.00, 0.01, 0.00, 3.01, 0.00, 2.17]\n",
            "Epoch 2, batch: 1900/5760 (35.64 s), loss: 141.60, kl: [1.69, 2.41, 0.00, 0.00, 0.00, 0.01, 0.00, 3.01, 0.00, 2.18]\n",
            "Epoch 2, batch: 2000/5760 (35.04 s), loss: 141.88, kl: [1.70, 2.40, 0.00, 0.00, 0.00, 0.01, 0.00, 3.01, 0.00, 2.18]\n",
            "Epoch 2, batch: 2100/5760 (35.54 s), loss: 141.38, kl: [1.71, 2.40, 0.00, 0.00, 0.00, 0.01, 0.00, 3.02, 0.00, 2.18]\n",
            "Epoch 2, batch: 2200/5760 (35.07 s), loss: 140.41, kl: [1.70, 2.41, 0.00, 0.00, 0.00, 0.01, 0.00, 3.01, 0.00, 2.16]\n",
            "Epoch 2, batch: 2300/5760 (35.00 s), loss: 140.29, kl: [1.72, 2.41, 0.00, 0.00, 0.00, 0.01, 0.00, 3.01, 0.00, 2.16]\n",
            "Epoch 2, batch: 2400/5760 (35.24 s), loss: 140.06, kl: [1.74, 2.41, 0.00, 0.00, 0.00, 0.01, 0.00, 3.02, 0.00, 2.17]\n",
            "Epoch 2, batch: 2500/5760 (35.11 s), loss: 140.38, kl: [1.75, 2.41, 0.00, 0.00, 0.00, 0.01, 0.00, 3.02, 0.00, 2.17]\n",
            "Epoch 2, batch: 2600/5760 (35.12 s), loss: 140.38, kl: [1.75, 2.41, 0.00, 0.00, 0.00, 0.01, 0.00, 3.03, 0.00, 2.17]\n",
            "Epoch 2, batch: 2700/5760 (35.39 s), loss: 140.09, kl: [1.76, 2.41, 0.00, 0.00, 0.00, 0.01, 0.00, 3.01, 0.00, 2.17]\n",
            "Epoch 2, batch: 2800/5760 (35.24 s), loss: 140.12, kl: [1.76, 2.42, 0.00, 0.00, 0.00, 0.01, 0.00, 3.00, 0.00, 2.18]\n",
            "Epoch 2, batch: 2900/5760 (35.03 s), loss: 140.07, kl: [1.79, 2.41, 0.00, 0.00, 0.00, 0.01, 0.00, 3.00, 0.00, 2.18]\n",
            "Epoch 2, batch: 3000/5760 (35.06 s), loss: 139.24, kl: [1.80, 2.41, 0.00, 0.00, 0.00, 0.01, 0.00, 3.01, 0.00, 2.19]\n",
            "Epoch 2, batch: 3100/5760 (34.96 s), loss: 138.53, kl: [1.80, 2.42, 0.00, 0.00, 0.00, 0.01, 0.00, 3.01, 0.00, 2.18]\n",
            "Epoch 2, batch: 3200/5760 (35.15 s), loss: 139.15, kl: [1.81, 2.41, 0.00, 0.00, 0.00, 0.01, 0.00, 3.02, 0.00, 2.17]\n",
            "Epoch 2, batch: 3300/5760 (35.39 s), loss: 138.67, kl: [1.80, 2.41, 0.00, 0.00, 0.00, 0.01, 0.00, 3.02, 0.00, 2.18]\n",
            "Epoch 2, batch: 3400/5760 (35.02 s), loss: 138.46, kl: [1.81, 2.41, 0.00, 0.00, 0.00, 0.01, 0.00, 3.02, 0.00, 2.19]\n",
            "Epoch 2, batch: 3500/5760 (34.81 s), loss: 138.56, kl: [1.82, 2.41, 0.00, 0.00, 0.00, 0.01, 0.00, 3.02, 0.00, 2.19]\n",
            "Epoch 2, batch: 3600/5760 (35.31 s), loss: 138.44, kl: [1.83, 2.41, 0.00, 0.00, 0.00, 0.01, 0.00, 3.02, 0.00, 2.17]\n",
            "Epoch 2, batch: 3700/5760 (34.59 s), loss: 138.24, kl: [1.84, 2.41, 0.00, 0.00, 0.00, 0.01, 0.00, 3.02, 0.00, 2.19]\n",
            "Epoch 2, batch: 3800/5760 (35.15 s), loss: 138.35, kl: [1.85, 2.41, 0.00, 0.00, 0.00, 0.01, 0.00, 3.02, 0.00, 2.19]\n",
            "Epoch 2, batch: 3900/5760 (35.24 s), loss: 137.87, kl: [1.86, 2.41, 0.00, 0.00, 0.00, 0.01, 0.00, 3.02, 0.00, 2.19]\n",
            "Epoch 2, batch: 4000/5760 (35.26 s), loss: 137.79, kl: [1.87, 2.42, 0.00, 0.00, 0.00, 0.01, 0.00, 3.01, 0.00, 2.18]\n",
            "Epoch 2, batch: 4100/5760 (35.41 s), loss: 137.32, kl: [1.87, 2.42, 0.00, 0.00, 0.00, 0.01, 0.00, 3.02, 0.00, 2.18]\n",
            "Epoch 2, batch: 4200/5760 (35.32 s), loss: 137.53, kl: [1.87, 2.43, 0.00, 0.00, 0.00, 0.01, 0.00, 3.03, 0.00, 2.18]\n",
            "Epoch 2, batch: 4300/5760 (34.90 s), loss: 138.41, kl: [1.87, 2.43, 0.00, 0.00, 0.00, 0.01, 0.00, 3.03, 0.00, 2.20]\n",
            "Epoch 2, batch: 4400/5760 (34.79 s), loss: 138.15, kl: [1.87, 2.44, 0.00, 0.00, 0.00, 0.01, 0.00, 3.03, 0.00, 2.21]\n",
            "Epoch 2, batch: 4500/5760 (35.48 s), loss: 137.41, kl: [1.87, 2.43, 0.00, 0.00, 0.00, 0.01, 0.00, 3.03, 0.00, 2.20]\n",
            "Epoch 2, batch: 4600/5760 (34.74 s), loss: 137.48, kl: [1.90, 2.42, 0.00, 0.00, 0.00, 0.01, 0.00, 3.03, 0.00, 2.21]\n",
            "Epoch 2, batch: 4700/5760 (34.71 s), loss: 137.93, kl: [1.90, 2.43, 0.00, 0.00, 0.00, 0.01, 0.00, 3.03, 0.00, 2.21]\n",
            "Epoch 2, batch: 4800/5760 (34.42 s), loss: 137.36, kl: [1.89, 2.42, 0.00, 0.00, 0.00, 0.01, 0.00, 3.03, 0.00, 2.20]\n",
            "Epoch 2, batch: 4900/5760 (34.37 s), loss: 137.52, kl: [1.89, 2.43, 0.00, 0.00, 0.00, 0.01, 0.00, 3.03, 0.00, 2.20]\n",
            "Epoch 2, batch: 5000/5760 (34.24 s), loss: 137.62, kl: [1.89, 2.42, 0.00, 0.00, 0.00, 0.01, 0.00, 3.03, 0.00, 2.20]\n",
            "Epoch 2, batch: 5100/5760 (34.58 s), loss: 137.37, kl: [1.90, 2.44, 0.00, 0.00, 0.00, 0.01, 0.00, 3.04, 0.00, 2.20]\n",
            "Epoch 2, batch: 5200/5760 (34.11 s), loss: 137.27, kl: [1.91, 2.42, 0.00, 0.00, 0.00, 0.01, 0.00, 3.04, 0.00, 2.20]\n",
            "Epoch 2, batch: 5300/5760 (34.14 s), loss: 137.61, kl: [1.91, 2.42, 0.00, 0.00, 0.00, 0.01, 0.00, 3.03, 0.00, 2.21]\n",
            "Epoch 2, batch: 5400/5760 (34.75 s), loss: 137.04, kl: [1.91, 2.42, 0.00, 0.00, 0.00, 0.01, 0.00, 3.02, 0.00, 2.21]\n",
            "Epoch 2, batch: 5500/5760 (34.06 s), loss: 136.62, kl: [1.90, 2.42, 0.00, 0.00, 0.00, 0.01, 0.00, 3.04, 0.00, 2.20]\n",
            "Epoch 2, batch: 5600/5760 (34.18 s), loss: 136.88, kl: [1.93, 2.43, 0.00, 0.00, 0.00, 0.01, 0.00, 3.04, 0.00, 2.20]\n",
            "Epoch 2, batch: 5700/5760 (33.91 s), loss: 136.71, kl: [1.92, 2.43, 0.00, 0.00, 0.00, 0.01, 0.00, 3.03, 0.00, 2.21]\n",
            "Epoch 3, batch: 0/5760 (20.59 s), loss: 136.48, kl: [1.91, 2.43, 0.00, 0.00, 0.00, 0.01, 0.00, 3.04, 0.00, 2.23]\n",
            "Epoch 3, batch: 100/5760 (34.41 s), loss: 136.47, kl: [1.92, 2.43, 0.00, 0.00, 0.00, 0.01, 0.00, 3.04, 0.00, 2.22]\n",
            "Epoch 3, batch: 200/5760 (34.47 s), loss: 136.54, kl: [1.93, 2.44, 0.00, 0.00, 0.00, 0.01, 0.00, 3.04, 0.00, 2.22]\n",
            "Epoch 3, batch: 300/5760 (34.15 s), loss: 136.32, kl: [1.94, 2.44, 0.00, 0.00, 0.00, 0.01, 0.00, 3.03, 0.00, 2.21]\n",
            "Epoch 3, batch: 400/5760 (34.29 s), loss: 136.70, kl: [1.94, 2.42, 0.00, 0.00, 0.00, 0.01, 0.00, 3.04, 0.00, 2.21]\n",
            "Epoch 3, batch: 500/5760 (34.21 s), loss: 136.50, kl: [1.93, 2.43, 0.00, 0.00, 0.00, 0.01, 0.00, 3.05, 0.00, 2.22]\n",
            "Epoch 3, batch: 600/5760 (34.78 s), loss: 136.47, kl: [1.93, 2.43, 0.00, 0.00, 0.00, 0.01, 0.00, 3.04, 0.00, 2.22]\n",
            "Epoch 3, batch: 700/5760 (34.26 s), loss: 136.13, kl: [1.92, 2.43, 0.00, 0.00, 0.00, 0.01, 0.00, 3.04, 0.00, 2.22]\n",
            "Epoch 3, batch: 800/5760 (34.23 s), loss: 136.12, kl: [1.94, 2.44, 0.00, 0.00, 0.00, 0.01, 0.00, 3.04, 0.00, 2.21]\n",
            "Epoch 3, batch: 900/5760 (34.27 s), loss: 135.81, kl: [1.93, 2.44, 0.00, 0.00, 0.00, 0.01, 0.00, 3.04, 0.00, 2.21]\n",
            "Epoch 3, batch: 1000/5760 (34.08 s), loss: 136.14, kl: [1.94, 2.44, 0.00, 0.00, 0.00, 0.01, 0.00, 3.05, 0.00, 2.21]\n",
            "Epoch 3, batch: 1100/5760 (34.66 s), loss: 136.09, kl: [1.94, 2.43, 0.00, 0.00, 0.00, 0.01, 0.00, 3.04, 0.00, 2.22]\n",
            "Epoch 3, batch: 1200/5760 (34.04 s), loss: 135.87, kl: [1.94, 2.44, 0.00, 0.00, 0.00, 0.01, 0.00, 3.04, 0.00, 2.22]\n",
            "Epoch 3, batch: 1300/5760 (34.11 s), loss: 135.82, kl: [1.93, 2.43, 0.00, 0.00, 0.00, 0.01, 0.00, 3.05, 0.00, 2.22]\n",
            "Epoch 3, batch: 1400/5760 (34.10 s), loss: 135.57, kl: [1.94, 2.43, 0.00, 0.00, 0.00, 0.01, 0.00, 3.04, 0.00, 2.23]\n",
            "Epoch 3, batch: 1500/5760 (34.63 s), loss: 136.05, kl: [1.93, 2.43, 0.00, 0.00, 0.00, 0.01, 0.00, 3.04, 0.00, 2.22]\n",
            "Epoch 3, batch: 1600/5760 (34.57 s), loss: 136.27, kl: [1.94, 2.43, 0.00, 0.00, 0.00, 0.01, 0.00, 3.05, 0.00, 2.22]\n",
            "Epoch 3, batch: 1700/5760 (34.32 s), loss: 136.01, kl: [1.93, 2.43, 0.00, 0.00, 0.00, 0.01, 0.00, 3.05, 0.00, 2.24]\n",
            "Epoch 3, batch: 1800/5760 (34.02 s), loss: 135.52, kl: [1.93, 2.43, 0.00, 0.00, 0.00, 0.01, 0.00, 3.05, 0.00, 2.24]\n",
            "Epoch 3, batch: 1900/5760 (34.06 s), loss: 135.48, kl: [1.93, 2.43, 0.00, 0.00, 0.00, 0.01, 0.00, 3.04, 0.00, 2.24]\n",
            "Epoch 3, batch: 2000/5760 (34.55 s), loss: 135.30, kl: [1.93, 2.43, 0.00, 0.00, 0.00, 0.01, 0.00, 3.04, 0.00, 2.23]\n",
            "Epoch 3, batch: 2100/5760 (34.17 s), loss: 135.23, kl: [1.94, 2.43, 0.00, 0.00, 0.00, 0.01, 0.00, 3.05, 0.00, 2.23]\n",
            "Epoch 3, batch: 2200/5760 (33.97 s), loss: 135.91, kl: [1.95, 2.44, 0.00, 0.00, 0.00, 0.01, 0.00, 3.05, 0.00, 2.22]\n",
            "Epoch 3, batch: 2300/5760 (34.27 s), loss: 135.52, kl: [1.95, 2.44, 0.00, 0.00, 0.00, 0.01, 0.00, 3.05, 0.00, 2.24]\n",
            "Epoch 3, batch: 2400/5760 (34.64 s), loss: 135.06, kl: [1.94, 2.44, 0.00, 0.00, 0.00, 0.01, 0.00, 3.05, 0.00, 2.24]\n",
            "Epoch 3, batch: 2500/5760 (34.06 s), loss: 135.33, kl: [1.93, 2.44, 0.00, 0.00, 0.00, 0.01, 0.00, 3.05, 0.00, 2.23]\n",
            "Epoch 3, batch: 2600/5760 (33.88 s), loss: 135.55, kl: [1.95, 2.44, 0.00, 0.00, 0.00, 0.01, 0.00, 3.05, 0.00, 2.23]\n",
            "Epoch 3, batch: 2700/5760 (34.04 s), loss: 135.34, kl: [1.94, 2.44, 0.00, 0.00, 0.00, 0.01, 0.00, 3.06, 0.00, 2.24]\n",
            "Epoch 3, batch: 2800/5760 (34.20 s), loss: 135.48, kl: [1.94, 2.43, 0.00, 0.00, 0.00, 0.01, 0.00, 3.05, 0.00, 2.23]\n",
            "Epoch 3, batch: 2900/5760 (34.58 s), loss: 135.72, kl: [1.94, 2.43, 0.00, 0.00, 0.00, 0.01, 0.00, 3.05, 0.00, 2.24]\n",
            "Epoch 3, batch: 3000/5760 (34.31 s), loss: 135.67, kl: [1.94, 2.44, 0.00, 0.00, 0.00, 0.01, 0.00, 3.05, 0.00, 2.23]\n",
            "Epoch 3, batch: 3100/5760 (34.35 s), loss: 135.64, kl: [1.95, 2.43, 0.00, 0.00, 0.00, 0.01, 0.00, 3.05, 0.00, 2.24]\n",
            "Epoch 3, batch: 3200/5760 (33.98 s), loss: 135.55, kl: [1.95, 2.44, 0.00, 0.00, 0.00, 0.01, 0.00, 3.06, 0.00, 2.24]\n",
            "Epoch 3, batch: 3300/5760 (35.02 s), loss: 135.59, kl: [1.96, 2.44, 0.00, 0.00, 0.00, 0.01, 0.00, 3.06, 0.00, 2.24]\n",
            "Epoch 3, batch: 3400/5760 (34.16 s), loss: 134.94, kl: [1.95, 2.44, 0.00, 0.00, 0.00, 0.01, 0.00, 3.06, 0.00, 2.22]\n",
            "Epoch 3, batch: 3500/5760 (34.35 s), loss: 135.33, kl: [1.95, 2.44, 0.00, 0.00, 0.00, 0.01, 0.00, 3.06, 0.00, 2.23]\n",
            "Epoch 3, batch: 3600/5760 (34.06 s), loss: 135.11, kl: [1.96, 2.44, 0.00, 0.00, 0.00, 0.01, 0.00, 3.06, 0.00, 2.22]\n",
            "Epoch 3, batch: 3700/5760 (34.53 s), loss: 134.96, kl: [1.96, 2.44, 0.00, 0.00, 0.00, 0.01, 0.00, 3.05, 0.00, 2.22]\n",
            "Epoch 3, batch: 3800/5760 (34.46 s), loss: 135.12, kl: [1.96, 2.44, 0.00, 0.00, 0.00, 0.01, 0.00, 3.05, 0.00, 2.23]\n",
            "Epoch 3, batch: 3900/5760 (33.85 s), loss: 135.10, kl: [1.97, 2.43, 0.00, 0.00, 0.00, 0.02, 0.00, 3.06, 0.00, 2.23]\n",
            "Epoch 3, batch: 4000/5760 (33.92 s), loss: 134.88, kl: [1.96, 2.43, 0.00, 0.00, 0.00, 0.02, 0.00, 3.06, 0.00, 2.23]\n",
            "Epoch 3, batch: 4100/5760 (34.27 s), loss: 134.98, kl: [1.96, 2.44, 0.00, 0.00, 0.00, 0.02, 0.00, 3.05, 0.00, 2.23]\n",
            "Epoch 3, batch: 4200/5760 (34.55 s), loss: 135.17, kl: [1.97, 2.44, 0.00, 0.00, 0.00, 0.02, 0.00, 3.05, 0.00, 2.23]\n",
            "Epoch 3, batch: 4300/5760 (34.14 s), loss: 134.76, kl: [1.97, 2.43, 0.00, 0.00, 0.00, 0.02, 0.00, 3.05, 0.00, 2.23]\n",
            "Epoch 3, batch: 4400/5760 (33.99 s), loss: 134.46, kl: [1.96, 2.43, 0.00, 0.00, 0.00, 0.02, 0.00, 3.05, 0.00, 2.22]\n",
            "Epoch 3, batch: 4500/5760 (33.88 s), loss: 134.68, kl: [1.97, 2.44, 0.00, 0.00, 0.00, 0.02, 0.00, 3.05, 0.00, 2.22]\n",
            "Epoch 3, batch: 4600/5760 (34.06 s), loss: 135.10, kl: [1.97, 2.45, 0.00, 0.00, 0.00, 0.03, 0.00, 3.05, 0.00, 2.23]\n",
            "Epoch 3, batch: 4700/5760 (34.29 s), loss: 135.55, kl: [1.97, 2.45, 0.00, 0.00, 0.00, 0.03, 0.00, 3.06, 0.00, 2.23]\n",
            "Epoch 3, batch: 4800/5760 (34.09 s), loss: 135.27, kl: [1.98, 2.43, 0.00, 0.00, 0.00, 0.03, 0.00, 3.06, 0.00, 2.22]\n",
            "Epoch 3, batch: 4900/5760 (34.03 s), loss: 134.99, kl: [1.96, 2.45, 0.00, 0.00, 0.00, 0.03, 0.00, 3.06, 0.00, 2.23]\n",
            "Epoch 3, batch: 5000/5760 (34.39 s), loss: 134.92, kl: [1.97, 2.45, 0.00, 0.00, 0.00, 0.03, 0.00, 3.06, 0.00, 2.23]\n",
            "Epoch 3, batch: 5100/5760 (35.49 s), loss: 134.65, kl: [1.97, 2.44, 0.00, 0.00, 0.00, 0.04, 0.00, 3.06, 0.00, 2.24]\n",
            "Epoch 3, batch: 5200/5760 (34.54 s), loss: 134.90, kl: [1.98, 2.44, 0.00, 0.00, 0.00, 0.04, 0.00, 3.06, 0.00, 2.23]\n",
            "Epoch 3, batch: 5300/5760 (34.58 s), loss: 134.26, kl: [1.96, 2.43, 0.01, 0.00, 0.00, 0.04, 0.00, 3.06, 0.00, 2.23]\n",
            "Epoch 3, batch: 5400/5760 (34.68 s), loss: 134.76, kl: [1.97, 2.45, 0.01, 0.01, 0.00, 0.05, 0.00, 3.07, 0.00, 2.24]\n",
            "Epoch 3, batch: 5500/5760 (35.06 s), loss: 134.55, kl: [1.96, 2.45, 0.01, 0.01, 0.00, 0.05, 0.00, 3.07, 0.00, 2.25]\n",
            "Epoch 3, batch: 5600/5760 (35.15 s), loss: 134.71, kl: [1.96, 2.45, 0.01, 0.01, 0.00, 0.05, 0.00, 3.06, 0.00, 2.24]\n",
            "Epoch 3, batch: 5700/5760 (34.80 s), loss: 134.04, kl: [1.97, 2.44, 0.01, 0.01, 0.00, 0.05, 0.00, 3.06, 0.00, 2.24]\n",
            "Epoch 4, batch: 0/5760 (21.10 s), loss: 134.22, kl: [1.97, 2.44, 0.01, 0.01, 0.00, 0.05, 0.00, 3.05, 0.00, 2.24]\n",
            "Epoch 4, batch: 100/5760 (35.12 s), loss: 134.33, kl: [1.98, 2.44, 0.01, 0.01, 0.00, 0.06, 0.00, 3.06, 0.00, 2.25]\n",
            "Epoch 4, batch: 200/5760 (35.04 s), loss: 134.57, kl: [1.99, 2.44, 0.01, 0.01, 0.00, 0.06, 0.00, 3.07, 0.00, 2.24]\n",
            "Epoch 4, batch: 300/5760 (35.19 s), loss: 134.21, kl: [1.97, 2.44, 0.01, 0.01, 0.00, 0.07, 0.00, 3.06, 0.00, 2.25]\n",
            "Epoch 4, batch: 400/5760 (34.88 s), loss: 134.53, kl: [1.98, 2.44, 0.01, 0.01, 0.00, 0.07, 0.00, 3.07, 0.00, 2.24]\n",
            "Epoch 4, batch: 500/5760 (34.56 s), loss: 134.12, kl: [1.98, 2.43, 0.01, 0.01, 0.00, 0.07, 0.00, 3.06, 0.00, 2.24]\n",
            "Epoch 4, batch: 600/5760 (34.40 s), loss: 133.75, kl: [1.98, 2.44, 0.01, 0.01, 0.00, 0.08, 0.00, 3.06, 0.00, 2.23]\n",
            "Epoch 4, batch: 700/5760 (34.60 s), loss: 134.26, kl: [1.99, 2.45, 0.01, 0.01, 0.00, 0.08, 0.00, 3.07, 0.00, 2.24]\n",
            "Epoch 4, batch: 800/5760 (34.60 s), loss: 134.02, kl: [1.98, 2.45, 0.01, 0.01, 0.00, 0.09, 0.00, 3.06, 0.00, 2.24]\n",
            "Epoch 4, batch: 900/5760 (34.34 s), loss: 134.11, kl: [1.96, 2.45, 0.01, 0.01, 0.00, 0.09, 0.00, 3.07, 0.00, 2.25]\n",
            "Epoch 4, batch: 1000/5760 (34.32 s), loss: 133.47, kl: [1.97, 2.44, 0.01, 0.01, 0.00, 0.10, 0.00, 3.06, 0.00, 2.24]\n",
            "Epoch 4, batch: 1100/5760 (34.61 s), loss: 133.56, kl: [1.97, 2.45, 0.01, 0.01, 0.00, 0.10, 0.00, 3.07, 0.00, 2.24]\n",
            "Epoch 4, batch: 1200/5760 (34.68 s), loss: 133.23, kl: [1.97, 2.46, 0.01, 0.01, 0.00, 0.11, 0.00, 3.05, 0.00, 2.24]\n",
            "Epoch 4, batch: 1300/5760 (34.20 s), loss: 133.81, kl: [1.96, 2.45, 0.01, 0.01, 0.00, 0.11, 0.00, 3.06, 0.00, 2.25]\n",
            "Epoch 4, batch: 1400/5760 (34.58 s), loss: 133.70, kl: [1.97, 2.45, 0.01, 0.01, 0.00, 0.11, 0.00, 3.05, 0.00, 2.25]\n",
            "Epoch 4, batch: 1500/5760 (34.20 s), loss: 133.52, kl: [1.97, 2.44, 0.01, 0.01, 0.00, 0.12, 0.00, 3.05, 0.00, 2.24]\n",
            "Epoch 4, batch: 1600/5760 (34.41 s), loss: 133.70, kl: [1.99, 2.45, 0.01, 0.01, 0.00, 0.12, 0.00, 3.06, 0.00, 2.24]\n",
            "Epoch 4, batch: 1700/5760 (34.20 s), loss: 133.54, kl: [1.99, 2.44, 0.01, 0.01, 0.00, 0.12, 0.00, 3.06, 0.00, 2.24]\n",
            "Epoch 4, batch: 1800/5760 (33.94 s), loss: 133.39, kl: [1.98, 2.44, 0.01, 0.01, 0.00, 0.13, 0.00, 3.06, 0.00, 2.24]\n",
            "Epoch 4, batch: 1900/5760 (34.12 s), loss: 133.91, kl: [1.98, 2.45, 0.01, 0.01, 0.00, 0.13, 0.00, 3.06, 0.00, 2.24]\n",
            "Epoch 4, batch: 2000/5760 (33.97 s), loss: 133.43, kl: [1.97, 2.44, 0.01, 0.01, 0.00, 0.14, 0.00, 3.06, 0.00, 2.24]\n",
            "Epoch 4, batch: 2100/5760 (35.03 s), loss: 133.36, kl: [1.97, 2.43, 0.01, 0.01, 0.00, 0.14, 0.00, 3.07, 0.00, 2.23]\n",
            "Epoch 4, batch: 2200/5760 (34.14 s), loss: 133.62, kl: [1.97, 2.43, 0.01, 0.01, 0.00, 0.15, 0.00, 3.07, 0.00, 2.24]\n",
            "Epoch 4, batch: 2300/5760 (34.16 s), loss: 133.27, kl: [1.97, 2.44, 0.01, 0.01, 0.00, 0.16, 0.00, 3.07, 0.00, 2.25]\n",
            "Epoch 4, batch: 2400/5760 (34.07 s), loss: 133.09, kl: [1.97, 2.43, 0.01, 0.01, 0.00, 0.16, 0.00, 3.07, 0.00, 2.25]\n",
            "Epoch 4, batch: 2500/5760 (34.31 s), loss: 133.15, kl: [1.98, 2.45, 0.01, 0.01, 0.00, 0.17, 0.00, 3.06, 0.00, 2.25]\n",
            "Epoch 4, batch: 2600/5760 (33.99 s), loss: 133.15, kl: [1.97, 2.45, 0.01, 0.01, 0.00, 0.18, 0.00, 3.07, 0.00, 2.25]\n",
            "Epoch 4, batch: 2700/5760 (33.94 s), loss: 132.89, kl: [1.98, 2.45, 0.01, 0.01, 0.00, 0.19, 0.00, 3.08, 0.00, 2.25]\n",
            "Epoch 4, batch: 2800/5760 (34.01 s), loss: 132.86, kl: [1.97, 2.44, 0.01, 0.01, 0.00, 0.19, 0.00, 3.07, 0.00, 2.25]\n",
            "Epoch 4, batch: 2900/5760 (33.84 s), loss: 132.85, kl: [1.98, 2.45, 0.01, 0.01, 0.00, 0.20, 0.00, 3.07, 0.00, 2.24]\n",
            "Epoch 4, batch: 3000/5760 (34.48 s), loss: 132.77, kl: [1.99, 2.44, 0.01, 0.01, 0.00, 0.20, 0.00, 3.07, 0.00, 2.24]\n",
            "Epoch 4, batch: 3100/5760 (34.28 s), loss: 132.77, kl: [1.98, 2.45, 0.01, 0.01, 0.00, 0.21, 0.00, 3.07, 0.00, 2.24]\n",
            "Epoch 4, batch: 3200/5760 (34.09 s), loss: 132.56, kl: [1.99, 2.43, 0.01, 0.01, 0.00, 0.21, 0.00, 3.07, 0.00, 2.24]\n",
            "Epoch 4, batch: 3300/5760 (33.99 s), loss: 132.82, kl: [1.99, 2.44, 0.01, 0.01, 0.00, 0.22, 0.00, 3.07, 0.00, 2.24]\n",
            "Epoch 4, batch: 3400/5760 (34.27 s), loss: 132.51, kl: [1.99, 2.46, 0.01, 0.01, 0.00, 0.23, 0.00, 3.07, 0.00, 2.24]\n",
            "Epoch 4, batch: 3500/5760 (33.87 s), loss: 132.51, kl: [1.97, 2.45, 0.01, 0.01, 0.00, 0.24, 0.00, 3.07, 0.00, 2.24]\n",
            "Epoch 4, batch: 3600/5760 (34.08 s), loss: 132.27, kl: [1.97, 2.46, 0.01, 0.01, 0.00, 0.24, 0.00, 3.07, 0.00, 2.25]\n",
            "Epoch 4, batch: 3700/5760 (34.05 s), loss: 132.78, kl: [1.98, 2.45, 0.01, 0.01, 0.00, 0.25, 0.00, 3.07, 0.00, 2.25]\n",
            "Epoch 4, batch: 3800/5760 (33.95 s), loss: 132.55, kl: [1.98, 2.45, 0.01, 0.01, 0.00, 0.26, 0.00, 3.08, 0.00, 2.24]\n",
            "Epoch 4, batch: 3900/5760 (34.77 s), loss: 132.58, kl: [1.98, 2.45, 0.01, 0.01, 0.00, 0.27, 0.00, 3.08, 0.00, 2.24]\n",
            "Epoch 4, batch: 4000/5760 (34.04 s), loss: 132.24, kl: [1.99, 2.44, 0.01, 0.01, 0.00, 0.28, 0.00, 3.07, 0.00, 2.25]\n",
            "Epoch 4, batch: 4100/5760 (34.21 s), loss: 132.18, kl: [1.99, 2.45, 0.01, 0.01, 0.00, 0.29, 0.00, 3.07, 0.00, 2.24]\n",
            "Epoch 4, batch: 4200/5760 (33.78 s), loss: 131.39, kl: [1.97, 2.44, 0.01, 0.01, 0.00, 0.29, 0.00, 3.07, 0.00, 2.23]\n",
            "Epoch 4, batch: 4300/5760 (34.20 s), loss: 131.72, kl: [1.98, 2.44, 0.01, 0.01, 0.00, 0.30, 0.00, 3.08, 0.00, 2.23]\n",
            "Epoch 4, batch: 4400/5760 (34.28 s), loss: 131.91, kl: [1.99, 2.44, 0.01, 0.01, 0.00, 0.31, 0.00, 3.08, 0.00, 2.24]\n",
            "Epoch 4, batch: 4500/5760 (34.08 s), loss: 131.72, kl: [2.00, 2.44, 0.01, 0.01, 0.00, 0.32, 0.00, 3.08, 0.00, 2.25]\n",
            "Epoch 4, batch: 4600/5760 (34.14 s), loss: 131.57, kl: [1.98, 2.45, 0.01, 0.01, 0.00, 0.33, 0.00, 3.08, 0.00, 2.26]\n",
            "Epoch 4, batch: 4700/5760 (34.09 s), loss: 131.57, kl: [1.98, 2.45, 0.01, 0.00, 0.00, 0.34, 0.00, 3.07, 0.00, 2.26]\n",
            "Epoch 4, batch: 4800/5760 (34.52 s), loss: 131.43, kl: [1.98, 2.46, 0.01, 0.00, 0.00, 0.35, 0.00, 3.07, 0.00, 2.25]\n",
            "Epoch 4, batch: 4900/5760 (34.04 s), loss: 131.50, kl: [1.98, 2.45, 0.01, 0.00, 0.00, 0.35, 0.00, 3.07, 0.00, 2.26]\n",
            "Epoch 4, batch: 5000/5760 (34.01 s), loss: 131.46, kl: [1.97, 2.45, 0.01, 0.00, 0.00, 0.37, 0.00, 3.08, 0.00, 2.26]\n",
            "Epoch 4, batch: 5100/5760 (34.34 s), loss: 130.93, kl: [1.97, 2.44, 0.01, 0.00, 0.00, 0.38, 0.00, 3.07, 0.00, 2.27]\n",
            "Epoch 4, batch: 5200/5760 (34.17 s), loss: 130.99, kl: [1.98, 2.44, 0.01, 0.00, 0.00, 0.38, 0.00, 3.08, 0.00, 2.25]\n",
            "Epoch 4, batch: 5300/5760 (34.57 s), loss: 130.68, kl: [1.99, 2.44, 0.01, 0.00, 0.00, 0.40, 0.00, 3.07, 0.00, 2.24]\n",
            "Epoch 4, batch: 5400/5760 (34.24 s), loss: 131.09, kl: [1.99, 2.44, 0.01, 0.00, 0.00, 0.41, 0.00, 3.07, 0.00, 2.26]\n",
            "Epoch 4, batch: 5500/5760 (33.95 s), loss: 130.66, kl: [1.99, 2.44, 0.01, 0.00, 0.00, 0.42, 0.00, 3.08, 0.00, 2.25]\n",
            "Epoch 4, batch: 5600/5760 (34.01 s), loss: 130.87, kl: [1.97, 2.45, 0.00, 0.00, 0.00, 0.43, 0.00, 3.08, 0.00, 2.26]\n",
            "Epoch 4, batch: 5700/5760 (34.12 s), loss: 130.46, kl: [1.97, 2.45, 0.00, 0.00, 0.00, 0.44, 0.00, 3.08, 0.00, 2.26]\n",
            "Epoch 5, batch: 0/5760 (20.79 s), loss: 130.76, kl: [1.99, 2.46, 0.00, 0.00, 0.00, 0.44, 0.00, 3.08, 0.00, 2.25]\n",
            "Epoch 5, batch: 100/5760 (34.39 s), loss: 130.58, kl: [1.98, 2.45, 0.00, 0.00, 0.00, 0.45, 0.00, 3.08, 0.00, 2.26]\n",
            "Epoch 5, batch: 200/5760 (33.85 s), loss: 130.41, kl: [1.99, 2.45, 0.00, 0.00, 0.00, 0.46, 0.00, 3.08, 0.00, 2.26]\n",
            "Epoch 5, batch: 300/5760 (34.26 s), loss: 130.78, kl: [1.99, 2.45, 0.00, 0.00, 0.00, 0.47, 0.00, 3.09, 0.00, 2.27]\n",
            "Epoch 5, batch: 400/5760 (34.38 s), loss: 130.53, kl: [2.00, 2.45, 0.00, 0.00, 0.00, 0.49, 0.00, 3.09, 0.00, 2.25]\n",
            "Epoch 5, batch: 500/5760 (34.27 s), loss: 129.96, kl: [1.98, 2.45, 0.00, 0.00, 0.00, 0.49, 0.00, 3.09, 0.00, 2.26]\n",
            "Epoch 5, batch: 600/5760 (34.07 s), loss: 129.86, kl: [1.99, 2.44, 0.00, 0.00, 0.00, 0.49, 0.00, 3.08, 0.00, 2.25]\n",
            "Epoch 5, batch: 700/5760 (34.03 s), loss: 129.56, kl: [1.98, 2.44, 0.00, 0.00, 0.00, 0.49, 0.00, 3.08, 0.00, 2.25]\n",
            "Epoch 5, batch: 800/5760 (34.00 s), loss: 129.99, kl: [1.98, 2.45, 0.00, 0.00, 0.00, 0.50, 0.00, 3.08, 0.00, 2.26]\n",
            "Epoch 5, batch: 900/5760 (34.79 s), loss: 129.98, kl: [1.98, 2.46, 0.00, 0.00, 0.00, 0.51, 0.00, 3.08, 0.00, 2.26]\n",
            "Epoch 5, batch: 1000/5760 (33.83 s), loss: 129.65, kl: [1.98, 2.46, 0.00, 0.00, 0.00, 0.52, 0.00, 3.08, 0.00, 2.26]\n",
            "Epoch 5, batch: 1100/5760 (34.52 s), loss: 129.47, kl: [1.98, 2.45, 0.00, 0.00, 0.00, 0.52, 0.00, 3.09, 0.00, 2.26]\n",
            "Epoch 5, batch: 1200/5760 (34.24 s), loss: 129.26, kl: [1.97, 2.45, 0.00, 0.00, 0.00, 0.53, 0.00, 3.09, 0.00, 2.26]\n",
            "Epoch 5, batch: 1300/5760 (34.60 s), loss: 129.64, kl: [1.98, 2.45, 0.00, 0.00, 0.00, 0.55, 0.00, 3.09, 0.00, 2.26]\n",
            "Epoch 5, batch: 1400/5760 (34.33 s), loss: 129.47, kl: [1.98, 2.45, 0.00, 0.00, 0.00, 0.56, 0.00, 3.09, 0.00, 2.27]\n",
            "Epoch 5, batch: 1500/5760 (34.32 s), loss: 129.85, kl: [1.99, 2.45, 0.00, 0.00, 0.00, 0.57, 0.00, 3.09, 0.00, 2.27]\n",
            "Epoch 5, batch: 1600/5760 (34.20 s), loss: 128.75, kl: [1.99, 2.44, 0.00, 0.00, 0.00, 0.57, 0.00, 3.09, 0.00, 2.26]\n",
            "Epoch 5, batch: 1700/5760 (34.61 s), loss: 129.00, kl: [1.99, 2.45, 0.00, 0.00, 0.00, 0.57, 0.00, 3.10, 0.00, 2.26]\n",
            "Epoch 5, batch: 1800/5760 (35.12 s), loss: 129.23, kl: [1.98, 2.46, 0.00, 0.00, 0.00, 0.58, 0.00, 3.10, 0.00, 2.26]\n",
            "Epoch 5, batch: 1900/5760 (34.74 s), loss: 128.96, kl: [1.98, 2.46, 0.00, 0.00, 0.00, 0.58, 0.00, 3.10, 0.00, 2.27]\n",
            "Epoch 5, batch: 2000/5760 (34.67 s), loss: 128.78, kl: [1.97, 2.46, 0.00, 0.00, 0.00, 0.59, 0.00, 3.09, 0.00, 2.27]\n",
            "Epoch 5, batch: 2100/5760 (35.13 s), loss: 128.87, kl: [1.98, 2.45, 0.00, 0.00, 0.00, 0.61, 0.00, 3.10, 0.00, 2.27]\n",
            "Epoch 5, batch: 2200/5760 (35.00 s), loss: 128.98, kl: [1.98, 2.45, 0.00, 0.00, 0.00, 0.62, 0.00, 3.09, 0.00, 2.27]\n",
            "Epoch 5, batch: 2300/5760 (35.39 s), loss: 128.83, kl: [1.99, 2.45, 0.00, 0.00, 0.00, 0.62, 0.00, 3.09, 0.00, 2.26]\n",
            "Epoch 5, batch: 2400/5760 (35.13 s), loss: 128.63, kl: [1.99, 2.46, 0.00, 0.00, 0.00, 0.62, 0.00, 3.09, 0.00, 2.27]\n",
            "Epoch 5, batch: 2500/5760 (35.15 s), loss: 128.48, kl: [1.98, 2.46, 0.00, 0.00, 0.00, 0.63, 0.00, 3.09, 0.00, 2.27]\n",
            "Epoch 5, batch: 2600/5760 (35.21 s), loss: 128.70, kl: [1.98, 2.47, 0.00, 0.00, 0.00, 0.63, 0.00, 3.08, 0.00, 2.27]\n",
            "Epoch 5, batch: 2700/5760 (35.54 s), loss: 128.58, kl: [1.98, 2.46, 0.00, 0.00, 0.00, 0.64, 0.00, 3.08, 0.00, 2.27]\n",
            "Epoch 5, batch: 2800/5760 (35.04 s), loss: 128.41, kl: [1.98, 2.47, 0.00, 0.00, 0.00, 0.63, 0.00, 3.08, 0.00, 2.27]\n",
            "Epoch 5, batch: 2900/5760 (34.86 s), loss: 128.17, kl: [1.97, 2.46, 0.00, 0.00, 0.00, 0.65, 0.00, 3.10, 0.00, 2.27]\n",
            "Epoch 5, batch: 3000/5760 (35.42 s), loss: 128.22, kl: [1.98, 2.46, 0.00, 0.00, 0.00, 0.65, 0.00, 3.10, 0.00, 2.28]\n",
            "Epoch 5, batch: 3100/5760 (35.61 s), loss: 128.51, kl: [1.99, 2.47, 0.00, 0.00, 0.00, 0.65, 0.00, 3.09, 0.00, 2.28]\n",
            "Epoch 5, batch: 3200/5760 (35.19 s), loss: 127.96, kl: [1.97, 2.47, 0.00, 0.00, 0.00, 0.67, 0.00, 3.09, 0.00, 2.27]\n",
            "Epoch 5, batch: 3300/5760 (35.35 s), loss: 128.16, kl: [1.98, 2.47, 0.00, 0.00, 0.00, 0.68, 0.00, 3.10, 0.00, 2.28]\n",
            "Epoch 5, batch: 3400/5760 (35.32 s), loss: 128.08, kl: [1.99, 2.46, 0.00, 0.00, 0.00, 0.69, 0.00, 3.10, 0.00, 2.27]\n",
            "Epoch 5, batch: 3500/5760 (35.05 s), loss: 128.31, kl: [1.98, 2.46, 0.00, 0.00, 0.00, 0.70, 0.00, 3.10, 0.00, 2.29]\n",
            "Epoch 5, batch: 3600/5760 (35.34 s), loss: 127.92, kl: [1.98, 2.47, 0.00, 0.00, 0.00, 0.68, 0.00, 3.10, 0.00, 2.29]\n",
            "Epoch 5, batch: 3700/5760 (35.04 s), loss: 128.02, kl: [1.98, 2.47, 0.00, 0.00, 0.00, 0.68, 0.00, 3.10, 0.00, 2.28]\n",
            "Epoch 5, batch: 3800/5760 (35.14 s), loss: 127.86, kl: [1.97, 2.47, 0.00, 0.00, 0.00, 0.69, 0.00, 3.10, 0.00, 2.29]\n",
            "Epoch 5, batch: 3900/5760 (35.38 s), loss: 127.89, kl: [1.99, 2.47, 0.00, 0.00, 0.00, 0.69, 0.00, 3.11, 0.00, 2.29]\n",
            "Epoch 5, batch: 4000/5760 (35.36 s), loss: 127.74, kl: [2.00, 2.47, 0.00, 0.00, 0.00, 0.69, 0.00, 3.10, 0.00, 2.28]\n",
            "Epoch 5, batch: 4100/5760 (35.47 s), loss: 127.31, kl: [1.98, 2.47, 0.00, 0.00, 0.00, 0.69, 0.00, 3.09, 0.00, 2.27]\n",
            "Epoch 5, batch: 4200/5760 (35.18 s), loss: 127.82, kl: [2.00, 2.46, 0.00, 0.00, 0.00, 0.70, 0.00, 3.11, 0.00, 2.27]\n",
            "Epoch 5, batch: 4300/5760 (35.24 s), loss: 127.68, kl: [1.99, 2.47, 0.00, 0.00, 0.00, 0.70, 0.00, 3.10, 0.00, 2.28]\n",
            "Epoch 5, batch: 4400/5760 (34.91 s), loss: 127.26, kl: [1.98, 2.46, 0.00, 0.00, 0.00, 0.70, 0.00, 3.09, 0.00, 2.28]\n",
            "Epoch 5, batch: 4500/5760 (35.35 s), loss: 127.48, kl: [1.98, 2.46, 0.00, 0.00, 0.00, 0.71, 0.00, 3.09, 0.00, 2.28]\n",
            "Epoch 5, batch: 4600/5760 (35.05 s), loss: 127.58, kl: [1.98, 2.46, 0.00, 0.00, 0.00, 0.72, 0.00, 3.10, 0.00, 2.28]\n",
            "Epoch 5, batch: 4700/5760 (35.03 s), loss: 127.28, kl: [1.98, 2.47, 0.00, 0.00, 0.00, 0.73, 0.00, 3.10, 0.00, 2.29]\n",
            "Epoch 5, batch: 4800/5760 (35.60 s), loss: 127.85, kl: [1.99, 2.47, 0.00, 0.00, 0.00, 0.73, 0.00, 3.10, 0.00, 2.28]\n",
            "Epoch 5, batch: 4900/5760 (35.42 s), loss: 127.44, kl: [1.98, 2.47, 0.00, 0.00, 0.00, 0.72, 0.00, 3.11, 0.00, 2.28]\n",
            "Epoch 5, batch: 5000/5760 (34.92 s), loss: 127.54, kl: [1.98, 2.47, 0.00, 0.00, 0.00, 0.74, 0.00, 3.11, 0.00, 2.28]\n",
            "Epoch 5, batch: 5100/5760 (35.22 s), loss: 127.44, kl: [1.99, 2.46, 0.00, 0.00, 0.00, 0.74, 0.00, 3.11, 0.00, 2.29]\n",
            "Epoch 5, batch: 5200/5760 (34.70 s), loss: 126.80, kl: [1.97, 2.46, 0.00, 0.00, 0.00, 0.74, 0.00, 3.11, 0.00, 2.29]\n",
            "Epoch 5, batch: 5300/5760 (35.02 s), loss: 127.42, kl: [1.99, 2.46, 0.00, 0.00, 0.00, 0.74, 0.00, 3.11, 0.00, 2.30]\n",
            "Epoch 5, batch: 5400/5760 (34.88 s), loss: 127.21, kl: [1.98, 2.46, 0.00, 0.00, 0.00, 0.74, 0.00, 3.11, 0.00, 2.30]\n",
            "Epoch 5, batch: 5500/5760 (34.53 s), loss: 127.28, kl: [1.99, 2.47, 0.00, 0.00, 0.00, 0.74, 0.00, 3.12, 0.00, 2.29]\n",
            "Epoch 5, batch: 5600/5760 (34.31 s), loss: 127.05, kl: [1.99, 2.46, 0.00, 0.00, 0.00, 0.74, 0.00, 3.11, 0.00, 2.29]\n",
            "Epoch 5, batch: 5700/5760 (34.93 s), loss: 126.97, kl: [1.99, 2.47, 0.00, 0.00, 0.00, 0.74, 0.00, 3.11, 0.00, 2.28]\n",
            "Epoch 6, batch: 0/5760 (20.59 s), loss: 127.19, kl: [1.99, 2.47, 0.00, 0.00, 0.00, 0.74, 0.00, 3.11, 0.00, 2.29]\n",
            "Epoch 6, batch: 100/5760 (34.54 s), loss: 127.45, kl: [2.00, 2.47, 0.00, 0.00, 0.00, 0.75, 0.00, 3.11, 0.00, 2.30]\n",
            "Epoch 6, batch: 200/5760 (34.21 s), loss: 126.96, kl: [1.99, 2.47, 0.00, 0.00, 0.00, 0.76, 0.00, 3.11, 0.00, 2.30]\n",
            "Epoch 6, batch: 300/5760 (34.25 s), loss: 126.73, kl: [2.00, 2.46, 0.00, 0.00, 0.00, 0.77, 0.00, 3.10, 0.00, 2.29]\n",
            "Epoch 6, batch: 400/5760 (34.24 s), loss: 126.54, kl: [1.99, 2.47, 0.00, 0.00, 0.00, 0.76, 0.00, 3.11, 0.00, 2.29]\n",
            "Epoch 6, batch: 500/5760 (34.69 s), loss: 127.20, kl: [2.00, 2.47, 0.00, 0.00, 0.00, 0.77, 0.00, 3.10, 0.00, 2.29]\n",
            "Epoch 6, batch: 600/5760 (34.11 s), loss: 126.48, kl: [1.99, 2.47, 0.00, 0.00, 0.00, 0.77, 0.00, 3.10, 0.00, 2.28]\n",
            "Epoch 6, batch: 700/5760 (34.27 s), loss: 126.43, kl: [1.98, 2.48, 0.00, 0.00, 0.00, 0.77, 0.00, 3.10, 0.00, 2.29]\n",
            "Epoch 6, batch: 800/5760 (34.62 s), loss: 126.09, kl: [1.96, 2.48, 0.00, 0.00, 0.00, 0.77, 0.00, 3.10, 0.00, 2.31]\n",
            "Epoch 6, batch: 900/5760 (34.07 s), loss: 126.24, kl: [1.97, 2.48, 0.00, 0.00, 0.00, 0.77, 0.00, 3.10, 0.00, 2.30]\n",
            "Epoch 6, batch: 1000/5760 (34.23 s), loss: 126.50, kl: [1.98, 2.47, 0.00, 0.00, 0.00, 0.78, 0.00, 3.11, 0.00, 2.29]\n",
            "Epoch 6, batch: 1100/5760 (34.54 s), loss: 126.65, kl: [1.99, 2.47, 0.00, 0.00, 0.00, 0.79, 0.00, 3.11, 0.00, 2.29]\n",
            "Epoch 6, batch: 1200/5760 (33.96 s), loss: 126.71, kl: [1.98, 2.48, 0.00, 0.00, 0.00, 0.78, 0.00, 3.11, 0.00, 2.28]\n",
            "Epoch 6, batch: 1300/5760 (34.18 s), loss: 126.37, kl: [1.97, 2.48, 0.00, 0.00, 0.00, 0.78, 0.00, 3.11, 0.00, 2.29]\n",
            "Epoch 6, batch: 1400/5760 (34.63 s), loss: 126.35, kl: [1.98, 2.47, 0.00, 0.00, 0.00, 0.79, 0.00, 3.11, 0.00, 2.29]\n",
            "Epoch 6, batch: 1500/5760 (34.06 s), loss: 126.34, kl: [1.99, 2.48, 0.00, 0.00, 0.00, 0.78, 0.00, 3.11, 0.00, 2.29]\n",
            "Epoch 6, batch: 1600/5760 (34.05 s), loss: 126.25, kl: [1.99, 2.48, 0.00, 0.00, 0.00, 0.78, 0.00, 3.11, 0.00, 2.29]\n",
            "Epoch 6, batch: 1700/5760 (34.44 s), loss: 126.33, kl: [1.98, 2.49, 0.00, 0.00, 0.00, 0.79, 0.00, 3.11, 0.00, 2.28]\n",
            "Epoch 6, batch: 1800/5760 (34.00 s), loss: 126.35, kl: [1.99, 2.48, 0.00, 0.00, 0.00, 0.78, 0.00, 3.11, 0.00, 2.29]\n",
            "Epoch 6, batch: 1900/5760 (33.98 s), loss: 126.51, kl: [1.99, 2.48, 0.00, 0.00, 0.00, 0.79, 0.00, 3.12, 0.00, 2.29]\n",
            "Epoch 6, batch: 2000/5760 (34.10 s), loss: 126.48, kl: [1.99, 2.48, 0.00, 0.00, 0.00, 0.80, 0.00, 3.12, 0.00, 2.29]\n",
            "Epoch 6, batch: 2100/5760 (34.31 s), loss: 126.34, kl: [1.99, 2.47, 0.00, 0.00, 0.00, 0.80, 0.00, 3.11, 0.00, 2.30]\n",
            "Epoch 6, batch: 2200/5760 (33.93 s), loss: 126.29, kl: [1.99, 2.47, 0.00, 0.00, 0.00, 0.80, 0.00, 3.11, 0.00, 2.30]\n",
            "Epoch 6, batch: 2300/5760 (34.68 s), loss: 126.22, kl: [1.99, 2.47, 0.00, 0.00, 0.00, 0.81, 0.00, 3.12, 0.00, 2.29]\n",
            "Epoch 6, batch: 2400/5760 (34.21 s), loss: 126.20, kl: [1.99, 2.47, 0.00, 0.00, 0.00, 0.81, 0.00, 3.12, 0.00, 2.29]\n",
            "Epoch 6, batch: 2500/5760 (33.99 s), loss: 126.11, kl: [2.00, 2.47, 0.00, 0.00, 0.00, 0.82, 0.00, 3.13, 0.00, 2.30]\n",
            "Epoch 6, batch: 2600/5760 (34.17 s), loss: 125.77, kl: [1.99, 2.47, 0.00, 0.00, 0.00, 0.81, 0.00, 3.12, 0.00, 2.31]\n",
            "Epoch 6, batch: 2700/5760 (33.92 s), loss: 126.00, kl: [1.99, 2.47, 0.00, 0.00, 0.00, 0.80, 0.00, 3.12, 0.00, 2.30]\n",
            "Epoch 6, batch: 2800/5760 (33.98 s), loss: 125.74, kl: [1.99, 2.47, 0.00, 0.00, 0.00, 0.81, 0.00, 3.12, 0.00, 2.29]\n",
            "Epoch 6, batch: 2900/5760 (34.13 s), loss: 125.77, kl: [1.99, 2.47, 0.00, 0.00, 0.00, 0.80, 0.00, 3.12, 0.00, 2.30]\n",
            "Epoch 6, batch: 3000/5760 (33.93 s), loss: 125.90, kl: [1.98, 2.48, 0.00, 0.00, 0.00, 0.82, 0.00, 3.11, 0.00, 2.31]\n",
            "Epoch 6, batch: 3100/5760 (34.26 s), loss: 126.16, kl: [1.99, 2.48, 0.00, 0.00, 0.00, 0.83, 0.00, 3.12, 0.00, 2.32]\n",
            "Epoch 6, batch: 3200/5760 (34.45 s), loss: 126.03, kl: [1.98, 2.47, 0.00, 0.00, 0.00, 0.83, 0.00, 3.12, 0.00, 2.32]\n",
            "Epoch 6, batch: 3300/5760 (34.10 s), loss: 126.00, kl: [1.98, 2.48, 0.00, 0.00, 0.00, 0.83, 0.00, 3.12, 0.00, 2.31]\n",
            "Epoch 6, batch: 3400/5760 (34.22 s), loss: 125.63, kl: [1.98, 2.47, 0.00, 0.00, 0.00, 0.83, 0.00, 3.12, 0.00, 2.30]\n",
            "Epoch 6, batch: 3500/5760 (34.66 s), loss: 125.65, kl: [2.00, 2.47, 0.00, 0.00, 0.00, 0.82, 0.00, 3.12, 0.00, 2.29]\n",
            "Epoch 6, batch: 3600/5760 (34.33 s), loss: 125.69, kl: [2.00, 2.48, 0.00, 0.00, 0.00, 0.83, 0.00, 3.11, 0.00, 2.29]\n",
            "Epoch 6, batch: 3700/5760 (34.79 s), loss: 125.89, kl: [1.99, 2.48, 0.00, 0.00, 0.00, 0.84, 0.00, 3.11, 0.00, 2.30]\n",
            "Epoch 6, batch: 3800/5760 (34.87 s), loss: 125.86, kl: [1.99, 2.48, 0.00, 0.00, 0.00, 0.83, 0.00, 3.12, 0.00, 2.30]\n",
            "Epoch 6, batch: 3900/5760 (34.81 s), loss: 125.67, kl: [2.00, 2.48, 0.00, 0.00, 0.00, 0.83, 0.00, 3.13, 0.00, 2.30]\n",
            "Epoch 6, batch: 4000/5760 (34.93 s), loss: 126.09, kl: [1.99, 2.49, 0.00, 0.00, 0.00, 0.83, 0.00, 3.13, 0.00, 2.31]\n",
            "Epoch 6, batch: 4100/5760 (35.78 s), loss: 125.76, kl: [2.00, 2.48, 0.00, 0.00, 0.00, 0.84, 0.00, 3.13, 0.00, 2.30]\n",
            "Epoch 6, batch: 4200/5760 (35.27 s), loss: 125.47, kl: [2.00, 2.48, 0.00, 0.00, 0.00, 0.84, 0.00, 3.12, 0.00, 2.30]\n",
            "Epoch 6, batch: 4300/5760 (35.23 s), loss: 125.40, kl: [1.99, 2.47, 0.00, 0.00, 0.00, 0.84, 0.00, 3.12, 0.00, 2.30]\n",
            "Epoch 6, batch: 4400/5760 (35.67 s), loss: 125.51, kl: [1.99, 2.48, 0.00, 0.00, 0.00, 0.84, 0.00, 3.12, 0.00, 2.32]\n",
            "Epoch 6, batch: 4500/5760 (35.31 s), loss: 125.59, kl: [1.98, 2.48, 0.00, 0.00, 0.00, 0.83, 0.00, 3.12, 0.00, 2.34]\n",
            "Epoch 6, batch: 4600/5760 (35.27 s), loss: 125.20, kl: [1.98, 2.48, 0.00, 0.00, 0.00, 0.82, 0.00, 3.12, 0.00, 2.32]\n",
            "Epoch 6, batch: 4700/5760 (35.11 s), loss: 125.36, kl: [1.98, 2.48, 0.00, 0.00, 0.00, 0.83, 0.00, 3.12, 0.00, 2.31]\n",
            "Epoch 6, batch: 4800/5760 (35.22 s), loss: 125.53, kl: [1.99, 2.49, 0.00, 0.00, 0.00, 0.84, 0.00, 3.12, 0.00, 2.31]\n",
            "Epoch 6, batch: 4900/5760 (35.31 s), loss: 125.38, kl: [1.99, 2.48, 0.00, 0.00, 0.00, 0.84, 0.00, 3.12, 0.00, 2.30]\n",
            "Epoch 6, batch: 5000/5760 (35.49 s), loss: 125.32, kl: [1.99, 2.49, 0.00, 0.00, 0.00, 0.84, 0.00, 3.12, 0.00, 2.30]\n",
            "Epoch 6, batch: 5100/5760 (35.59 s), loss: 125.20, kl: [1.99, 2.49, 0.00, 0.00, 0.00, 0.84, 0.00, 3.13, 0.00, 2.31]\n",
            "Epoch 6, batch: 5200/5760 (35.01 s), loss: 124.96, kl: [2.00, 2.48, 0.00, 0.00, 0.00, 0.84, 0.00, 3.12, 0.00, 2.30]\n",
            "Epoch 6, batch: 5300/5760 (35.28 s), loss: 125.39, kl: [1.99, 2.49, 0.00, 0.00, 0.00, 0.83, 0.00, 3.12, 0.00, 2.31]\n",
            "Epoch 6, batch: 5400/5760 (34.83 s), loss: 125.02, kl: [1.99, 2.49, 0.00, 0.00, 0.00, 0.86, 0.00, 3.13, 0.00, 2.31]\n",
            "Epoch 6, batch: 5500/5760 (34.73 s), loss: 124.92, kl: [1.99, 2.49, 0.00, 0.00, 0.00, 0.84, 0.00, 3.13, 0.00, 2.32]\n",
            "Epoch 6, batch: 5600/5760 (34.59 s), loss: 124.88, kl: [1.99, 2.48, 0.00, 0.00, 0.00, 0.83, 0.00, 3.13, 0.00, 2.31]\n",
            "Epoch 6, batch: 5700/5760 (34.45 s), loss: 125.10, kl: [2.00, 2.48, 0.00, 0.00, 0.00, 0.84, 0.00, 3.13, 0.00, 2.30]\n",
            "Epoch 7, batch: 0/5760 (20.77 s), loss: 125.24, kl: [2.01, 2.48, 0.00, 0.00, 0.00, 0.85, 0.00, 3.13, 0.00, 2.31]\n",
            "Epoch 7, batch: 100/5760 (35.06 s), loss: 125.37, kl: [2.02, 2.48, 0.00, 0.00, 0.00, 0.84, 0.00, 3.13, 0.00, 2.31]\n",
            "Epoch 7, batch: 200/5760 (34.59 s), loss: 124.93, kl: [2.00, 2.48, 0.00, 0.00, 0.00, 0.83, 0.00, 3.12, 0.00, 2.31]\n",
            "Epoch 7, batch: 300/5760 (34.23 s), loss: 125.09, kl: [2.00, 2.49, 0.00, 0.00, 0.00, 0.85, 0.00, 3.13, 0.00, 2.31]\n",
            "Epoch 7, batch: 400/5760 (34.46 s), loss: 125.08, kl: [2.00, 2.48, 0.00, 0.00, 0.00, 0.86, 0.00, 3.13, 0.00, 2.31]\n",
            "Epoch 7, batch: 500/5760 (34.65 s), loss: 124.95, kl: [1.99, 2.49, 0.00, 0.00, 0.00, 0.86, 0.00, 3.13, 0.00, 2.32]\n",
            "Epoch 7, batch: 600/5760 (34.44 s), loss: 124.99, kl: [2.00, 2.49, 0.00, 0.00, 0.00, 0.86, 0.00, 3.13, 0.00, 2.32]\n",
            "Epoch 7, batch: 700/5760 (34.01 s), loss: 124.63, kl: [1.99, 2.49, 0.00, 0.00, 0.00, 0.87, 0.00, 3.13, 0.00, 2.31]\n",
            "Epoch 7, batch: 800/5760 (34.18 s), loss: 125.18, kl: [1.98, 2.48, 0.00, 0.00, 0.00, 0.87, 0.00, 3.13, 0.00, 2.32]\n",
            "Epoch 7, batch: 900/5760 (34.18 s), loss: 125.50, kl: [1.99, 2.47, 0.00, 0.00, 0.00, 0.87, 0.00, 3.13, 0.00, 2.32]\n",
            "Epoch 7, batch: 1000/5760 (34.09 s), loss: 125.21, kl: [1.99, 2.48, 0.00, 0.00, 0.00, 0.86, 0.00, 3.13, 0.00, 2.32]\n",
            "Epoch 7, batch: 1100/5760 (34.87 s), loss: 125.09, kl: [1.99, 2.48, 0.00, 0.00, 0.00, 0.85, 0.00, 3.13, 0.00, 2.31]\n",
            "Epoch 7, batch: 1200/5760 (34.13 s), loss: 125.52, kl: [1.99, 2.49, 0.00, 0.00, 0.00, 0.85, 0.00, 3.14, 0.00, 2.31]\n",
            "Epoch 7, batch: 1300/5760 (34.51 s), loss: 125.00, kl: [1.99, 2.48, 0.00, 0.00, 0.00, 0.85, 0.00, 3.13, 0.00, 2.30]\n",
            "Epoch 7, batch: 1400/5760 (33.98 s), loss: 124.73, kl: [1.99, 2.49, 0.00, 0.00, 0.00, 0.85, 0.00, 3.14, 0.00, 2.30]\n",
            "Epoch 7, batch: 1500/5760 (33.99 s), loss: 124.98, kl: [2.00, 2.49, 0.00, 0.00, 0.00, 0.85, 0.00, 3.13, 0.00, 2.31]\n",
            "Epoch 7, batch: 1600/5760 (34.15 s), loss: 124.76, kl: [2.01, 2.48, 0.00, 0.00, 0.00, 0.85, 0.00, 3.13, 0.00, 2.30]\n",
            "Epoch 7, batch: 1700/5760 (33.94 s), loss: 124.95, kl: [2.01, 2.49, 0.00, 0.00, 0.00, 0.86, 0.00, 3.14, 0.00, 2.31]\n",
            "Epoch 7, batch: 1800/5760 (33.98 s), loss: 124.96, kl: [1.99, 2.50, 0.00, 0.00, 0.00, 0.86, 0.00, 3.14, 0.00, 2.31]\n",
            "Epoch 7, batch: 1900/5760 (34.06 s), loss: 124.66, kl: [1.99, 2.49, 0.00, 0.00, 0.00, 0.85, 0.00, 3.13, 0.00, 2.31]\n",
            "Epoch 7, batch: 2000/5760 (34.89 s), loss: 124.73, kl: [1.98, 2.48, 0.00, 0.00, 0.00, 0.87, 0.00, 3.13, 0.00, 2.32]\n",
            "Epoch 7, batch: 2100/5760 (34.17 s), loss: 124.87, kl: [1.98, 2.48, 0.00, 0.00, 0.00, 0.88, 0.00, 3.12, 0.00, 2.32]\n",
            "Epoch 7, batch: 2200/5760 (34.52 s), loss: 124.74, kl: [2.00, 2.48, 0.00, 0.00, 0.00, 0.86, 0.00, 3.13, 0.00, 2.31]\n",
            "Epoch 7, batch: 2300/5760 (33.93 s), loss: 124.86, kl: [2.00, 2.49, 0.00, 0.00, 0.00, 0.86, 0.00, 3.13, 0.00, 2.32]\n",
            "Epoch 7, batch: 2400/5760 (34.01 s), loss: 124.60, kl: [2.01, 2.49, 0.00, 0.00, 0.00, 0.86, 0.00, 3.12, 0.00, 2.31]\n",
            "Epoch 7, batch: 2500/5760 (33.96 s), loss: 124.81, kl: [2.02, 2.50, 0.00, 0.00, 0.00, 0.87, 0.00, 3.12, 0.00, 2.31]\n",
            "Epoch 7, batch: 2600/5760 (33.96 s), loss: 124.87, kl: [2.01, 2.49, 0.00, 0.00, 0.00, 0.86, 0.00, 3.12, 0.00, 2.32]\n",
            "Epoch 7, batch: 2700/5760 (33.69 s), loss: 124.63, kl: [2.00, 2.49, 0.00, 0.00, 0.00, 0.86, 0.00, 3.13, 0.00, 2.33]\n",
            "Epoch 7, batch: 2800/5760 (33.81 s), loss: 124.48, kl: [2.00, 2.48, 0.00, 0.00, 0.00, 0.86, 0.00, 3.13, 0.00, 2.31]\n",
            "Epoch 7, batch: 2900/5760 (34.74 s), loss: 124.46, kl: [2.00, 2.49, 0.00, 0.00, 0.00, 0.85, 0.00, 3.13, 0.00, 2.32]\n",
            "Epoch 7, batch: 3000/5760 (34.12 s), loss: 124.37, kl: [2.00, 2.49, 0.00, 0.00, 0.00, 0.85, 0.00, 3.12, 0.00, 2.31]\n",
            "Epoch 7, batch: 3100/5760 (34.77 s), loss: 124.43, kl: [1.99, 2.50, 0.00, 0.00, 0.00, 0.84, 0.00, 3.13, 0.00, 2.32]\n",
            "Epoch 7, batch: 3200/5760 (34.10 s), loss: 124.28, kl: [1.99, 2.51, 0.00, 0.00, 0.00, 0.86, 0.00, 3.13, 0.00, 2.32]\n",
            "Epoch 7, batch: 3300/5760 (34.29 s), loss: 124.23, kl: [1.98, 2.50, 0.00, 0.00, 0.00, 0.86, 0.00, 3.13, 0.00, 2.32]\n",
            "Epoch 7, batch: 3400/5760 (34.11 s), loss: 124.50, kl: [2.01, 2.50, 0.00, 0.00, 0.00, 0.87, 0.00, 3.14, 0.00, 2.32]\n",
            "Epoch 7, batch: 3500/5760 (33.92 s), loss: 124.64, kl: [2.00, 2.49, 0.00, 0.00, 0.00, 0.87, 0.00, 3.14, 0.00, 2.33]\n",
            "Epoch 7, batch: 3600/5760 (34.07 s), loss: 124.49, kl: [1.98, 2.49, 0.00, 0.00, 0.00, 0.87, 0.00, 3.14, 0.00, 2.33]\n",
            "Epoch 7, batch: 3700/5760 (34.08 s), loss: 124.12, kl: [1.99, 2.49, 0.00, 0.00, 0.00, 0.88, 0.00, 3.14, 0.00, 2.32]\n",
            "Epoch 7, batch: 3800/5760 (34.57 s), loss: 124.40, kl: [1.99, 2.49, 0.00, 0.00, 0.00, 0.87, 0.00, 3.14, 0.00, 2.32]\n",
            "Epoch 7, batch: 3900/5760 (34.22 s), loss: 124.20, kl: [2.00, 2.49, 0.00, 0.00, 0.00, 0.86, 0.00, 3.12, 0.00, 2.32]\n",
            "Epoch 7, batch: 4000/5760 (34.37 s), loss: 124.30, kl: [2.00, 2.49, 0.00, 0.00, 0.00, 0.85, 0.00, 3.14, 0.00, 2.33]\n",
            "Epoch 7, batch: 4100/5760 (34.19 s), loss: 124.35, kl: [2.00, 2.49, 0.00, 0.00, 0.00, 0.85, 0.00, 3.13, 0.00, 2.32]\n",
            "Epoch 7, batch: 4200/5760 (34.04 s), loss: 124.39, kl: [2.00, 2.49, 0.00, 0.00, 0.00, 0.86, 0.00, 3.13, 0.00, 2.32]\n",
            "Epoch 7, batch: 4300/5760 (34.10 s), loss: 124.77, kl: [2.00, 2.49, 0.00, 0.00, 0.00, 0.86, 0.00, 3.14, 0.00, 2.32]\n",
            "Epoch 7, batch: 4400/5760 (33.97 s), loss: 124.34, kl: [2.01, 2.49, 0.00, 0.00, 0.00, 0.87, 0.00, 3.15, 0.00, 2.33]\n",
            "Epoch 7, batch: 4500/5760 (33.89 s), loss: 124.44, kl: [2.01, 2.49, 0.00, 0.00, 0.00, 0.87, 0.00, 3.14, 0.00, 2.34]\n",
            "Epoch 7, batch: 4600/5760 (34.02 s), loss: 124.59, kl: [2.00, 2.49, 0.00, 0.00, 0.00, 0.88, 0.00, 3.14, 0.00, 2.33]\n",
            "Epoch 7, batch: 4700/5760 (34.57 s), loss: 124.28, kl: [2.01, 2.49, 0.00, 0.00, 0.00, 0.87, 0.00, 3.14, 0.00, 2.31]\n",
            "Epoch 7, batch: 4800/5760 (34.16 s), loss: 124.36, kl: [2.00, 2.49, 0.00, 0.00, 0.00, 0.87, 0.00, 3.14, 0.00, 2.32]\n",
            "Epoch 7, batch: 4900/5760 (34.30 s), loss: 124.40, kl: [2.00, 2.48, 0.00, 0.00, 0.00, 0.87, 0.00, 3.14, 0.00, 2.32]\n",
            "Epoch 7, batch: 5000/5760 (34.04 s), loss: 124.46, kl: [2.00, 2.50, 0.00, 0.00, 0.00, 0.87, 0.00, 3.14, 0.00, 2.31]\n",
            "Epoch 7, batch: 5100/5760 (34.46 s), loss: 124.32, kl: [1.99, 2.49, 0.00, 0.00, 0.00, 0.87, 0.00, 3.14, 0.00, 2.31]\n",
            "Epoch 7, batch: 5200/5760 (34.13 s), loss: 124.44, kl: [1.98, 2.50, 0.00, 0.00, 0.00, 0.87, 0.00, 3.14, 0.00, 2.32]\n",
            "Epoch 7, batch: 5300/5760 (34.17 s), loss: 124.49, kl: [1.99, 2.50, 0.00, 0.00, 0.00, 0.87, 0.00, 3.14, 0.00, 2.32]\n",
            "Epoch 7, batch: 5400/5760 (34.35 s), loss: 124.21, kl: [1.99, 2.49, 0.00, 0.00, 0.00, 0.87, 0.00, 3.14, 0.00, 2.32]\n",
            "Epoch 7, batch: 5500/5760 (34.21 s), loss: 124.11, kl: [1.99, 2.49, 0.00, 0.00, 0.00, 0.88, 0.00, 3.14, 0.00, 2.32]\n",
            "Epoch 7, batch: 5600/5760 (34.40 s), loss: 124.33, kl: [2.00, 2.49, 0.00, 0.00, 0.00, 0.88, 0.00, 3.15, 0.00, 2.32]\n",
            "Epoch 7, batch: 5700/5760 (34.01 s), loss: 124.36, kl: [2.00, 2.49, 0.00, 0.00, 0.00, 0.88, 0.00, 3.15, 0.00, 2.32]\n",
            "Epoch 8, batch: 0/5760 (20.67 s), loss: 124.35, kl: [2.00, 2.49, 0.00, 0.00, 0.00, 0.88, 0.00, 3.15, 0.00, 2.33]\n",
            "Epoch 8, batch: 100/5760 (35.01 s), loss: 124.10, kl: [1.99, 2.49, 0.00, 0.00, 0.00, 0.88, 0.00, 3.14, 0.00, 2.34]\n",
            "Epoch 8, batch: 200/5760 (34.34 s), loss: 124.19, kl: [1.98, 2.50, 0.00, 0.00, 0.00, 0.87, 0.00, 3.14, 0.00, 2.34]\n",
            "Epoch 8, batch: 300/5760 (34.40 s), loss: 124.18, kl: [1.99, 2.50, 0.00, 0.00, 0.00, 0.87, 0.00, 3.15, 0.00, 2.33]\n",
            "Epoch 8, batch: 400/5760 (34.52 s), loss: 123.98, kl: [2.00, 2.49, 0.00, 0.00, 0.00, 0.88, 0.00, 3.15, 0.00, 2.34]\n",
            "Epoch 8, batch: 500/5760 (34.41 s), loss: 123.94, kl: [1.99, 2.49, 0.00, 0.00, 0.00, 0.87, 0.00, 3.14, 0.00, 2.33]\n",
            "Epoch 8, batch: 600/5760 (34.66 s), loss: 123.95, kl: [2.00, 2.49, 0.00, 0.00, 0.00, 0.88, 0.00, 3.13, 0.00, 2.33]\n",
            "Epoch 8, batch: 700/5760 (34.71 s), loss: 124.29, kl: [2.00, 2.50, 0.00, 0.00, 0.00, 0.87, 0.00, 3.13, 0.00, 2.32]\n",
            "Epoch 8, batch: 800/5760 (35.09 s), loss: 124.35, kl: [2.00, 2.50, 0.00, 0.00, 0.00, 0.88, 0.00, 3.13, 0.00, 2.33]\n",
            "Epoch 8, batch: 900/5760 (34.73 s), loss: 124.10, kl: [1.99, 2.50, 0.00, 0.00, 0.00, 0.87, 0.00, 3.14, 0.00, 2.34]\n",
            "Epoch 8, batch: 1000/5760 (34.77 s), loss: 124.00, kl: [1.99, 2.50, 0.00, 0.00, 0.00, 0.87, 0.00, 3.13, 0.00, 2.33]\n",
            "Epoch 8, batch: 1100/5760 (34.89 s), loss: 123.89, kl: [1.99, 2.49, 0.00, 0.00, 0.00, 0.87, 0.00, 3.14, 0.00, 2.33]\n",
            "Epoch 8, batch: 1200/5760 (34.59 s), loss: 123.80, kl: [1.99, 2.49, 0.00, 0.00, 0.00, 0.88, 0.00, 3.14, 0.00, 2.34]\n",
            "Epoch 8, batch: 1300/5760 (34.48 s), loss: 124.22, kl: [2.01, 2.50, 0.00, 0.00, 0.00, 0.87, 0.00, 3.15, 0.00, 2.34]\n",
            "Epoch 8, batch: 1400/5760 (34.47 s), loss: 123.93, kl: [2.00, 2.50, 0.00, 0.00, 0.00, 0.87, 0.00, 3.13, 0.00, 2.33]\n",
            "Epoch 8, batch: 1500/5760 (34.39 s), loss: 123.55, kl: [1.99, 2.49, 0.00, 0.00, 0.00, 0.87, 0.00, 3.14, 0.00, 2.33]\n",
            "Epoch 8, batch: 1600/5760 (34.44 s), loss: 123.78, kl: [1.99, 2.49, 0.00, 0.00, 0.00, 0.87, 0.00, 3.14, 0.00, 2.33]\n",
            "Epoch 8, batch: 1700/5760 (35.05 s), loss: 123.95, kl: [2.00, 2.49, 0.00, 0.00, 0.00, 0.88, 0.00, 3.13, 0.00, 2.33]\n",
            "Epoch 8, batch: 1800/5760 (34.47 s), loss: 123.89, kl: [2.00, 2.49, 0.00, 0.00, 0.00, 0.87, 0.00, 3.14, 0.00, 2.34]\n",
            "Epoch 8, batch: 1900/5760 (34.00 s), loss: 123.62, kl: [2.02, 2.49, 0.00, 0.00, 0.00, 0.87, 0.00, 3.13, 0.00, 2.33]\n",
            "Epoch 8, batch: 2000/5760 (33.89 s), loss: 123.85, kl: [2.01, 2.48, 0.00, 0.00, 0.00, 0.88, 0.00, 3.14, 0.00, 2.33]\n",
            "Epoch 8, batch: 2100/5760 (34.51 s), loss: 123.82, kl: [2.01, 2.48, 0.00, 0.00, 0.00, 0.88, 0.00, 3.13, 0.00, 2.32]\n",
            "Epoch 8, batch: 2200/5760 (34.00 s), loss: 124.00, kl: [2.01, 2.50, 0.00, 0.00, 0.00, 0.88, 0.00, 3.14, 0.00, 2.32]\n",
            "Epoch 8, batch: 2300/5760 (34.13 s), loss: 123.80, kl: [2.01, 2.49, 0.00, 0.00, 0.00, 0.88, 0.00, 3.14, 0.00, 2.32]\n",
            "Epoch 8, batch: 2400/5760 (34.17 s), loss: 123.71, kl: [2.00, 2.51, 0.00, 0.00, 0.00, 0.88, 0.00, 3.14, 0.00, 2.33]\n",
            "Epoch 8, batch: 2500/5760 (33.94 s), loss: 123.80, kl: [1.99, 2.50, 0.00, 0.00, 0.00, 0.88, 0.00, 3.14, 0.00, 2.34]\n",
            "Epoch 8, batch: 2600/5760 (34.37 s), loss: 124.10, kl: [2.00, 2.50, 0.00, 0.00, 0.00, 0.88, 0.00, 3.14, 0.00, 2.35]\n",
            "Epoch 8, batch: 2700/5760 (34.29 s), loss: 123.80, kl: [2.00, 2.50, 0.00, 0.00, 0.00, 0.88, 0.00, 3.15, 0.00, 2.35]\n",
            "Epoch 8, batch: 2800/5760 (34.00 s), loss: 123.63, kl: [1.99, 2.49, 0.00, 0.00, 0.00, 0.88, 0.00, 3.15, 0.00, 2.33]\n",
            "Epoch 8, batch: 2900/5760 (33.98 s), loss: 123.34, kl: [1.98, 2.49, 0.00, 0.00, 0.00, 0.88, 0.00, 3.15, 0.00, 2.34]\n",
            "Epoch 8, batch: 3000/5760 (33.86 s), loss: 123.41, kl: [2.00, 2.49, 0.00, 0.00, 0.00, 0.88, 0.00, 3.14, 0.00, 2.33]\n",
            "Epoch 8, batch: 3100/5760 (34.51 s), loss: 123.71, kl: [1.98, 2.50, 0.00, 0.00, 0.00, 0.88, 0.00, 3.14, 0.00, 2.33]\n",
            "Epoch 8, batch: 3200/5760 (33.95 s), loss: 123.74, kl: [1.99, 2.49, 0.00, 0.00, 0.00, 0.88, 0.00, 3.15, 0.00, 2.33]\n",
            "Epoch 8, batch: 3300/5760 (33.95 s), loss: 123.72, kl: [2.00, 2.49, 0.00, 0.00, 0.00, 0.88, 0.00, 3.15, 0.00, 2.33]\n",
            "Epoch 8, batch: 3400/5760 (34.06 s), loss: 123.81, kl: [2.01, 2.49, 0.00, 0.00, 0.00, 0.88, 0.00, 3.15, 0.00, 2.33]\n",
            "Epoch 8, batch: 3500/5760 (34.51 s), loss: 123.96, kl: [2.01, 2.49, 0.00, 0.00, 0.00, 0.88, 0.00, 3.15, 0.00, 2.33]\n",
            "Epoch 8, batch: 3600/5760 (34.29 s), loss: 123.44, kl: [2.01, 2.48, 0.00, 0.00, 0.00, 0.88, 0.00, 3.15, 0.00, 2.32]\n",
            "Epoch 8, batch: 3700/5760 (34.08 s), loss: 123.52, kl: [2.01, 2.50, 0.00, 0.00, 0.00, 0.88, 0.00, 3.15, 0.00, 2.31]\n",
            "Epoch 8, batch: 3800/5760 (34.21 s), loss: 123.41, kl: [2.00, 2.50, 0.00, 0.00, 0.00, 0.88, 0.00, 3.15, 0.00, 2.32]\n",
            "Epoch 8, batch: 3900/5760 (33.99 s), loss: 123.78, kl: [1.99, 2.50, 0.00, 0.00, 0.00, 0.88, 0.00, 3.15, 0.00, 2.33]\n",
            "Epoch 8, batch: 4000/5760 (33.84 s), loss: 123.58, kl: [1.99, 2.50, 0.00, 0.00, 0.00, 0.88, 0.00, 3.16, 0.00, 2.34]\n",
            "Epoch 8, batch: 4100/5760 (34.26 s), loss: 123.51, kl: [2.00, 2.49, 0.00, 0.00, 0.00, 0.89, 0.00, 3.15, 0.00, 2.34]\n",
            "Epoch 8, batch: 4200/5760 (33.92 s), loss: 123.69, kl: [2.01, 2.49, 0.00, 0.00, 0.00, 0.90, 0.00, 3.15, 0.00, 2.33]\n",
            "Epoch 8, batch: 4300/5760 (33.99 s), loss: 123.81, kl: [2.01, 2.50, 0.00, 0.00, 0.00, 0.89, 0.00, 3.15, 0.00, 2.34]\n",
            "Epoch 8, batch: 4400/5760 (34.51 s), loss: 123.64, kl: [2.01, 2.49, 0.00, 0.00, 0.00, 0.89, 0.00, 3.14, 0.00, 2.33]\n",
            "Epoch 8, batch: 4500/5760 (34.44 s), loss: 123.30, kl: [2.01, 2.49, 0.00, 0.00, 0.00, 0.88, 0.00, 3.14, 0.00, 2.33]\n",
            "Epoch 8, batch: 4600/5760 (34.24 s), loss: 123.64, kl: [2.00, 2.49, 0.00, 0.00, 0.00, 0.88, 0.00, 3.15, 0.00, 2.34]\n",
            "Epoch 8, batch: 4700/5760 (33.81 s), loss: 123.65, kl: [2.00, 2.50, 0.00, 0.00, 0.00, 0.88, 0.00, 3.15, 0.00, 2.33]\n",
            "Epoch 8, batch: 4800/5760 (33.96 s), loss: 123.78, kl: [2.00, 2.49, 0.00, 0.00, 0.00, 0.89, 0.00, 3.16, 0.00, 2.34]\n",
            "Epoch 8, batch: 4900/5760 (33.89 s), loss: 123.90, kl: [1.99, 2.49, 0.00, 0.00, 0.00, 0.89, 0.00, 3.15, 0.00, 2.34]\n",
            "Epoch 8, batch: 5000/5760 (34.07 s), loss: 123.77, kl: [2.00, 2.50, 0.00, 0.00, 0.00, 0.89, 0.00, 3.16, 0.00, 2.35]\n",
            "Epoch 8, batch: 5100/5760 (34.37 s), loss: 123.74, kl: [2.00, 2.49, 0.00, 0.00, 0.00, 0.89, 0.00, 3.15, 0.00, 2.34]\n",
            "Epoch 8, batch: 5200/5760 (34.22 s), loss: 123.37, kl: [2.00, 2.49, 0.00, 0.00, 0.00, 0.89, 0.00, 3.14, 0.00, 2.34]\n",
            "Epoch 8, batch: 5300/5760 (34.64 s), loss: 123.47, kl: [2.00, 2.49, 0.00, 0.00, 0.00, 0.89, 0.00, 3.14, 0.00, 2.34]\n",
            "Epoch 8, batch: 5400/5760 (34.26 s), loss: 123.24, kl: [2.00, 2.50, 0.00, 0.00, 0.00, 0.89, 0.00, 3.15, 0.00, 2.34]\n",
            "Epoch 8, batch: 5500/5760 (33.97 s), loss: 123.36, kl: [1.99, 2.50, 0.00, 0.00, 0.00, 0.89, 0.00, 3.15, 0.00, 2.35]\n",
            "Epoch 8, batch: 5600/5760 (33.87 s), loss: 123.43, kl: [2.00, 2.50, 0.00, 0.00, 0.00, 0.89, 0.00, 3.15, 0.00, 2.34]\n",
            "Epoch 8, batch: 5700/5760 (33.87 s), loss: 123.43, kl: [2.01, 2.50, 0.00, 0.00, 0.00, 0.89, 0.00, 3.15, 0.00, 2.34]\n",
            "Epoch 9, batch: 0/5760 (20.55 s), loss: 123.44, kl: [2.01, 2.51, 0.00, 0.00, 0.00, 0.88, 0.00, 3.15, 0.00, 2.34]\n",
            "Epoch 9, batch: 100/5760 (34.53 s), loss: 123.14, kl: [2.01, 2.50, 0.00, 0.00, 0.00, 0.89, 0.00, 3.15, 0.00, 2.34]\n",
            "Epoch 9, batch: 200/5760 (34.27 s), loss: 123.26, kl: [2.02, 2.50, 0.00, 0.00, 0.00, 0.89, 0.00, 3.15, 0.00, 2.34]\n",
            "Epoch 9, batch: 300/5760 (34.18 s), loss: 123.65, kl: [2.01, 2.49, 0.00, 0.00, 0.00, 0.88, 0.00, 3.15, 0.00, 2.34]\n",
            "Epoch 9, batch: 400/5760 (34.14 s), loss: 123.27, kl: [1.99, 2.50, 0.00, 0.00, 0.00, 0.88, 0.00, 3.14, 0.00, 2.35]\n",
            "Epoch 9, batch: 500/5760 (34.65 s), loss: 123.27, kl: [2.00, 2.49, 0.00, 0.00, 0.00, 0.88, 0.00, 3.14, 0.00, 2.34]\n",
            "Epoch 9, batch: 600/5760 (34.25 s), loss: 123.55, kl: [2.01, 2.51, 0.00, 0.00, 0.00, 0.89, 0.00, 3.14, 0.00, 2.34]\n",
            "Epoch 9, batch: 700/5760 (34.03 s), loss: 123.18, kl: [2.01, 2.50, 0.00, 0.00, 0.00, 0.89, 0.00, 3.15, 0.00, 2.34]\n",
            "Epoch 9, batch: 800/5760 (33.87 s), loss: 123.21, kl: [2.00, 2.50, 0.00, 0.00, 0.00, 0.89, 0.00, 3.15, 0.00, 2.34]\n",
            "Epoch 9, batch: 900/5760 (33.98 s), loss: 123.63, kl: [2.01, 2.50, 0.00, 0.00, 0.00, 0.89, 0.00, 3.15, 0.00, 2.34]\n",
            "Epoch 9, batch: 1000/5760 (34.17 s), loss: 123.27, kl: [2.00, 2.49, 0.00, 0.00, 0.00, 0.90, 0.00, 3.15, 0.00, 2.34]\n",
            "Epoch 9, batch: 1100/5760 (34.27 s), loss: 123.40, kl: [2.00, 2.49, 0.00, 0.00, 0.00, 0.90, 0.00, 3.15, 0.00, 2.33]\n",
            "Epoch 9, batch: 1200/5760 (34.37 s), loss: 123.71, kl: [2.00, 2.51, 0.00, 0.00, 0.00, 0.90, 0.00, 3.15, 0.00, 2.33]\n",
            "Epoch 9, batch: 1300/5760 (34.05 s), loss: 123.04, kl: [1.99, 2.50, 0.00, 0.00, 0.00, 0.90, 0.00, 3.14, 0.00, 2.34]\n",
            "Epoch 9, batch: 1400/5760 (34.51 s), loss: 123.12, kl: [2.00, 2.51, 0.00, 0.00, 0.00, 0.90, 0.00, 3.15, 0.00, 2.34]\n",
            "Epoch 9, batch: 1500/5760 (34.23 s), loss: 123.27, kl: [1.98, 2.51, 0.00, 0.00, 0.00, 0.90, 0.00, 3.15, 0.00, 2.36]\n",
            "Epoch 9, batch: 1600/5760 (34.06 s), loss: 123.05, kl: [1.99, 2.50, 0.00, 0.00, 0.00, 0.89, 0.00, 3.15, 0.00, 2.34]\n",
            "Epoch 9, batch: 1700/5760 (33.92 s), loss: 123.20, kl: [1.99, 2.50, 0.00, 0.00, 0.00, 0.89, 0.00, 3.16, 0.00, 2.35]\n",
            "Epoch 9, batch: 1800/5760 (33.98 s), loss: 123.36, kl: [2.00, 2.50, 0.00, 0.00, 0.00, 0.90, 0.00, 3.15, 0.00, 2.35]\n",
            "Epoch 9, batch: 1900/5760 (34.26 s), loss: 123.21, kl: [2.01, 2.50, 0.00, 0.00, 0.00, 0.90, 0.00, 3.14, 0.00, 2.33]\n",
            "Epoch 9, batch: 2000/5760 (33.95 s), loss: 122.87, kl: [2.00, 2.49, 0.00, 0.00, 0.00, 0.89, 0.00, 3.14, 0.00, 2.34]\n",
            "Epoch 9, batch: 2100/5760 (34.47 s), loss: 123.19, kl: [1.99, 2.49, 0.00, 0.00, 0.00, 0.89, 0.00, 3.15, 0.00, 2.35]\n",
            "Epoch 9, batch: 2200/5760 (34.02 s), loss: 123.27, kl: [1.99, 2.49, 0.00, 0.00, 0.00, 0.90, 0.00, 3.14, 0.00, 2.35]\n",
            "Epoch 9, batch: 2300/5760 (34.71 s), loss: 123.22, kl: [2.00, 2.49, 0.00, 0.00, 0.00, 0.90, 0.00, 3.14, 0.00, 2.34]\n",
            "Epoch 9, batch: 2400/5760 (34.33 s), loss: 123.13, kl: [2.00, 2.50, 0.00, 0.00, 0.00, 0.91, 0.00, 3.15, 0.00, 2.34]\n",
            "Epoch 9, batch: 2500/5760 (33.98 s), loss: 122.89, kl: [2.00, 2.49, 0.00, 0.00, 0.00, 0.91, 0.00, 3.14, 0.00, 2.34]\n",
            "Epoch 9, batch: 2600/5760 (33.80 s), loss: 123.21, kl: [1.99, 2.50, 0.00, 0.00, 0.00, 0.90, 0.00, 3.15, 0.00, 2.35]\n",
            "Epoch 9, batch: 2700/5760 (33.88 s), loss: 122.99, kl: [2.00, 2.50, 0.00, 0.00, 0.00, 0.91, 0.00, 3.14, 0.00, 2.35]\n",
            "Epoch 9, batch: 2800/5760 (34.05 s), loss: 122.91, kl: [2.00, 2.50, 0.00, 0.00, 0.00, 0.91, 0.00, 3.15, 0.00, 2.35]\n",
            "Epoch 9, batch: 2900/5760 (34.01 s), loss: 123.39, kl: [2.00, 2.51, 0.00, 0.00, 0.00, 0.89, 0.00, 3.15, 0.00, 2.34]\n",
            "Epoch 9, batch: 3000/5760 (34.04 s), loss: 123.01, kl: [2.00, 2.50, 0.00, 0.00, 0.00, 0.89, 0.00, 3.15, 0.00, 2.34]\n",
            "Epoch 9, batch: 3100/5760 (34.08 s), loss: 123.04, kl: [2.00, 2.51, 0.00, 0.00, 0.00, 0.90, 0.00, 3.14, 0.00, 2.36]\n",
            "Epoch 9, batch: 3200/5760 (34.18 s), loss: 122.83, kl: [2.00, 2.49, 0.00, 0.00, 0.00, 0.89, 0.00, 3.14, 0.00, 2.36]\n",
            "Epoch 9, batch: 3300/5760 (34.51 s), loss: 122.83, kl: [2.00, 2.49, 0.00, 0.00, 0.00, 0.88, 0.00, 3.14, 0.00, 2.34]\n",
            "Epoch 9, batch: 3400/5760 (33.66 s), loss: 123.06, kl: [2.00, 2.49, 0.00, 0.00, 0.00, 0.90, 0.00, 3.15, 0.00, 2.35]\n",
            "Epoch 9, batch: 3500/5760 (33.85 s), loss: 122.97, kl: [1.99, 2.49, 0.00, 0.00, 0.00, 0.90, 0.00, 3.15, 0.00, 2.35]\n",
            "Epoch 9, batch: 3600/5760 (33.84 s), loss: 122.80, kl: [1.98, 2.49, 0.00, 0.00, 0.00, 0.90, 0.00, 3.15, 0.00, 2.35]\n",
            "Epoch 9, batch: 3700/5760 (33.76 s), loss: 123.21, kl: [1.99, 2.50, 0.00, 0.00, 0.00, 0.90, 0.00, 3.15, 0.00, 2.35]\n",
            "Epoch 9, batch: 3800/5760 (33.69 s), loss: 123.16, kl: [1.99, 2.50, 0.00, 0.00, 0.00, 0.91, 0.00, 3.15, 0.00, 2.35]\n",
            "Epoch 9, batch: 3900/5760 (33.79 s), loss: 122.97, kl: [1.99, 2.48, 0.00, 0.00, 0.00, 0.90, 0.00, 3.15, 0.00, 2.35]\n",
            "Epoch 9, batch: 4000/5760 (34.01 s), loss: 122.93, kl: [1.99, 2.49, 0.00, 0.00, 0.00, 0.89, 0.00, 3.16, 0.00, 2.35]\n",
            "Epoch 9, batch: 4100/5760 (34.53 s), loss: 122.97, kl: [1.99, 2.50, 0.00, 0.00, 0.00, 0.89, 0.00, 3.16, 0.00, 2.34]\n",
            "Epoch 9, batch: 4200/5760 (34.30 s), loss: 123.08, kl: [2.00, 2.50, 0.00, 0.00, 0.00, 0.88, 0.00, 3.16, 0.00, 2.33]\n",
            "Epoch 9, batch: 4300/5760 (33.87 s), loss: 122.96, kl: [2.00, 2.50, 0.00, 0.00, 0.00, 0.89, 0.00, 3.16, 0.00, 2.35]\n",
            "Epoch 9, batch: 4400/5760 (34.04 s), loss: 122.95, kl: [2.01, 2.50, 0.00, 0.00, 0.00, 0.90, 0.00, 3.16, 0.00, 2.34]\n",
            "Epoch 9, batch: 4500/5760 (34.16 s), loss: 122.82, kl: [1.99, 2.50, 0.00, 0.00, 0.00, 0.90, 0.00, 3.16, 0.00, 2.35]\n",
            "Epoch 9, batch: 4600/5760 (33.73 s), loss: 123.20, kl: [2.00, 2.50, 0.00, 0.00, 0.00, 0.90, 0.00, 3.16, 0.00, 2.35]\n",
            "Epoch 9, batch: 4700/5760 (33.88 s), loss: 123.20, kl: [2.01, 2.51, 0.00, 0.00, 0.00, 0.90, 0.00, 3.16, 0.00, 2.35]\n",
            "Epoch 9, batch: 4800/5760 (33.94 s), loss: 123.12, kl: [1.98, 2.51, 0.00, 0.00, 0.00, 0.90, 0.00, 3.15, 0.00, 2.35]\n",
            "Epoch 9, batch: 4900/5760 (34.22 s), loss: 122.72, kl: [1.98, 2.50, 0.00, 0.00, 0.00, 0.89, 0.00, 3.15, 0.00, 2.35]\n",
            "Epoch 9, batch: 5000/5760 (34.26 s), loss: 122.93, kl: [1.98, 2.50, 0.00, 0.00, 0.00, 0.89, 0.00, 3.15, 0.00, 2.35]\n",
            "Epoch 9, batch: 5100/5760 (34.96 s), loss: 122.83, kl: [2.00, 2.50, 0.00, 0.00, 0.00, 0.89, 0.00, 3.16, 0.00, 2.33]\n",
            "Epoch 9, batch: 5200/5760 (34.13 s), loss: 122.61, kl: [1.99, 2.49, 0.00, 0.00, 0.00, 0.89, 0.00, 3.15, 0.00, 2.35]\n",
            "Epoch 9, batch: 5300/5760 (33.87 s), loss: 123.02, kl: [1.99, 2.49, 0.00, 0.00, 0.00, 0.89, 0.00, 3.15, 0.00, 2.36]\n",
            "Epoch 9, batch: 5400/5760 (34.10 s), loss: 122.99, kl: [2.01, 2.50, 0.00, 0.00, 0.00, 0.89, 0.00, 3.16, 0.00, 2.35]\n",
            "Epoch 9, batch: 5500/5760 (34.00 s), loss: 122.80, kl: [2.00, 2.50, 0.00, 0.00, 0.00, 0.90, 0.00, 3.16, 0.00, 2.34]\n",
            "Epoch 9, batch: 5600/5760 (34.17 s), loss: 122.85, kl: [2.00, 2.49, 0.00, 0.00, 0.00, 0.89, 0.00, 3.17, 0.00, 2.35]\n",
            "Epoch 9, batch: 5700/5760 (33.99 s), loss: 122.69, kl: [2.00, 2.49, 0.00, 0.00, 0.00, 0.89, 0.00, 3.16, 0.00, 2.35]\n",
            "Epoch 10, batch: 0/5760 (20.41 s), loss: 122.90, kl: [2.01, 2.49, 0.00, 0.00, 0.00, 0.89, 0.00, 3.16, 0.00, 2.35]\n",
            "Epoch 10, batch: 100/5760 (34.25 s), loss: 122.96, kl: [2.01, 2.50, 0.00, 0.00, 0.00, 0.90, 0.00, 3.16, 0.00, 2.34]\n",
            "Epoch 10, batch: 200/5760 (34.84 s), loss: 123.04, kl: [2.00, 2.50, 0.00, 0.00, 0.00, 0.90, 0.00, 3.15, 0.00, 2.35]\n",
            "Epoch 10, batch: 300/5760 (34.28 s), loss: 122.59, kl: [2.01, 2.49, 0.00, 0.00, 0.00, 0.90, 0.00, 3.15, 0.00, 2.34]\n",
            "Epoch 10, batch: 400/5760 (34.04 s), loss: 122.72, kl: [2.00, 2.50, 0.00, 0.00, 0.00, 0.90, 0.00, 3.15, 0.00, 2.33]\n",
            "Epoch 10, batch: 500/5760 (33.97 s), loss: 122.62, kl: [2.00, 2.50, 0.00, 0.00, 0.00, 0.90, 0.00, 3.15, 0.00, 2.34]\n",
            "Epoch 10, batch: 600/5760 (33.96 s), loss: 122.59, kl: [2.01, 2.50, 0.00, 0.00, 0.00, 0.90, 0.00, 3.16, 0.00, 2.34]\n",
            "Epoch 10, batch: 700/5760 (34.01 s), loss: 122.64, kl: [2.01, 2.50, 0.00, 0.00, 0.00, 0.89, 0.00, 3.15, 0.00, 2.35]\n",
            "Epoch 10, batch: 800/5760 (34.09 s), loss: 122.59, kl: [2.00, 2.50, 0.00, 0.00, 0.00, 0.88, 0.00, 3.15, 0.00, 2.35]\n",
            "Epoch 10, batch: 900/5760 (33.71 s), loss: 122.58, kl: [1.98, 2.50, 0.00, 0.00, 0.00, 0.90, 0.00, 3.15, 0.00, 2.35]\n",
            "Epoch 10, batch: 1000/5760 (33.81 s), loss: 122.52, kl: [1.99, 2.50, 0.00, 0.00, 0.00, 0.90, 0.00, 3.15, 0.00, 2.35]\n",
            "Epoch 10, batch: 1100/5760 (34.68 s), loss: 122.14, kl: [2.00, 2.50, 0.00, 0.00, 0.00, 0.90, 0.00, 3.15, 0.00, 2.33]\n",
            "Epoch 10, batch: 1200/5760 (34.02 s), loss: 122.53, kl: [2.00, 2.50, 0.00, 0.00, 0.00, 0.90, 0.00, 3.15, 0.00, 2.34]\n",
            "Epoch 10, batch: 1300/5760 (33.76 s), loss: 122.63, kl: [2.00, 2.50, 0.00, 0.00, 0.00, 0.90, 0.00, 3.15, 0.00, 2.34]\n",
            "Epoch 10, batch: 1400/5760 (34.02 s), loss: 122.55, kl: [2.00, 2.50, 0.00, 0.00, 0.00, 0.90, 0.00, 3.16, 0.00, 2.34]\n",
            "Epoch 10, batch: 1500/5760 (34.27 s), loss: 122.94, kl: [1.99, 2.51, 0.00, 0.00, 0.00, 0.90, 0.00, 3.16, 0.00, 2.34]\n",
            "Epoch 10, batch: 1600/5760 (33.75 s), loss: 122.94, kl: [1.98, 2.51, 0.00, 0.00, 0.00, 0.90, 0.00, 3.15, 0.00, 2.35]\n",
            "Epoch 10, batch: 1700/5760 (34.06 s), loss: 122.99, kl: [1.99, 2.51, 0.00, 0.00, 0.00, 0.91, 0.00, 3.16, 0.00, 2.35]\n",
            "Epoch 10, batch: 1800/5760 (34.04 s), loss: 122.44, kl: [1.99, 2.51, 0.00, 0.00, 0.00, 0.91, 0.00, 3.15, 0.00, 2.35]\n",
            "Epoch 10, batch: 1900/5760 (34.00 s), loss: 122.13, kl: [2.00, 2.50, 0.00, 0.00, 0.00, 0.91, 0.00, 3.15, 0.00, 2.36]\n",
            "Epoch 10, batch: 2000/5760 (34.38 s), loss: 122.22, kl: [2.02, 2.50, 0.00, 0.00, 0.00, 0.91, 0.00, 3.15, 0.00, 2.34]\n",
            "Epoch 10, batch: 2100/5760 (34.86 s), loss: 122.54, kl: [2.02, 2.51, 0.00, 0.00, 0.00, 0.91, 0.00, 3.15, 0.00, 2.35]\n",
            "Epoch 10, batch: 2200/5760 (33.67 s), loss: 122.49, kl: [2.02, 2.50, 0.00, 0.00, 0.00, 0.90, 0.00, 3.15, 0.00, 2.35]\n",
            "Epoch 10, batch: 2300/5760 (34.14 s), loss: 122.58, kl: [2.01, 2.51, 0.00, 0.00, 0.00, 0.90, 0.00, 3.16, 0.00, 2.36]\n",
            "Epoch 10, batch: 2400/5760 (34.07 s), loss: 122.37, kl: [2.00, 2.50, 0.00, 0.00, 0.00, 0.91, 0.00, 3.16, 0.00, 2.35]\n",
            "Epoch 10, batch: 2500/5760 (33.97 s), loss: 122.22, kl: [2.00, 2.50, 0.00, 0.00, 0.00, 0.92, 0.00, 3.16, 0.00, 2.36]\n",
            "Epoch 10, batch: 2600/5760 (33.44 s), loss: 122.33, kl: [2.00, 2.50, 0.00, 0.00, 0.00, 0.91, 0.00, 3.15, 0.00, 2.36]\n",
            "Epoch 10, batch: 2700/5760 (33.81 s), loss: 122.70, kl: [2.00, 2.50, 0.00, 0.00, 0.00, 0.90, 0.00, 3.16, 0.00, 2.37]\n",
            "Epoch 10, batch: 2800/5760 (33.82 s), loss: 122.52, kl: [2.01, 2.50, 0.00, 0.00, 0.00, 0.90, 0.00, 3.16, 0.00, 2.35]\n",
            "Epoch 10, batch: 2900/5760 (34.35 s), loss: 122.76, kl: [2.01, 2.50, 0.00, 0.00, 0.00, 0.91, 0.00, 3.16, 0.00, 2.37]\n",
            "Epoch 10, batch: 3000/5760 (35.11 s), loss: 122.47, kl: [2.00, 2.49, 0.00, 0.00, 0.00, 0.90, 0.00, 3.16, 0.00, 2.37]\n",
            "Epoch 10, batch: 3100/5760 (34.42 s), loss: 122.61, kl: [2.00, 2.49, 0.00, 0.00, 0.00, 0.90, 0.00, 3.17, 0.00, 2.37]\n",
            "Epoch 10, batch: 3200/5760 (34.13 s), loss: 122.59, kl: [1.99, 2.50, 0.00, 0.00, 0.00, 0.91, 0.00, 3.17, 0.00, 2.37]\n",
            "Epoch 10, batch: 3300/5760 (34.05 s), loss: 122.80, kl: [2.00, 2.50, 0.00, 0.00, 0.00, 0.90, 0.00, 3.17, 0.00, 2.36]\n",
            "Epoch 10, batch: 3400/5760 (33.72 s), loss: 122.33, kl: [2.00, 2.50, 0.00, 0.00, 0.00, 0.89, 0.00, 3.16, 0.00, 2.36]\n",
            "Epoch 10, batch: 3500/5760 (33.81 s), loss: 122.51, kl: [2.01, 2.50, 0.00, 0.00, 0.00, 0.90, 0.00, 3.16, 0.00, 2.36]\n",
            "Epoch 10, batch: 3600/5760 (33.71 s), loss: 122.56, kl: [1.99, 2.49, 0.00, 0.00, 0.00, 0.91, 0.00, 3.16, 0.00, 2.36]\n",
            "Epoch 10, batch: 3700/5760 (34.00 s), loss: 122.39, kl: [1.99, 2.50, 0.00, 0.00, 0.00, 0.90, 0.00, 3.15, 0.00, 2.36]\n",
            "Epoch 10, batch: 3800/5760 (33.65 s), loss: 122.29, kl: [1.99, 2.49, 0.00, 0.00, 0.00, 0.90, 0.00, 3.15, 0.00, 2.35]\n",
            "Epoch 10, batch: 3900/5760 (35.20 s), loss: 122.46, kl: [2.00, 2.50, 0.00, 0.00, 0.00, 0.91, 0.00, 3.16, 0.00, 2.36]\n",
            "Epoch 10, batch: 4000/5760 (33.93 s), loss: 122.15, kl: [1.99, 2.49, 0.00, 0.00, 0.00, 0.90, 0.00, 3.16, 0.00, 2.35]\n",
            "Epoch 10, batch: 4100/5760 (34.07 s), loss: 122.30, kl: [2.00, 2.50, 0.00, 0.00, 0.00, 0.91, 0.00, 3.16, 0.00, 2.34]\n",
            "Epoch 10, batch: 4200/5760 (33.71 s), loss: 122.32, kl: [1.99, 2.50, 0.00, 0.00, 0.00, 0.91, 0.00, 3.16, 0.00, 2.35]\n",
            "Epoch 10, batch: 4300/5760 (33.88 s), loss: 122.22, kl: [1.98, 2.50, 0.00, 0.00, 0.00, 0.91, 0.00, 3.15, 0.00, 2.36]\n",
            "Epoch 10, batch: 4400/5760 (34.04 s), loss: 122.54, kl: [2.00, 2.49, 0.00, 0.00, 0.00, 0.91, 0.00, 3.16, 0.00, 2.36]\n",
            "Epoch 10, batch: 4500/5760 (33.89 s), loss: 121.97, kl: [1.99, 2.50, 0.00, 0.00, 0.00, 0.91, 0.00, 3.16, 0.00, 2.35]\n",
            "Epoch 10, batch: 4600/5760 (33.81 s), loss: 122.07, kl: [2.00, 2.49, 0.00, 0.00, 0.00, 0.92, 0.00, 3.15, 0.00, 2.35]\n",
            "Epoch 10, batch: 4700/5760 (34.03 s), loss: 122.27, kl: [2.01, 2.50, 0.00, 0.00, 0.00, 0.91, 0.00, 3.17, 0.00, 2.35]\n",
            "Epoch 10, batch: 4800/5760 (34.75 s), loss: 122.26, kl: [2.02, 2.50, 0.00, 0.00, 0.00, 0.91, 0.00, 3.17, 0.00, 2.35]\n",
            "Epoch 10, batch: 4900/5760 (34.18 s), loss: 122.19, kl: [2.00, 2.50, 0.00, 0.00, 0.00, 0.90, 0.00, 3.17, 0.00, 2.36]\n",
            "Epoch 10, batch: 5000/5760 (33.85 s), loss: 122.50, kl: [2.02, 2.51, 0.00, 0.00, 0.00, 0.91, 0.00, 3.16, 0.00, 2.37]\n",
            "Epoch 10, batch: 5100/5760 (34.18 s), loss: 122.33, kl: [2.01, 2.50, 0.00, 0.00, 0.00, 0.91, 0.00, 3.16, 0.00, 2.37]\n",
            "Epoch 10, batch: 5200/5760 (33.77 s), loss: 122.08, kl: [2.00, 2.51, 0.00, 0.00, 0.00, 0.91, 0.00, 3.15, 0.00, 2.36]\n",
            "Epoch 10, batch: 5300/5760 (33.57 s), loss: 122.12, kl: [1.99, 2.51, 0.00, 0.00, 0.00, 0.91, 0.00, 3.16, 0.00, 2.36]\n",
            "Epoch 10, batch: 5400/5760 (33.63 s), loss: 122.22, kl: [1.99, 2.51, 0.00, 0.00, 0.00, 0.91, 0.00, 3.16, 0.00, 2.36]\n",
            "Epoch 10, batch: 5500/5760 (34.10 s), loss: 122.41, kl: [2.01, 2.50, 0.00, 0.00, 0.00, 0.91, 0.00, 3.16, 0.00, 2.36]\n",
            "Epoch 10, batch: 5600/5760 (34.14 s), loss: 121.86, kl: [1.98, 2.50, 0.00, 0.00, 0.00, 0.90, 0.00, 3.15, 0.00, 2.36]\n",
            "Epoch 10, batch: 5700/5760 (34.50 s), loss: 122.38, kl: [2.00, 2.50, 0.00, 0.00, 0.00, 0.91, 0.00, 3.16, 0.00, 2.37]\n",
            "Epoch 11, batch: 0/5760 (20.33 s), loss: 122.52, kl: [2.02, 2.51, 0.00, 0.00, 0.00, 0.91, 0.00, 3.16, 0.00, 2.37]\n",
            "Epoch 11, batch: 100/5760 (33.88 s), loss: 122.16, kl: [2.01, 2.50, 0.00, 0.00, 0.00, 0.90, 0.00, 3.16, 0.00, 2.37]\n",
            "Epoch 11, batch: 200/5760 (33.96 s), loss: 122.15, kl: [2.00, 2.50, 0.00, 0.00, 0.00, 0.91, 0.00, 3.16, 0.00, 2.37]\n",
            "Epoch 11, batch: 300/5760 (34.05 s), loss: 122.02, kl: [1.98, 2.50, 0.00, 0.00, 0.00, 0.90, 0.00, 3.16, 0.00, 2.37]\n",
            "Epoch 11, batch: 400/5760 (33.80 s), loss: 122.04, kl: [1.99, 2.50, 0.00, 0.00, 0.00, 0.91, 0.00, 3.16, 0.00, 2.37]\n",
            "Epoch 11, batch: 500/5760 (33.46 s), loss: 122.46, kl: [1.99, 2.51, 0.00, 0.00, 0.00, 0.91, 0.00, 3.16, 0.00, 2.37]\n",
            "Epoch 11, batch: 600/5760 (33.60 s), loss: 122.16, kl: [1.99, 2.50, 0.00, 0.00, 0.00, 0.91, 0.00, 3.16, 0.00, 2.36]\n",
            "Epoch 11, batch: 700/5760 (33.80 s), loss: 121.99, kl: [1.99, 2.50, 0.00, 0.00, 0.00, 0.92, 0.00, 3.15, 0.00, 2.36]\n",
            "Epoch 11, batch: 800/5760 (33.80 s), loss: 122.08, kl: [2.00, 2.50, 0.00, 0.00, 0.00, 0.91, 0.00, 3.15, 0.00, 2.36]\n",
            "Epoch 11, batch: 900/5760 (34.48 s), loss: 122.05, kl: [1.99, 2.50, 0.00, 0.00, 0.00, 0.91, 0.00, 3.15, 0.00, 2.37]\n",
            "Epoch 11, batch: 1000/5760 (33.95 s), loss: 121.92, kl: [2.00, 2.49, 0.00, 0.00, 0.00, 0.91, 0.00, 3.16, 0.00, 2.35]\n",
            "Epoch 11, batch: 1100/5760 (34.09 s), loss: 121.65, kl: [1.99, 2.49, 0.00, 0.00, 0.00, 0.91, 0.00, 3.16, 0.00, 2.35]\n",
            "Epoch 11, batch: 1200/5760 (33.86 s), loss: 121.62, kl: [1.98, 2.49, 0.00, 0.00, 0.00, 0.90, 0.00, 3.16, 0.00, 2.35]\n",
            "Epoch 11, batch: 1300/5760 (34.05 s), loss: 121.77, kl: [1.99, 2.49, 0.00, 0.00, 0.00, 0.90, 0.00, 3.17, 0.00, 2.35]\n",
            "Epoch 11, batch: 1400/5760 (33.95 s), loss: 122.35, kl: [2.00, 2.50, 0.00, 0.00, 0.00, 0.92, 0.00, 3.16, 0.00, 2.37]\n",
            "Epoch 11, batch: 1500/5760 (33.93 s), loss: 122.19, kl: [2.01, 2.51, 0.00, 0.00, 0.00, 0.92, 0.00, 3.16, 0.00, 2.38]\n",
            "Epoch 11, batch: 1600/5760 (34.08 s), loss: 121.98, kl: [2.01, 2.50, 0.00, 0.00, 0.00, 0.92, 0.00, 3.15, 0.00, 2.36]\n",
            "Epoch 11, batch: 1700/5760 (34.49 s), loss: 122.06, kl: [2.01, 2.50, 0.00, 0.00, 0.00, 0.92, 0.00, 3.16, 0.00, 2.36]\n",
            "Epoch 11, batch: 1800/5760 (34.77 s), loss: 122.15, kl: [2.00, 2.50, 0.00, 0.00, 0.00, 0.92, 0.00, 3.16, 0.00, 2.36]\n",
            "Epoch 11, batch: 1900/5760 (34.37 s), loss: 122.08, kl: [2.01, 2.50, 0.00, 0.00, 0.00, 0.92, 0.00, 3.16, 0.00, 2.37]\n",
            "Epoch 11, batch: 2000/5760 (34.18 s), loss: 122.53, kl: [2.01, 2.50, 0.00, 0.00, 0.00, 0.91, 0.00, 3.16, 0.00, 2.37]\n",
            "Epoch 11, batch: 2100/5760 (34.07 s), loss: 122.12, kl: [2.00, 2.50, 0.00, 0.00, 0.00, 0.91, 0.00, 3.16, 0.00, 2.38]\n",
            "Epoch 11, batch: 2200/5760 (34.26 s), loss: 121.85, kl: [2.01, 2.49, 0.00, 0.00, 0.00, 0.90, 0.00, 3.16, 0.00, 2.36]\n",
            "Epoch 11, batch: 2300/5760 (34.22 s), loss: 121.87, kl: [2.00, 2.50, 0.00, 0.00, 0.00, 0.91, 0.00, 3.16, 0.00, 2.36]\n",
            "Epoch 11, batch: 2400/5760 (33.82 s), loss: 121.81, kl: [1.99, 2.50, 0.00, 0.00, 0.00, 0.92, 0.00, 3.16, 0.00, 2.36]\n",
            "Epoch 11, batch: 2500/5760 (34.00 s), loss: 121.55, kl: [1.99, 2.50, 0.00, 0.00, 0.00, 0.92, 0.00, 3.16, 0.00, 2.36]\n",
            "Epoch 11, batch: 2600/5760 (33.98 s), loss: 121.54, kl: [2.00, 2.49, 0.00, 0.00, 0.00, 0.92, 0.00, 3.16, 0.00, 2.37]\n",
            "Epoch 11, batch: 2700/5760 (34.71 s), loss: 121.59, kl: [1.99, 2.49, 0.00, 0.00, 0.00, 0.93, 0.00, 3.17, 0.00, 2.37]\n",
            "Epoch 11, batch: 2800/5760 (33.72 s), loss: 121.84, kl: [1.98, 2.49, 0.00, 0.00, 0.00, 0.91, 0.00, 3.17, 0.00, 2.37]\n",
            "Epoch 11, batch: 2900/5760 (34.00 s), loss: 121.93, kl: [2.00, 2.49, 0.00, 0.00, 0.00, 0.91, 0.00, 3.16, 0.00, 2.36]\n",
            "Epoch 11, batch: 3000/5760 (33.90 s), loss: 121.86, kl: [1.99, 2.50, 0.00, 0.00, 0.00, 0.92, 0.00, 3.16, 0.00, 2.35]\n",
            "Epoch 11, batch: 3100/5760 (34.36 s), loss: 121.85, kl: [2.00, 2.50, 0.00, 0.00, 0.00, 0.92, 0.00, 3.16, 0.00, 2.37]\n",
            "Epoch 11, batch: 3200/5760 (34.23 s), loss: 121.75, kl: [1.99, 2.50, 0.00, 0.00, 0.00, 0.92, 0.00, 3.16, 0.00, 2.37]\n",
            "Epoch 11, batch: 3300/5760 (33.86 s), loss: 121.95, kl: [1.99, 2.51, 0.00, 0.00, 0.00, 0.92, 0.00, 3.16, 0.00, 2.37]\n",
            "Epoch 11, batch: 3400/5760 (33.95 s), loss: 122.10, kl: [2.00, 2.50, 0.00, 0.00, 0.00, 0.91, 0.00, 3.17, 0.00, 2.37]\n",
            "Epoch 11, batch: 3500/5760 (34.21 s), loss: 121.40, kl: [2.01, 2.50, 0.00, 0.00, 0.00, 0.91, 0.00, 3.17, 0.00, 2.37]\n",
            "Epoch 11, batch: 3600/5760 (34.70 s), loss: 121.47, kl: [2.00, 2.49, 0.00, 0.00, 0.00, 0.92, 0.00, 3.17, 0.00, 2.37]\n",
            "Epoch 11, batch: 3700/5760 (33.83 s), loss: 121.85, kl: [1.99, 2.49, 0.00, 0.00, 0.00, 0.92, 0.00, 3.17, 0.00, 2.39]\n",
            "Epoch 11, batch: 3800/5760 (34.16 s), loss: 121.66, kl: [2.00, 2.49, 0.00, 0.00, 0.00, 0.90, 0.00, 3.16, 0.00, 2.38]\n",
            "Epoch 11, batch: 3900/5760 (33.99 s), loss: 122.05, kl: [2.00, 2.51, 0.00, 0.00, 0.00, 0.91, 0.00, 3.16, 0.00, 2.37]\n",
            "Epoch 11, batch: 4000/5760 (33.62 s), loss: 121.87, kl: [2.02, 2.50, 0.00, 0.00, 0.00, 0.91, 0.00, 3.16, 0.00, 2.36]\n",
            "Epoch 11, batch: 4100/5760 (34.09 s), loss: 121.90, kl: [2.02, 2.50, 0.00, 0.00, 0.00, 0.91, 0.00, 3.16, 0.00, 2.36]\n",
            "Epoch 11, batch: 4200/5760 (33.84 s), loss: 121.53, kl: [2.00, 2.50, 0.00, 0.00, 0.00, 0.92, 0.00, 3.17, 0.00, 2.36]\n",
            "Epoch 11, batch: 4300/5760 (33.65 s), loss: 121.65, kl: [1.97, 2.50, 0.00, 0.00, 0.00, 0.92, 0.00, 3.17, 0.00, 2.37]\n",
            "Epoch 11, batch: 4400/5760 (33.89 s), loss: 121.87, kl: [1.99, 2.51, 0.00, 0.00, 0.00, 0.92, 0.00, 3.16, 0.00, 2.37]\n",
            "Epoch 11, batch: 4500/5760 (34.93 s), loss: 121.80, kl: [1.98, 2.51, 0.00, 0.00, 0.00, 0.92, 0.00, 3.17, 0.00, 2.38]\n",
            "Epoch 11, batch: 4600/5760 (34.25 s), loss: 121.96, kl: [1.99, 2.51, 0.00, 0.00, 0.00, 0.91, 0.00, 3.17, 0.00, 2.38]\n",
            "Epoch 11, batch: 4700/5760 (33.83 s), loss: 122.33, kl: [2.00, 2.50, 0.00, 0.00, 0.00, 0.91, 0.00, 3.18, 0.00, 2.37]\n",
            "Epoch 11, batch: 4800/5760 (33.67 s), loss: 121.97, kl: [2.01, 2.51, 0.00, 0.00, 0.00, 0.93, 0.00, 3.17, 0.00, 2.37]\n",
            "Epoch 11, batch: 4900/5760 (33.69 s), loss: 121.81, kl: [2.00, 2.50, 0.00, 0.00, 0.00, 0.92, 0.00, 3.16, 0.00, 2.39]\n",
            "Epoch 11, batch: 5000/5760 (33.74 s), loss: 121.65, kl: [2.01, 2.50, 0.00, 0.00, 0.00, 0.91, 0.00, 3.17, 0.00, 2.37]\n",
            "Epoch 11, batch: 5100/5760 (33.99 s), loss: 121.63, kl: [2.01, 2.50, 0.00, 0.00, 0.00, 0.92, 0.00, 3.16, 0.00, 2.36]\n",
            "Epoch 11, batch: 5200/5760 (33.83 s), loss: 121.60, kl: [2.00, 2.51, 0.00, 0.00, 0.00, 0.92, 0.00, 3.16, 0.00, 2.36]\n",
            "Epoch 11, batch: 5300/5760 (33.85 s), loss: 121.61, kl: [2.00, 2.50, 0.00, 0.00, 0.00, 0.92, 0.00, 3.17, 0.00, 2.37]\n",
            "Epoch 11, batch: 5400/5760 (34.45 s), loss: 121.67, kl: [2.00, 2.51, 0.00, 0.00, 0.00, 0.92, 0.00, 3.16, 0.00, 2.37]\n",
            "Epoch 11, batch: 5500/5760 (34.47 s), loss: 121.76, kl: [1.99, 2.51, 0.00, 0.00, 0.00, 0.92, 0.00, 3.16, 0.00, 2.36]\n",
            "Epoch 11, batch: 5600/5760 (33.77 s), loss: 121.72, kl: [1.99, 2.51, 0.00, 0.00, 0.00, 0.92, 0.00, 3.16, 0.00, 2.35]\n",
            "Epoch 11, batch: 5700/5760 (33.82 s), loss: 121.69, kl: [1.98, 2.51, 0.00, 0.00, 0.00, 0.92, 0.00, 3.16, 0.00, 2.37]\n",
            "Epoch 12, batch: 0/5760 (20.27 s), loss: 121.59, kl: [1.99, 2.51, 0.00, 0.00, 0.00, 0.92, 0.00, 3.17, 0.00, 2.37]\n",
            "Epoch 12, batch: 100/5760 (34.11 s), loss: 122.00, kl: [1.99, 2.51, 0.00, 0.00, 0.00, 0.92, 0.00, 3.17, 0.00, 2.37]\n",
            "Epoch 12, batch: 200/5760 (34.12 s), loss: 121.96, kl: [2.00, 2.51, 0.00, 0.00, 0.00, 0.92, 0.00, 3.16, 0.00, 2.36]\n",
            "Epoch 12, batch: 300/5760 (33.82 s), loss: 121.76, kl: [2.00, 2.50, 0.00, 0.00, 0.00, 0.92, 0.00, 3.16, 0.00, 2.37]\n",
            "Epoch 12, batch: 400/5760 (33.98 s), loss: 121.67, kl: [2.01, 2.50, 0.00, 0.00, 0.00, 0.92, 0.00, 3.17, 0.00, 2.37]\n",
            "Epoch 12, batch: 500/5760 (34.16 s), loss: 121.84, kl: [1.99, 2.50, 0.00, 0.00, 0.00, 0.92, 0.00, 3.17, 0.00, 2.38]\n",
            "Epoch 12, batch: 600/5760 (34.35 s), loss: 121.55, kl: [1.99, 2.50, 0.00, 0.00, 0.00, 0.91, 0.00, 3.16, 0.00, 2.37]\n",
            "Epoch 12, batch: 700/5760 (33.86 s), loss: 121.53, kl: [2.01, 2.50, 0.00, 0.00, 0.00, 0.92, 0.00, 3.16, 0.00, 2.37]\n",
            "Epoch 12, batch: 800/5760 (33.84 s), loss: 121.30, kl: [2.01, 2.50, 0.00, 0.00, 0.00, 0.92, 0.00, 3.16, 0.00, 2.37]\n",
            "Epoch 12, batch: 900/5760 (33.86 s), loss: 121.57, kl: [2.01, 2.50, 0.00, 0.00, 0.00, 0.93, 0.00, 3.16, 0.00, 2.37]\n",
            "Epoch 12, batch: 1000/5760 (33.82 s), loss: 121.43, kl: [2.01, 2.50, 0.00, 0.00, 0.00, 0.92, 0.00, 3.18, 0.00, 2.37]\n",
            "Epoch 12, batch: 1100/5760 (34.08 s), loss: 121.50, kl: [2.00, 2.50, 0.00, 0.00, 0.00, 0.91, 0.00, 3.17, 0.00, 2.37]\n",
            "Epoch 12, batch: 1200/5760 (33.95 s), loss: 121.77, kl: [2.01, 2.49, 0.00, 0.00, 0.00, 0.92, 0.00, 3.17, 0.00, 2.37]\n",
            "Epoch 12, batch: 1300/5760 (33.99 s), loss: 121.68, kl: [2.02, 2.50, 0.00, 0.00, 0.00, 0.92, 0.00, 3.18, 0.00, 2.36]\n",
            "Epoch 12, batch: 1400/5760 (34.19 s), loss: 121.57, kl: [2.02, 2.50, 0.00, 0.00, 0.00, 0.91, 0.00, 3.17, 0.00, 2.36]\n",
            "Epoch 12, batch: 1500/5760 (34.35 s), loss: 121.68, kl: [2.01, 2.50, 0.00, 0.00, 0.00, 0.91, 0.00, 3.17, 0.00, 2.37]\n",
            "Epoch 12, batch: 1600/5760 (34.20 s), loss: 121.47, kl: [2.01, 2.50, 0.00, 0.00, 0.00, 0.91, 0.00, 3.17, 0.00, 2.37]\n",
            "Epoch 12, batch: 1700/5760 (34.05 s), loss: 121.30, kl: [1.99, 2.50, 0.00, 0.00, 0.00, 0.92, 0.00, 3.17, 0.00, 2.38]\n",
            "Epoch 12, batch: 1800/5760 (33.80 s), loss: 121.31, kl: [1.99, 2.50, 0.00, 0.00, 0.00, 0.92, 0.00, 3.17, 0.00, 2.37]\n",
            "Epoch 12, batch: 1900/5760 (34.44 s), loss: 121.41, kl: [2.00, 2.51, 0.00, 0.00, 0.00, 0.92, 0.00, 3.17, 0.00, 2.37]\n",
            "Epoch 12, batch: 2000/5760 (34.42 s), loss: 121.31, kl: [2.00, 2.50, 0.00, 0.00, 0.00, 0.92, 0.00, 3.17, 0.00, 2.38]\n",
            "Epoch 12, batch: 2100/5760 (34.47 s), loss: 121.52, kl: [2.00, 2.50, 0.00, 0.00, 0.00, 0.92, 0.00, 3.17, 0.00, 2.38]\n",
            "Epoch 12, batch: 2200/5760 (34.18 s), loss: 121.39, kl: [2.01, 2.50, 0.00, 0.00, 0.00, 0.92, 0.00, 3.16, 0.00, 2.37]\n",
            "Epoch 12, batch: 2300/5760 (34.72 s), loss: 121.15, kl: [2.01, 2.49, 0.00, 0.00, 0.00, 0.93, 0.00, 3.16, 0.00, 2.37]\n",
            "Epoch 12, batch: 2400/5760 (34.51 s), loss: 121.20, kl: [2.00, 2.49, 0.00, 0.00, 0.00, 0.92, 0.00, 3.16, 0.00, 2.38]\n",
            "Epoch 12, batch: 2500/5760 (34.16 s), loss: 121.10, kl: [2.00, 2.50, 0.00, 0.00, 0.00, 0.93, 0.00, 3.16, 0.00, 2.36]\n",
            "Epoch 12, batch: 2600/5760 (33.66 s), loss: 121.29, kl: [2.00, 2.50, 0.00, 0.00, 0.00, 0.92, 0.00, 3.16, 0.00, 2.35]\n",
            "Epoch 12, batch: 2700/5760 (34.19 s), loss: 121.38, kl: [1.99, 2.50, 0.00, 0.00, 0.00, 0.92, 0.00, 3.17, 0.00, 2.38]\n",
            "Epoch 12, batch: 2800/5760 (33.95 s), loss: 121.27, kl: [1.99, 2.51, 0.00, 0.00, 0.00, 0.92, 0.00, 3.17, 0.00, 2.38]\n",
            "Epoch 12, batch: 2900/5760 (33.96 s), loss: 121.47, kl: [1.98, 2.51, 0.00, 0.00, 0.00, 0.92, 0.00, 3.17, 0.00, 2.38]\n",
            "Epoch 12, batch: 3000/5760 (33.96 s), loss: 121.23, kl: [1.99, 2.51, 0.00, 0.00, 0.00, 0.93, 0.00, 3.16, 0.00, 2.38]\n",
            "Epoch 12, batch: 3100/5760 (34.47 s), loss: 121.32, kl: [1.99, 2.51, 0.00, 0.00, 0.00, 0.94, 0.00, 3.17, 0.00, 2.39]\n",
            "Epoch 12, batch: 3200/5760 (34.37 s), loss: 121.40, kl: [1.99, 2.50, 0.00, 0.00, 0.00, 0.92, 0.00, 3.18, 0.00, 2.37]\n",
            "Epoch 12, batch: 3300/5760 (34.88 s), loss: 121.44, kl: [1.99, 2.50, 0.00, 0.00, 0.00, 0.92, 0.00, 3.18, 0.00, 2.38]\n",
            "Epoch 12, batch: 3400/5760 (34.62 s), loss: 121.20, kl: [1.97, 2.50, 0.00, 0.00, 0.00, 0.93, 0.00, 3.17, 0.00, 2.39]\n",
            "Epoch 12, batch: 3500/5760 (34.22 s), loss: 121.37, kl: [2.00, 2.50, 0.00, 0.00, 0.00, 0.92, 0.00, 3.17, 0.00, 2.38]\n",
            "Epoch 12, batch: 3600/5760 (34.17 s), loss: 121.29, kl: [1.98, 2.50, 0.00, 0.00, 0.00, 0.91, 0.00, 3.17, 0.00, 2.38]\n",
            "Epoch 12, batch: 3700/5760 (34.19 s), loss: 121.50, kl: [1.99, 2.50, 0.00, 0.00, 0.00, 0.92, 0.00, 3.18, 0.00, 2.38]\n",
            "Epoch 12, batch: 3800/5760 (34.27 s), loss: 121.61, kl: [2.01, 2.50, 0.00, 0.00, 0.00, 0.92, 0.00, 3.18, 0.00, 2.38]\n",
            "Epoch 12, batch: 3900/5760 (34.08 s), loss: 121.01, kl: [2.00, 2.49, 0.00, 0.00, 0.00, 0.92, 0.00, 3.17, 0.00, 2.39]\n",
            "Epoch 12, batch: 4000/5760 (34.05 s), loss: 121.42, kl: [2.00, 2.50, 0.00, 0.00, 0.00, 0.92, 0.00, 3.18, 0.00, 2.38]\n",
            "Epoch 12, batch: 4100/5760 (34.76 s), loss: 121.43, kl: [1.99, 2.50, 0.00, 0.00, 0.00, 0.92, 0.00, 3.17, 0.00, 2.38]\n",
            "Epoch 12, batch: 4200/5760 (34.22 s), loss: 121.59, kl: [2.00, 2.50, 0.00, 0.00, 0.00, 0.92, 0.00, 3.17, 0.00, 2.37]\n",
            "Epoch 12, batch: 4300/5760 (34.42 s), loss: 121.50, kl: [2.00, 2.50, 0.00, 0.00, 0.00, 0.94, 0.00, 3.17, 0.00, 2.38]\n",
            "Epoch 12, batch: 4400/5760 (34.11 s), loss: 121.32, kl: [2.02, 2.50, 0.00, 0.00, 0.00, 0.94, 0.00, 3.17, 0.00, 2.38]\n",
            "Epoch 12, batch: 4500/5760 (34.05 s), loss: 121.13, kl: [2.02, 2.50, 0.00, 0.00, 0.00, 0.94, 0.00, 3.17, 0.00, 2.39]\n",
            "Epoch 12, batch: 4600/5760 (33.94 s), loss: 121.79, kl: [2.02, 2.51, 0.00, 0.00, 0.00, 0.93, 0.00, 3.17, 0.00, 2.40]\n",
            "Epoch 12, batch: 4700/5760 (33.78 s), loss: 121.23, kl: [2.01, 2.51, 0.00, 0.00, 0.00, 0.93, 0.00, 3.16, 0.00, 2.38]\n",
            "Epoch 12, batch: 4800/5760 (33.98 s), loss: 121.04, kl: [2.01, 2.50, 0.00, 0.00, 0.00, 0.93, 0.00, 3.16, 0.00, 2.39]\n",
            "Epoch 12, batch: 4900/5760 (34.13 s), loss: 121.13, kl: [2.01, 2.50, 0.00, 0.00, 0.00, 0.93, 0.00, 3.17, 0.00, 2.38]\n",
            "Epoch 12, batch: 5000/5760 (34.05 s), loss: 121.26, kl: [2.02, 2.49, 0.00, 0.00, 0.00, 0.92, 0.00, 3.17, 0.00, 2.38]\n",
            "Epoch 12, batch: 5100/5760 (34.14 s), loss: 121.14, kl: [2.01, 2.49, 0.00, 0.00, 0.00, 0.91, 0.00, 3.17, 0.00, 2.38]\n",
            "Epoch 12, batch: 5200/5760 (34.35 s), loss: 120.84, kl: [2.01, 2.50, 0.00, 0.00, 0.00, 0.93, 0.00, 3.17, 0.00, 2.37]\n",
            "Epoch 12, batch: 5300/5760 (33.71 s), loss: 120.79, kl: [2.00, 2.51, 0.00, 0.00, 0.00, 0.94, 0.00, 3.17, 0.00, 2.38]\n",
            "Epoch 12, batch: 5400/5760 (33.84 s), loss: 120.76, kl: [2.00, 2.51, 0.00, 0.00, 0.00, 0.94, 0.00, 3.17, 0.00, 2.38]\n",
            "Epoch 12, batch: 5500/5760 (33.93 s), loss: 120.65, kl: [2.00, 2.51, 0.00, 0.00, 0.00, 0.94, 0.00, 3.16, 0.00, 2.37]\n",
            "Epoch 12, batch: 5600/5760 (33.82 s), loss: 120.75, kl: [2.00, 2.51, 0.00, 0.00, 0.00, 0.95, 0.00, 3.17, 0.00, 2.37]\n",
            "Epoch 12, batch: 5700/5760 (33.96 s), loss: 120.83, kl: [2.00, 2.50, 0.00, 0.00, 0.00, 0.94, 0.00, 3.17, 0.00, 2.39]\n",
            "Epoch 13, batch: 0/5760 (20.41 s), loss: 120.80, kl: [2.01, 2.50, 0.00, 0.00, 0.00, 0.93, 0.00, 3.17, 0.00, 2.38]\n",
            "Epoch 13, batch: 100/5760 (34.11 s), loss: 121.04, kl: [2.00, 2.50, 0.00, 0.00, 0.00, 0.93, 0.00, 3.17, 0.00, 2.38]\n",
            "Epoch 13, batch: 200/5760 (34.12 s), loss: 120.74, kl: [2.00, 2.50, 0.00, 0.00, 0.00, 0.92, 0.00, 3.17, 0.00, 2.38]\n",
            "Epoch 13, batch: 300/5760 (34.16 s), loss: 121.24, kl: [2.00, 2.50, 0.00, 0.00, 0.00, 0.92, 0.00, 3.17, 0.00, 2.39]\n",
            "Epoch 13, batch: 400/5760 (33.91 s), loss: 121.19, kl: [1.99, 2.50, 0.00, 0.00, 0.00, 0.92, 0.00, 3.17, 0.00, 2.39]\n",
            "Epoch 13, batch: 500/5760 (34.06 s), loss: 121.30, kl: [2.00, 2.50, 0.00, 0.00, 0.00, 0.93, 0.00, 3.17, 0.00, 2.38]\n",
            "Epoch 13, batch: 600/5760 (33.76 s), loss: 120.82, kl: [2.01, 2.50, 0.00, 0.00, 0.00, 0.92, 0.00, 3.16, 0.00, 2.38]\n",
            "Epoch 13, batch: 700/5760 (34.00 s), loss: 120.72, kl: [2.02, 2.50, 0.00, 0.00, 0.00, 0.92, 0.00, 3.17, 0.00, 2.38]\n",
            "Epoch 13, batch: 800/5760 (34.21 s), loss: 121.20, kl: [2.01, 2.51, 0.00, 0.00, 0.00, 0.92, 0.00, 3.18, 0.00, 2.39]\n",
            "Epoch 13, batch: 900/5760 (34.18 s), loss: 120.81, kl: [2.00, 2.51, 0.00, 0.00, 0.00, 0.92, 0.00, 3.17, 0.00, 2.38]\n",
            "Epoch 13, batch: 1000/5760 (34.22 s), loss: 120.80, kl: [2.02, 2.50, 0.00, 0.00, 0.00, 0.92, 0.00, 3.18, 0.00, 2.38]\n",
            "Epoch 13, batch: 1100/5760 (34.90 s), loss: 121.14, kl: [2.01, 2.51, 0.00, 0.00, 0.00, 0.92, 0.00, 3.17, 0.00, 2.38]\n",
            "Epoch 13, batch: 1200/5760 (34.40 s), loss: 121.05, kl: [2.02, 2.51, 0.00, 0.00, 0.00, 0.93, 0.00, 3.17, 0.00, 2.38]\n",
            "Epoch 13, batch: 1300/5760 (34.46 s), loss: 121.03, kl: [2.03, 2.51, 0.00, 0.00, 0.00, 0.93, 0.00, 3.16, 0.00, 2.38]\n",
            "Epoch 13, batch: 1400/5760 (34.17 s), loss: 121.02, kl: [2.01, 2.51, 0.00, 0.00, 0.00, 0.94, 0.00, 3.17, 0.00, 2.38]\n",
            "Epoch 13, batch: 1500/5760 (33.87 s), loss: 121.24, kl: [2.01, 2.49, 0.00, 0.00, 0.00, 0.94, 0.00, 3.17, 0.00, 2.38]\n",
            "Epoch 13, batch: 1600/5760 (33.87 s), loss: 120.95, kl: [2.00, 2.49, 0.00, 0.00, 0.00, 0.94, 0.00, 3.17, 0.00, 2.38]\n",
            "Epoch 13, batch: 1700/5760 (33.87 s), loss: 120.56, kl: [1.99, 2.50, 0.00, 0.00, 0.00, 0.94, 0.00, 3.18, 0.00, 2.38]\n",
            "Epoch 13, batch: 1800/5760 (34.00 s), loss: 120.73, kl: [2.00, 2.50, 0.00, 0.00, 0.00, 0.93, 0.00, 3.17, 0.00, 2.36]\n",
            "Epoch 13, batch: 1900/5760 (33.84 s), loss: 120.98, kl: [2.00, 2.51, 0.00, 0.00, 0.00, 0.93, 0.00, 3.18, 0.00, 2.38]\n",
            "Epoch 13, batch: 2000/5760 (34.28 s), loss: 120.74, kl: [2.00, 2.50, 0.00, 0.00, 0.00, 0.93, 0.00, 3.17, 0.00, 2.39]\n",
            "Epoch 13, batch: 2100/5760 (34.23 s), loss: 120.74, kl: [2.00, 2.50, 0.00, 0.00, 0.00, 0.92, 0.00, 3.17, 0.00, 2.38]\n",
            "Epoch 13, batch: 2200/5760 (34.42 s), loss: 121.05, kl: [2.00, 2.51, 0.00, 0.00, 0.00, 0.92, 0.00, 3.17, 0.00, 2.40]\n",
            "Epoch 13, batch: 2300/5760 (33.89 s), loss: 120.94, kl: [2.01, 2.50, 0.00, 0.00, 0.00, 0.93, 0.00, 3.17, 0.00, 2.40]\n",
            "Epoch 13, batch: 2400/5760 (33.92 s), loss: 121.19, kl: [2.00, 2.50, 0.00, 0.00, 0.00, 0.93, 0.00, 3.17, 0.00, 2.40]\n",
            "Epoch 13, batch: 2500/5760 (34.07 s), loss: 120.61, kl: [2.00, 2.50, 0.00, 0.00, 0.00, 0.93, 0.00, 3.17, 0.00, 2.40]\n",
            "Epoch 13, batch: 2600/5760 (34.10 s), loss: 120.83, kl: [2.00, 2.50, 0.00, 0.00, 0.00, 0.92, 0.00, 3.17, 0.00, 2.39]\n",
            "Epoch 13, batch: 2700/5760 (33.84 s), loss: 120.89, kl: [2.00, 2.50, 0.00, 0.00, 0.00, 0.92, 0.00, 3.17, 0.00, 2.38]\n",
            "Epoch 13, batch: 2800/5760 (33.90 s), loss: 120.94, kl: [2.00, 2.50, 0.00, 0.00, 0.00, 0.93, 0.00, 3.17, 0.00, 2.38]\n",
            "Epoch 13, batch: 2900/5760 (34.33 s), loss: 121.01, kl: [2.01, 2.51, 0.00, 0.00, 0.00, 0.93, 0.00, 3.18, 0.00, 2.40]\n",
            "Epoch 13, batch: 3000/5760 (33.96 s), loss: 121.14, kl: [2.01, 2.51, 0.00, 0.00, 0.00, 0.94, 0.00, 3.17, 0.00, 2.39]\n",
            "Epoch 13, batch: 3100/5760 (34.78 s), loss: 121.45, kl: [2.02, 2.51, 0.00, 0.00, 0.00, 0.94, 0.00, 3.17, 0.00, 2.39]\n",
            "Epoch 13, batch: 3200/5760 (33.91 s), loss: 120.84, kl: [2.01, 2.50, 0.00, 0.00, 0.00, 0.93, 0.00, 3.18, 0.00, 2.39]\n",
            "Epoch 13, batch: 3300/5760 (33.85 s), loss: 120.88, kl: [2.00, 2.50, 0.00, 0.00, 0.00, 0.93, 0.00, 3.18, 0.00, 2.39]\n",
            "Epoch 13, batch: 3400/5760 (34.14 s), loss: 120.56, kl: [2.00, 2.50, 0.00, 0.00, 0.00, 0.94, 0.00, 3.18, 0.00, 2.39]\n",
            "Epoch 13, batch: 3500/5760 (34.20 s), loss: 120.55, kl: [2.01, 2.50, 0.00, 0.00, 0.00, 0.93, 0.00, 3.18, 0.00, 2.38]\n",
            "Epoch 13, batch: 3600/5760 (34.35 s), loss: 120.88, kl: [1.99, 2.50, 0.00, 0.00, 0.00, 0.93, 0.00, 3.18, 0.00, 2.39]\n",
            "Epoch 13, batch: 3700/5760 (34.26 s), loss: 120.80, kl: [2.00, 2.50, 0.00, 0.00, 0.00, 0.94, 0.00, 3.18, 0.00, 2.38]\n",
            "Epoch 13, batch: 3800/5760 (34.64 s), loss: 120.52, kl: [1.99, 2.50, 0.00, 0.00, 0.00, 0.94, 0.00, 3.17, 0.00, 2.39]\n",
            "Epoch 13, batch: 3900/5760 (34.26 s), loss: 120.47, kl: [2.00, 2.51, 0.00, 0.00, 0.00, 0.93, 0.00, 3.18, 0.00, 2.41]\n",
            "Epoch 13, batch: 4000/5760 (34.96 s), loss: 120.72, kl: [2.01, 2.51, 0.00, 0.00, 0.00, 0.93, 0.00, 3.19, 0.00, 2.41]\n",
            "Epoch 13, batch: 4100/5760 (34.72 s), loss: 120.34, kl: [2.02, 2.51, 0.00, 0.00, 0.00, 0.95, 0.00, 3.18, 0.00, 2.40]\n",
            "Epoch 13, batch: 4200/5760 (34.48 s), loss: 120.51, kl: [2.01, 2.51, 0.00, 0.00, 0.00, 0.94, 0.00, 3.18, 0.00, 2.39]\n",
            "Epoch 13, batch: 4300/5760 (34.42 s), loss: 120.86, kl: [1.99, 2.51, 0.00, 0.00, 0.00, 0.94, 0.00, 3.18, 0.00, 2.40]\n",
            "Epoch 13, batch: 4400/5760 (34.76 s), loss: 120.36, kl: [2.01, 2.50, 0.00, 0.00, 0.00, 0.93, 0.00, 3.18, 0.00, 2.39]\n",
            "Epoch 13, batch: 4500/5760 (34.33 s), loss: 120.81, kl: [2.01, 2.51, 0.00, 0.00, 0.00, 0.93, 0.00, 3.17, 0.00, 2.39]\n",
            "Epoch 13, batch: 4600/5760 (34.39 s), loss: 120.56, kl: [2.01, 2.51, 0.00, 0.00, 0.00, 0.93, 0.00, 3.17, 0.00, 2.39]\n",
            "Epoch 13, batch: 4700/5760 (34.81 s), loss: 120.49, kl: [2.01, 2.50, 0.00, 0.00, 0.00, 0.92, 0.00, 3.17, 0.00, 2.39]\n",
            "Epoch 13, batch: 4800/5760 (34.30 s), loss: 120.60, kl: [2.00, 2.50, 0.00, 0.00, 0.00, 0.94, 0.00, 3.18, 0.00, 2.38]\n",
            "Epoch 13, batch: 4900/5760 (34.75 s), loss: 120.59, kl: [2.00, 2.50, 0.00, 0.00, 0.00, 0.94, 0.00, 3.18, 0.00, 2.40]\n",
            "Epoch 13, batch: 5000/5760 (34.18 s), loss: 120.34, kl: [2.00, 2.49, 0.00, 0.00, 0.00, 0.94, 0.00, 3.17, 0.00, 2.40]\n",
            "Epoch 13, batch: 5100/5760 (34.30 s), loss: 120.66, kl: [2.02, 2.50, 0.00, 0.00, 0.00, 0.94, 0.00, 3.18, 0.00, 2.39]\n",
            "Epoch 13, batch: 5200/5760 (33.91 s), loss: 120.55, kl: [2.01, 2.50, 0.00, 0.00, 0.00, 0.94, 0.00, 3.18, 0.00, 2.38]\n",
            "Epoch 13, batch: 5300/5760 (34.24 s), loss: 120.49, kl: [2.00, 2.50, 0.00, 0.00, 0.00, 0.94, 0.00, 3.18, 0.00, 2.38]\n",
            "Epoch 13, batch: 5400/5760 (34.30 s), loss: 120.49, kl: [2.01, 2.51, 0.00, 0.00, 0.00, 0.93, 0.00, 3.18, 0.00, 2.38]\n",
            "Epoch 13, batch: 5500/5760 (34.06 s), loss: 120.37, kl: [2.02, 2.51, 0.00, 0.00, 0.00, 0.95, 0.00, 3.17, 0.00, 2.38]\n",
            "Epoch 13, batch: 5600/5760 (34.76 s), loss: 120.61, kl: [2.00, 2.51, 0.00, 0.00, 0.00, 0.94, 0.00, 3.17, 0.00, 2.39]\n",
            "Epoch 13, batch: 5700/5760 (34.08 s), loss: 120.50, kl: [2.00, 2.51, 0.00, 0.00, 0.00, 0.93, 0.00, 3.17, 0.00, 2.39]\n",
            "Epoch 14, batch: 0/5760 (21.13 s), loss: 120.59, kl: [2.01, 2.50, 0.00, 0.00, 0.00, 0.93, 0.00, 3.18, 0.00, 2.39]\n",
            "Epoch 14, batch: 100/5760 (34.91 s), loss: 120.57, kl: [1.99, 2.50, 0.00, 0.00, 0.00, 0.94, 0.00, 3.18, 0.00, 2.39]\n",
            "Epoch 14, batch: 200/5760 (34.05 s), loss: 120.34, kl: [2.00, 2.51, 0.00, 0.00, 0.00, 0.93, 0.00, 3.18, 0.00, 2.38]\n",
            "Epoch 14, batch: 300/5760 (33.97 s), loss: 120.49, kl: [2.01, 2.51, 0.00, 0.00, 0.00, 0.94, 0.00, 3.19, 0.00, 2.39]\n",
            "Epoch 14, batch: 400/5760 (34.14 s), loss: 120.32, kl: [2.02, 2.51, 0.00, 0.00, 0.00, 0.95, 0.00, 3.18, 0.00, 2.38]\n",
            "Epoch 14, batch: 500/5760 (34.30 s), loss: 120.47, kl: [2.02, 2.50, 0.00, 0.00, 0.00, 0.94, 0.00, 3.17, 0.00, 2.38]\n",
            "Epoch 14, batch: 600/5760 (34.44 s), loss: 120.24, kl: [2.01, 2.51, 0.00, 0.00, 0.00, 0.93, 0.00, 3.18, 0.00, 2.38]\n",
            "Epoch 14, batch: 700/5760 (34.50 s), loss: 120.46, kl: [2.01, 2.51, 0.00, 0.00, 0.00, 0.93, 0.00, 3.18, 0.00, 2.38]\n",
            "Epoch 14, batch: 800/5760 (34.31 s), loss: 120.31, kl: [2.01, 2.51, 0.00, 0.00, 0.00, 0.93, 0.00, 3.17, 0.00, 2.39]\n",
            "Epoch 14, batch: 900/5760 (34.60 s), loss: 120.72, kl: [2.01, 2.51, 0.00, 0.00, 0.00, 0.94, 0.00, 3.16, 0.00, 2.39]\n",
            "Epoch 14, batch: 1000/5760 (34.66 s), loss: 120.34, kl: [2.01, 2.50, 0.00, 0.00, 0.00, 0.94, 0.00, 3.17, 0.00, 2.39]\n",
            "Epoch 14, batch: 1100/5760 (34.73 s), loss: 120.21, kl: [2.01, 2.51, 0.00, 0.00, 0.00, 0.94, 0.00, 3.17, 0.00, 2.39]\n",
            "Epoch 14, batch: 1200/5760 (34.42 s), loss: 120.39, kl: [2.02, 2.51, 0.00, 0.00, 0.00, 0.93, 0.00, 3.17, 0.00, 2.40]\n",
            "Epoch 14, batch: 1300/5760 (34.56 s), loss: 120.30, kl: [2.02, 2.50, 0.00, 0.00, 0.00, 0.94, 0.00, 3.18, 0.00, 2.39]\n",
            "Epoch 14, batch: 1400/5760 (34.68 s), loss: 120.58, kl: [2.01, 2.50, 0.00, 0.00, 0.00, 0.94, 0.00, 3.18, 0.00, 2.40]\n",
            "Epoch 14, batch: 1500/5760 (34.34 s), loss: 120.54, kl: [2.00, 2.50, 0.00, 0.00, 0.00, 0.94, 0.00, 3.18, 0.00, 2.39]\n",
            "Epoch 14, batch: 1600/5760 (34.58 s), loss: 120.68, kl: [2.00, 2.51, 0.00, 0.00, 0.00, 0.94, 0.00, 3.18, 0.00, 2.40]\n",
            "Epoch 14, batch: 1700/5760 (34.06 s), loss: 120.40, kl: [2.00, 2.51, 0.00, 0.00, 0.00, 0.93, 0.00, 3.18, 0.00, 2.39]\n",
            "Epoch 14, batch: 1800/5760 (34.49 s), loss: 120.50, kl: [2.01, 2.52, 0.00, 0.00, 0.00, 0.94, 0.00, 3.18, 0.00, 2.39]\n",
            "Epoch 14, batch: 1900/5760 (34.67 s), loss: 120.69, kl: [2.01, 2.51, 0.00, 0.00, 0.00, 0.94, 0.00, 3.19, 0.00, 2.40]\n",
            "Epoch 14, batch: 2000/5760 (34.47 s), loss: 120.54, kl: [2.02, 2.51, 0.00, 0.00, 0.00, 0.94, 0.00, 3.18, 0.00, 2.39]\n",
            "Epoch 14, batch: 2100/5760 (34.83 s), loss: 120.38, kl: [2.03, 2.52, 0.00, 0.00, 0.00, 0.95, 0.00, 3.18, 0.00, 2.38]\n",
            "Epoch 14, batch: 2200/5760 (34.38 s), loss: 120.86, kl: [2.02, 2.51, 0.00, 0.00, 0.00, 0.94, 0.00, 3.17, 0.00, 2.38]\n",
            "Epoch 14, batch: 2300/5760 (34.32 s), loss: 120.19, kl: [2.02, 2.50, 0.00, 0.00, 0.00, 0.93, 0.00, 3.17, 0.00, 2.39]\n",
            "Epoch 14, batch: 2400/5760 (34.47 s), loss: 120.08, kl: [2.00, 2.50, 0.00, 0.00, 0.00, 0.94, 0.00, 3.18, 0.00, 2.40]\n",
            "Epoch 14, batch: 2500/5760 (34.57 s), loss: 120.20, kl: [2.02, 2.51, 0.00, 0.00, 0.00, 0.95, 0.00, 3.18, 0.00, 2.39]\n",
            "Epoch 14, batch: 2600/5760 (33.94 s), loss: 120.20, kl: [2.01, 2.50, 0.00, 0.00, 0.00, 0.95, 0.00, 3.18, 0.00, 2.40]\n",
            "Epoch 14, batch: 2700/5760 (33.74 s), loss: 120.26, kl: [2.02, 2.50, 0.00, 0.00, 0.00, 0.94, 0.00, 3.18, 0.00, 2.39]\n",
            "Epoch 14, batch: 2800/5760 (34.35 s), loss: 120.13, kl: [2.03, 2.50, 0.00, 0.00, 0.00, 0.95, 0.00, 3.18, 0.00, 2.38]\n",
            "Epoch 14, batch: 2900/5760 (33.92 s), loss: 120.18, kl: [2.02, 2.51, 0.00, 0.00, 0.00, 0.94, 0.00, 3.17, 0.00, 2.38]\n",
            "Epoch 14, batch: 3000/5760 (34.25 s), loss: 120.62, kl: [2.03, 2.50, 0.00, 0.00, 0.00, 0.95, 0.00, 3.18, 0.00, 2.39]\n",
            "Epoch 14, batch: 3100/5760 (34.61 s), loss: 120.51, kl: [2.03, 2.50, 0.00, 0.00, 0.00, 0.94, 0.00, 3.18, 0.00, 2.40]\n",
            "Epoch 14, batch: 3200/5760 (33.99 s), loss: 120.34, kl: [2.01, 2.50, 0.00, 0.00, 0.00, 0.95, 0.00, 3.18, 0.00, 2.40]\n",
            "Epoch 14, batch: 3300/5760 (33.94 s), loss: 120.26, kl: [2.02, 2.50, 0.00, 0.00, 0.00, 0.95, 0.00, 3.17, 0.00, 2.40]\n",
            "Epoch 14, batch: 3400/5760 (34.29 s), loss: 120.02, kl: [2.02, 2.50, 0.00, 0.00, 0.00, 0.95, 0.00, 3.17, 0.00, 2.40]\n",
            "Epoch 14, batch: 3500/5760 (34.06 s), loss: 120.18, kl: [2.02, 2.50, 0.00, 0.00, 0.00, 0.96, 0.00, 3.18, 0.00, 2.41]\n",
            "Epoch 14, batch: 3600/5760 (34.06 s), loss: 120.27, kl: [2.01, 2.49, 0.00, 0.00, 0.00, 0.96, 0.00, 3.18, 0.00, 2.40]\n",
            "Epoch 14, batch: 3700/5760 (34.55 s), loss: 120.46, kl: [2.02, 2.50, 0.00, 0.00, 0.00, 0.96, 0.00, 3.18, 0.00, 2.41]\n",
            "Epoch 14, batch: 3800/5760 (34.14 s), loss: 120.32, kl: [2.01, 2.51, 0.00, 0.00, 0.00, 0.95, 0.00, 3.17, 0.00, 2.40]\n",
            "Epoch 14, batch: 3900/5760 (33.96 s), loss: 120.27, kl: [2.02, 2.51, 0.00, 0.00, 0.00, 0.94, 0.00, 3.19, 0.00, 2.41]\n",
            "Epoch 14, batch: 4000/5760 (34.00 s), loss: 120.28, kl: [2.02, 2.50, 0.00, 0.00, 0.00, 0.95, 0.00, 3.18, 0.00, 2.40]\n",
            "Epoch 14, batch: 4100/5760 (34.58 s), loss: 120.31, kl: [2.02, 2.51, 0.00, 0.00, 0.00, 0.95, 0.00, 3.18, 0.00, 2.38]\n",
            "Epoch 14, batch: 4200/5760 (34.05 s), loss: 120.07, kl: [2.01, 2.50, 0.00, 0.00, 0.00, 0.95, 0.00, 3.18, 0.00, 2.39]\n",
            "Epoch 14, batch: 4300/5760 (34.12 s), loss: 119.78, kl: [2.01, 2.51, 0.00, 0.00, 0.00, 0.95, 0.00, 3.17, 0.00, 2.39]\n",
            "Epoch 14, batch: 4400/5760 (33.87 s), loss: 119.89, kl: [1.99, 2.50, 0.00, 0.00, 0.00, 0.95, 0.00, 3.17, 0.00, 2.40]\n",
            "Epoch 14, batch: 4500/5760 (34.21 s), loss: 119.85, kl: [1.99, 2.50, 0.00, 0.00, 0.00, 0.95, 0.00, 3.18, 0.00, 2.41]\n",
            "Epoch 14, batch: 4600/5760 (34.28 s), loss: 120.06, kl: [1.99, 2.50, 0.00, 0.00, 0.00, 0.95, 0.00, 3.17, 0.00, 2.41]\n",
            "Epoch 14, batch: 4700/5760 (34.05 s), loss: 120.00, kl: [2.00, 2.50, 0.00, 0.00, 0.00, 0.94, 0.00, 3.19, 0.00, 2.40]\n",
            "Epoch 14, batch: 4800/5760 (34.11 s), loss: 119.91, kl: [1.99, 2.50, 0.00, 0.00, 0.00, 0.95, 0.00, 3.18, 0.00, 2.39]\n",
            "Epoch 14, batch: 4900/5760 (34.19 s), loss: 120.24, kl: [1.99, 2.51, 0.00, 0.00, 0.00, 0.95, 0.00, 3.18, 0.00, 2.40]\n",
            "Epoch 14, batch: 5000/5760 (33.97 s), loss: 119.80, kl: [2.01, 2.51, 0.00, 0.00, 0.00, 0.94, 0.00, 3.18, 0.00, 2.39]\n",
            "Epoch 14, batch: 5100/5760 (34.19 s), loss: 120.11, kl: [2.01, 2.51, 0.00, 0.00, 0.00, 0.93, 0.00, 3.17, 0.00, 2.39]\n",
            "Epoch 14, batch: 5200/5760 (34.08 s), loss: 119.88, kl: [2.00, 2.51, 0.00, 0.00, 0.00, 0.94, 0.00, 3.17, 0.00, 2.40]\n",
            "Epoch 14, batch: 5300/5760 (33.73 s), loss: 119.88, kl: [2.01, 2.51, 0.00, 0.00, 0.00, 0.94, 0.00, 3.17, 0.00, 2.40]\n",
            "Epoch 14, batch: 5400/5760 (33.65 s), loss: 119.57, kl: [2.02, 2.50, 0.00, 0.00, 0.00, 0.95, 0.00, 3.17, 0.00, 2.39]\n",
            "Epoch 14, batch: 5500/5760 (34.56 s), loss: 119.34, kl: [2.03, 2.50, 0.00, 0.00, 0.00, 0.95, 0.00, 3.17, 0.00, 2.40]\n",
            "Epoch 14, batch: 5600/5760 (34.10 s), loss: 119.97, kl: [2.02, 2.51, 0.00, 0.00, 0.00, 0.96, 0.00, 3.18, 0.00, 2.41]\n",
            "Epoch 14, batch: 5700/5760 (34.16 s), loss: 119.90, kl: [2.01, 2.51, 0.00, 0.00, 0.00, 0.95, 0.00, 3.19, 0.00, 2.41]\n",
            "Epoch 15, batch: 0/5760 (20.56 s), loss: 120.25, kl: [2.01, 2.51, 0.00, 0.00, 0.00, 0.95, 0.00, 3.18, 0.00, 2.41]\n",
            "Epoch 15, batch: 100/5760 (34.26 s), loss: 119.98, kl: [2.02, 2.51, 0.00, 0.00, 0.00, 0.95, 0.00, 3.17, 0.00, 2.41]\n",
            "Epoch 15, batch: 200/5760 (33.97 s), loss: 119.60, kl: [2.02, 2.51, 0.00, 0.00, 0.00, 0.96, 0.00, 3.17, 0.00, 2.40]\n",
            "Epoch 15, batch: 300/5760 (33.84 s), loss: 119.61, kl: [2.02, 2.50, 0.00, 0.00, 0.00, 0.96, 0.00, 3.17, 0.00, 2.39]\n",
            "Epoch 15, batch: 400/5760 (34.54 s), loss: 119.41, kl: [2.01, 2.50, 0.00, 0.00, 0.00, 0.96, 0.00, 3.18, 0.00, 2.40]\n",
            "Epoch 15, batch: 500/5760 (34.08 s), loss: 119.74, kl: [2.01, 2.50, 0.00, 0.00, 0.00, 0.95, 0.00, 3.18, 0.00, 2.41]\n",
            "Epoch 15, batch: 600/5760 (33.91 s), loss: 120.15, kl: [2.01, 2.51, 0.00, 0.00, 0.00, 0.95, 0.00, 3.19, 0.00, 2.41]\n",
            "Epoch 15, batch: 700/5760 (34.31 s), loss: 119.82, kl: [2.01, 2.51, 0.00, 0.00, 0.00, 0.95, 0.00, 3.18, 0.00, 2.41]\n",
            "Epoch 15, batch: 800/5760 (33.94 s), loss: 119.90, kl: [2.02, 2.51, 0.00, 0.00, 0.00, 0.94, 0.00, 3.19, 0.00, 2.40]\n",
            "Epoch 15, batch: 900/5760 (33.64 s), loss: 119.86, kl: [2.01, 2.50, 0.00, 0.00, 0.00, 0.95, 0.00, 3.19, 0.00, 2.41]\n",
            "Epoch 15, batch: 1000/5760 (33.80 s), loss: 119.80, kl: [2.01, 2.51, 0.00, 0.00, 0.00, 0.95, 0.00, 3.18, 0.00, 2.41]\n",
            "Epoch 15, batch: 1100/5760 (34.41 s), loss: 119.84, kl: [2.01, 2.50, 0.00, 0.00, 0.00, 0.95, 0.00, 3.18, 0.00, 2.41]\n",
            "Epoch 15, batch: 1200/5760 (33.93 s), loss: 119.90, kl: [2.02, 2.51, 0.00, 0.00, 0.00, 0.95, 0.00, 3.18, 0.00, 2.40]\n",
            "Epoch 15, batch: 1300/5760 (34.19 s), loss: 119.87, kl: [2.02, 2.50, 0.00, 0.00, 0.00, 0.95, 0.00, 3.19, 0.00, 2.41]\n",
            "Epoch 15, batch: 1400/5760 (33.77 s), loss: 120.38, kl: [2.03, 2.50, 0.00, 0.00, 0.00, 0.95, 0.00, 3.19, 0.00, 2.40]\n",
            "Epoch 15, batch: 1500/5760 (33.82 s), loss: 119.80, kl: [2.04, 2.50, 0.00, 0.00, 0.00, 0.96, 0.00, 3.18, 0.00, 2.41]\n",
            "Epoch 15, batch: 1600/5760 (34.79 s), loss: 119.70, kl: [2.02, 2.51, 0.00, 0.00, 0.00, 0.95, 0.00, 3.18, 0.00, 2.41]\n",
            "Epoch 15, batch: 1700/5760 (33.73 s), loss: 119.68, kl: [2.02, 2.50, 0.00, 0.00, 0.00, 0.95, 0.00, 3.18, 0.00, 2.40]\n",
            "Epoch 15, batch: 1800/5760 (34.26 s), loss: 119.55, kl: [2.02, 2.51, 0.00, 0.00, 0.00, 0.96, 0.00, 3.18, 0.00, 2.41]\n",
            "Epoch 15, batch: 1900/5760 (33.93 s), loss: 119.81, kl: [2.01, 2.51, 0.00, 0.00, 0.00, 0.95, 0.00, 3.19, 0.00, 2.41]\n",
            "Epoch 15, batch: 2000/5760 (33.70 s), loss: 119.32, kl: [1.99, 2.50, 0.00, 0.00, 0.00, 0.95, 0.00, 3.19, 0.00, 2.41]\n",
            "Epoch 15, batch: 2100/5760 (34.16 s), loss: 119.50, kl: [2.01, 2.50, 0.00, 0.00, 0.00, 0.96, 0.00, 3.19, 0.00, 2.42]\n",
            "Epoch 15, batch: 2200/5760 (34.23 s), loss: 119.94, kl: [2.02, 2.51, 0.00, 0.00, 0.00, 0.95, 0.00, 3.19, 0.00, 2.41]\n",
            "Epoch 15, batch: 2300/5760 (33.65 s), loss: 120.16, kl: [2.02, 2.51, 0.00, 0.00, 0.00, 0.95, 0.00, 3.20, 0.00, 2.40]\n",
            "Epoch 15, batch: 2400/5760 (33.57 s), loss: 120.18, kl: [2.02, 2.51, 0.00, 0.00, 0.00, 0.95, 0.00, 3.19, 0.00, 2.40]\n",
            "Epoch 15, batch: 2500/5760 (34.66 s), loss: 119.80, kl: [2.02, 2.51, 0.00, 0.00, 0.00, 0.95, 0.00, 3.18, 0.00, 2.40]\n",
            "Epoch 15, batch: 2600/5760 (33.92 s), loss: 119.80, kl: [2.02, 2.51, 0.00, 0.00, 0.00, 0.95, 0.00, 3.17, 0.00, 2.40]\n",
            "Epoch 15, batch: 2700/5760 (33.83 s), loss: 119.28, kl: [2.01, 2.51, 0.00, 0.00, 0.00, 0.96, 0.00, 3.18, 0.00, 2.41]\n",
            "Epoch 15, batch: 2800/5760 (33.83 s), loss: 119.57, kl: [2.02, 2.51, 0.00, 0.00, 0.00, 0.96, 0.00, 3.18, 0.00, 2.41]\n",
            "Epoch 15, batch: 2900/5760 (34.01 s), loss: 119.38, kl: [2.02, 2.51, 0.00, 0.00, 0.00, 0.95, 0.00, 3.19, 0.00, 2.40]\n",
            "Epoch 15, batch: 3000/5760 (33.77 s), loss: 119.63, kl: [2.02, 2.51, 0.00, 0.00, 0.00, 0.96, 0.00, 3.19, 0.00, 2.41]\n",
            "Epoch 15, batch: 3100/5760 (34.35 s), loss: 119.72, kl: [2.02, 2.51, 0.00, 0.00, 0.00, 0.95, 0.00, 3.18, 0.00, 2.41]\n",
            "Epoch 15, batch: 3200/5760 (33.78 s), loss: 119.82, kl: [2.02, 2.51, 0.00, 0.00, 0.00, 0.95, 0.00, 3.18, 0.00, 2.41]\n",
            "Epoch 15, batch: 3300/5760 (33.84 s), loss: 119.71, kl: [2.02, 2.51, 0.00, 0.00, 0.00, 0.97, 0.00, 3.19, 0.00, 2.41]\n",
            "Epoch 15, batch: 3400/5760 (34.60 s), loss: 119.51, kl: [2.01, 2.51, 0.00, 0.00, 0.00, 0.96, 0.00, 3.19, 0.00, 2.41]\n",
            "Epoch 15, batch: 3500/5760 (33.87 s), loss: 119.52, kl: [2.01, 2.50, 0.00, 0.00, 0.00, 0.96, 0.00, 3.18, 0.00, 2.41]\n",
            "Epoch 15, batch: 3600/5760 (33.99 s), loss: 119.61, kl: [2.02, 2.49, 0.00, 0.00, 0.00, 0.96, 0.00, 3.18, 0.00, 2.41]\n",
            "Epoch 15, batch: 3700/5760 (33.97 s), loss: 119.77, kl: [2.02, 2.50, 0.00, 0.00, 0.00, 0.95, 0.00, 3.18, 0.00, 2.42]\n",
            "Epoch 15, batch: 3800/5760 (33.89 s), loss: 119.64, kl: [2.01, 2.50, 0.00, 0.00, 0.00, 0.96, 0.00, 3.18, 0.00, 2.42]\n",
            "Epoch 15, batch: 3900/5760 (33.82 s), loss: 119.67, kl: [2.00, 2.51, 0.00, 0.00, 0.00, 0.95, 0.00, 3.18, 0.00, 2.42]\n",
            "Epoch 15, batch: 4000/5760 (34.21 s), loss: 120.01, kl: [2.02, 2.50, 0.00, 0.00, 0.00, 0.96, 0.00, 3.18, 0.00, 2.41]\n",
            "Epoch 15, batch: 4100/5760 (34.01 s), loss: 119.90, kl: [2.01, 2.51, 0.00, 0.00, 0.00, 0.96, 0.00, 3.18, 0.00, 2.42]\n",
            "Epoch 15, batch: 4200/5760 (33.89 s), loss: 119.64, kl: [2.03, 2.51, 0.00, 0.00, 0.00, 0.96, 0.00, 3.18, 0.00, 2.42]\n",
            "Epoch 15, batch: 4300/5760 (34.55 s), loss: 119.27, kl: [2.02, 2.51, 0.00, 0.00, 0.00, 0.97, 0.00, 3.17, 0.00, 2.41]\n",
            "Epoch 15, batch: 4400/5760 (34.01 s), loss: 119.25, kl: [2.02, 2.51, 0.00, 0.00, 0.00, 0.96, 0.00, 3.17, 0.00, 2.40]\n",
            "Epoch 15, batch: 4500/5760 (34.09 s), loss: 119.23, kl: [2.02, 2.51, 0.00, 0.00, 0.00, 0.96, 0.00, 3.17, 0.00, 2.41]\n",
            "Epoch 15, batch: 4600/5760 (34.07 s), loss: 119.25, kl: [2.03, 2.51, 0.00, 0.00, 0.00, 0.95, 0.00, 3.19, 0.00, 2.41]\n",
            "Epoch 15, batch: 4700/5760 (34.05 s), loss: 119.61, kl: [2.03, 2.52, 0.00, 0.00, 0.00, 0.96, 0.00, 3.19, 0.00, 2.40]\n",
            "Epoch 15, batch: 4800/5760 (33.82 s), loss: 119.64, kl: [2.04, 2.52, 0.00, 0.00, 0.00, 0.96, 0.00, 3.18, 0.00, 2.41]\n",
            "Epoch 15, batch: 4900/5760 (34.06 s), loss: 119.10, kl: [2.03, 2.51, 0.00, 0.00, 0.00, 0.95, 0.00, 3.16, 0.00, 2.40]\n",
            "Epoch 15, batch: 5000/5760 (33.76 s), loss: 119.68, kl: [2.03, 2.51, 0.00, 0.00, 0.00, 0.95, 0.00, 3.17, 0.00, 2.41]\n",
            "Epoch 15, batch: 5100/5760 (33.96 s), loss: 119.78, kl: [2.02, 2.51, 0.00, 0.00, 0.00, 0.96, 0.00, 3.19, 0.00, 2.41]\n",
            "Epoch 15, batch: 5200/5760 (33.76 s), loss: 119.24, kl: [2.02, 2.51, 0.00, 0.00, 0.00, 0.96, 0.00, 3.19, 0.00, 2.41]\n",
            "Epoch 15, batch: 5300/5760 (33.98 s), loss: 119.34, kl: [2.02, 2.51, 0.00, 0.00, 0.00, 0.96, 0.00, 3.19, 0.00, 2.41]\n",
            "Epoch 15, batch: 5400/5760 (33.51 s), loss: 119.60, kl: [2.03, 2.51, 0.00, 0.00, 0.00, 0.96, 0.00, 3.19, 0.00, 2.43]\n",
            "Epoch 15, batch: 5500/5760 (33.98 s), loss: 119.55, kl: [2.02, 2.50, 0.00, 0.00, 0.00, 0.96, 0.00, 3.18, 0.00, 2.42]\n",
            "Epoch 15, batch: 5600/5760 (33.71 s), loss: 119.01, kl: [2.02, 2.50, 0.00, 0.00, 0.00, 0.95, 0.00, 3.18, 0.00, 2.40]\n",
            "Epoch 15, batch: 5700/5760 (33.74 s), loss: 119.44, kl: [2.02, 2.51, 0.00, 0.00, 0.00, 0.96, 0.00, 3.19, 0.00, 2.41]\n",
            "Epoch 16, batch: 0/5760 (20.51 s), loss: 119.24, kl: [2.03, 2.50, 0.00, 0.00, 0.00, 0.96, 0.00, 3.19, 0.00, 2.41]\n",
            "Epoch 16, batch: 100/5760 (33.84 s), loss: 118.88, kl: [2.00, 2.50, 0.00, 0.00, 0.00, 0.96, 0.00, 3.19, 0.00, 2.41]\n",
            "Epoch 16, batch: 200/5760 (33.68 s), loss: 119.54, kl: [2.04, 2.51, 0.00, 0.00, 0.00, 0.97, 0.00, 3.19, 0.00, 2.42]\n",
            "Epoch 16, batch: 300/5760 (33.81 s), loss: 119.21, kl: [2.03, 2.50, 0.00, 0.00, 0.00, 0.97, 0.00, 3.18, 0.00, 2.41]\n",
            "Epoch 16, batch: 400/5760 (34.45 s), loss: 119.04, kl: [2.02, 2.50, 0.00, 0.00, 0.00, 0.96, 0.00, 3.19, 0.00, 2.42]\n",
            "Epoch 16, batch: 500/5760 (33.49 s), loss: 119.07, kl: [2.04, 2.50, 0.00, 0.00, 0.00, 0.97, 0.00, 3.17, 0.00, 2.41]\n",
            "Epoch 16, batch: 600/5760 (33.62 s), loss: 119.50, kl: [2.03, 2.50, 0.00, 0.00, 0.00, 0.97, 0.00, 3.18, 0.00, 2.41]\n",
            "Epoch 16, batch: 700/5760 (34.09 s), loss: 119.33, kl: [2.04, 2.51, 0.00, 0.00, 0.00, 0.97, 0.00, 3.18, 0.00, 2.42]\n",
            "Epoch 16, batch: 800/5760 (34.00 s), loss: 119.10, kl: [2.03, 2.51, 0.00, 0.00, 0.00, 0.98, 0.00, 3.18, 0.00, 2.41]\n",
            "Epoch 16, batch: 900/5760 (34.04 s), loss: 118.90, kl: [2.01, 2.51, 0.00, 0.00, 0.00, 0.97, 0.00, 3.18, 0.00, 2.41]\n",
            "Epoch 16, batch: 1000/5760 (34.59 s), loss: 119.22, kl: [2.02, 2.52, 0.00, 0.00, 0.00, 0.96, 0.00, 3.18, 0.00, 2.42]\n",
            "Epoch 16, batch: 1100/5760 (34.45 s), loss: 119.22, kl: [2.03, 2.51, 0.00, 0.00, 0.00, 0.96, 0.00, 3.18, 0.00, 2.41]\n",
            "Epoch 16, batch: 1200/5760 (33.79 s), loss: 119.17, kl: [2.02, 2.50, 0.00, 0.00, 0.00, 0.96, 0.00, 3.18, 0.00, 2.41]\n",
            "Epoch 16, batch: 1300/5760 (34.06 s), loss: 119.24, kl: [2.02, 2.51, 0.00, 0.00, 0.00, 0.96, 0.00, 3.19, 0.00, 2.40]\n",
            "Epoch 16, batch: 1400/5760 (33.62 s), loss: 119.33, kl: [2.02, 2.51, 0.00, 0.00, 0.00, 0.97, 0.00, 3.19, 0.00, 2.41]\n",
            "Epoch 16, batch: 1500/5760 (33.58 s), loss: 119.07, kl: [2.02, 2.51, 0.00, 0.00, 0.00, 0.98, 0.00, 3.18, 0.00, 2.42]\n",
            "Epoch 16, batch: 1600/5760 (33.85 s), loss: 119.19, kl: [2.04, 2.51, 0.00, 0.00, 0.00, 0.96, 0.00, 3.17, 0.00, 2.41]\n",
            "Epoch 16, batch: 1700/5760 (33.89 s), loss: 118.91, kl: [2.03, 2.51, 0.00, 0.00, 0.00, 0.96, 0.00, 3.18, 0.00, 2.41]\n",
            "Epoch 16, batch: 1800/5760 (33.87 s), loss: 119.12, kl: [2.02, 2.51, 0.00, 0.00, 0.00, 0.97, 0.00, 3.18, 0.00, 2.43]\n",
            "Epoch 16, batch: 1900/5760 (34.00 s), loss: 119.24, kl: [2.01, 2.50, 0.00, 0.00, 0.00, 0.97, 0.00, 3.19, 0.00, 2.42]\n",
            "Epoch 16, batch: 2000/5760 (34.13 s), loss: 119.20, kl: [2.02, 2.50, 0.00, 0.00, 0.00, 0.97, 0.00, 3.18, 0.00, 2.42]\n",
            "Epoch 16, batch: 2100/5760 (34.50 s), loss: 119.26, kl: [2.03, 2.51, 0.00, 0.00, 0.00, 0.96, 0.00, 3.18, 0.00, 2.42]\n",
            "Epoch 16, batch: 2200/5760 (34.43 s), loss: 119.36, kl: [2.03, 2.51, 0.00, 0.00, 0.00, 0.96, 0.00, 3.18, 0.00, 2.42]\n",
            "Epoch 16, batch: 2300/5760 (34.02 s), loss: 119.15, kl: [2.02, 2.50, 0.00, 0.00, 0.00, 0.97, 0.00, 3.18, 0.00, 2.41]\n",
            "Epoch 16, batch: 2400/5760 (33.70 s), loss: 119.42, kl: [2.01, 2.51, 0.00, 0.00, 0.00, 0.97, 0.00, 3.19, 0.00, 2.42]\n",
            "Epoch 16, batch: 2500/5760 (33.75 s), loss: 118.93, kl: [2.02, 2.50, 0.00, 0.00, 0.00, 0.96, 0.00, 3.19, 0.00, 2.42]\n",
            "Epoch 16, batch: 2600/5760 (33.97 s), loss: 119.11, kl: [2.02, 2.50, 0.00, 0.00, 0.00, 0.96, 0.00, 3.19, 0.00, 2.42]\n",
            "Epoch 16, batch: 2700/5760 (33.75 s), loss: 118.88, kl: [2.03, 2.49, 0.00, 0.00, 0.00, 0.96, 0.00, 3.19, 0.00, 2.42]\n",
            "Epoch 16, batch: 2800/5760 (34.31 s), loss: 119.03, kl: [2.03, 2.50, 0.00, 0.00, 0.00, 0.97, 0.00, 3.19, 0.00, 2.43]\n",
            "Epoch 16, batch: 2900/5760 (33.94 s), loss: 118.86, kl: [2.02, 2.50, 0.00, 0.00, 0.00, 0.96, 0.00, 3.19, 0.00, 2.43]\n",
            "Epoch 16, batch: 3000/5760 (33.69 s), loss: 118.98, kl: [2.02, 2.50, 0.00, 0.00, 0.00, 0.96, 0.00, 3.18, 0.00, 2.42]\n",
            "Epoch 16, batch: 3100/5760 (34.18 s), loss: 118.49, kl: [2.01, 2.50, 0.00, 0.00, 0.00, 0.96, 0.00, 3.17, 0.00, 2.41]\n",
            "Epoch 16, batch: 3200/5760 (34.47 s), loss: 118.72, kl: [2.02, 2.51, 0.00, 0.00, 0.00, 0.96, 0.00, 3.18, 0.00, 2.42]\n",
            "Epoch 16, batch: 3300/5760 (33.87 s), loss: 118.68, kl: [2.02, 2.50, 0.00, 0.00, 0.00, 0.98, 0.00, 3.19, 0.00, 2.42]\n",
            "Epoch 16, batch: 3400/5760 (34.04 s), loss: 118.83, kl: [2.03, 2.50, 0.00, 0.00, 0.00, 0.97, 0.00, 3.19, 0.00, 2.41]\n",
            "Epoch 16, batch: 3500/5760 (33.94 s), loss: 118.96, kl: [2.03, 2.50, 0.00, 0.00, 0.00, 0.96, 0.00, 3.20, 0.00, 2.42]\n",
            "Epoch 16, batch: 3600/5760 (34.12 s), loss: 118.82, kl: [2.02, 2.49, 0.00, 0.00, 0.00, 0.97, 0.00, 3.19, 0.00, 2.42]\n",
            "Epoch 16, batch: 3700/5760 (34.21 s), loss: 118.94, kl: [2.03, 2.50, 0.00, 0.00, 0.00, 0.97, 0.00, 3.19, 0.00, 2.42]\n",
            "Epoch 16, batch: 3800/5760 (34.27 s), loss: 118.78, kl: [2.03, 2.50, 0.00, 0.00, 0.00, 0.98, 0.00, 3.18, 0.00, 2.42]\n",
            "Epoch 16, batch: 3900/5760 (34.02 s), loss: 118.83, kl: [2.03, 2.49, 0.00, 0.00, 0.00, 0.98, 0.00, 3.18, 0.00, 2.43]\n",
            "Epoch 16, batch: 4000/5760 (34.42 s), loss: 118.83, kl: [2.03, 2.50, 0.00, 0.00, 0.00, 0.98, 0.00, 3.19, 0.00, 2.43]\n",
            "Epoch 16, batch: 4100/5760 (35.06 s), loss: 118.90, kl: [2.03, 2.51, 0.00, 0.00, 0.00, 0.97, 0.00, 3.19, 0.00, 2.42]\n",
            "Epoch 16, batch: 4200/5760 (34.00 s), loss: 118.74, kl: [2.05, 2.50, 0.00, 0.00, 0.00, 0.97, 0.00, 3.19, 0.00, 2.41]\n",
            "Epoch 16, batch: 4300/5760 (34.58 s), loss: 118.92, kl: [2.03, 2.50, 0.00, 0.00, 0.00, 0.98, 0.00, 3.20, 0.00, 2.42]\n",
            "Epoch 16, batch: 4400/5760 (34.10 s), loss: 118.74, kl: [2.03, 2.49, 0.00, 0.00, 0.00, 0.98, 0.00, 3.19, 0.00, 2.41]\n",
            "Epoch 16, batch: 4500/5760 (34.17 s), loss: 118.74, kl: [2.03, 2.50, 0.00, 0.00, 0.00, 0.97, 0.00, 3.19, 0.00, 2.42]\n",
            "Epoch 16, batch: 4600/5760 (34.84 s), loss: 119.44, kl: [2.03, 2.50, 0.00, 0.00, 0.00, 0.98, 0.00, 3.19, 0.00, 2.42]\n",
            "Epoch 16, batch: 4700/5760 (34.62 s), loss: 119.09, kl: [2.03, 2.51, 0.00, 0.00, 0.00, 0.98, 0.00, 3.19, 0.00, 2.41]\n",
            "Epoch 16, batch: 4800/5760 (34.49 s), loss: 119.25, kl: [2.03, 2.50, 0.00, 0.00, 0.00, 0.97, 0.00, 3.19, 0.00, 2.43]\n",
            "Epoch 16, batch: 4900/5760 (34.25 s), loss: 119.10, kl: [2.02, 2.51, 0.00, 0.00, 0.00, 0.97, 0.00, 3.19, 0.00, 2.43]\n",
            "Epoch 16, batch: 5000/5760 (34.45 s), loss: 118.91, kl: [2.04, 2.50, 0.00, 0.00, 0.00, 0.97, 0.00, 3.19, 0.00, 2.42]\n",
            "Epoch 16, batch: 5100/5760 (34.59 s), loss: 118.67, kl: [2.03, 2.51, 0.00, 0.00, 0.00, 0.98, 0.00, 3.18, 0.00, 2.42]\n",
            "Epoch 16, batch: 5200/5760 (34.26 s), loss: 118.78, kl: [2.03, 2.51, 0.00, 0.00, 0.00, 0.98, 0.00, 3.19, 0.00, 2.43]\n",
            "Epoch 16, batch: 5300/5760 (34.11 s), loss: 119.03, kl: [2.02, 2.51, 0.00, 0.00, 0.00, 0.98, 0.00, 3.19, 0.00, 2.43]\n",
            "Epoch 16, batch: 5400/5760 (33.98 s), loss: 118.99, kl: [2.01, 2.51, 0.00, 0.00, 0.00, 0.97, 0.00, 3.19, 0.00, 2.42]\n",
            "Epoch 16, batch: 5500/5760 (34.40 s), loss: 118.65, kl: [2.03, 2.50, 0.00, 0.00, 0.00, 0.98, 0.00, 3.20, 0.00, 2.42]\n",
            "Epoch 16, batch: 5600/5760 (33.90 s), loss: 118.95, kl: [2.02, 2.50, 0.00, 0.00, 0.00, 0.98, 0.00, 3.20, 0.00, 2.42]\n",
            "Epoch 16, batch: 5700/5760 (33.85 s), loss: 118.55, kl: [2.02, 2.50, 0.00, 0.00, 0.00, 0.99, 0.00, 3.20, 0.00, 2.42]\n",
            "Epoch 17, batch: 0/5760 (20.35 s), loss: 118.92, kl: [2.01, 2.50, 0.00, 0.00, 0.00, 0.98, 0.00, 3.20, 0.00, 2.43]\n",
            "Epoch 17, batch: 100/5760 (34.71 s), loss: 118.52, kl: [2.01, 2.51, 0.00, 0.00, 0.00, 0.98, 0.00, 3.20, 0.00, 2.43]\n",
            "Epoch 17, batch: 200/5760 (33.95 s), loss: 118.82, kl: [2.02, 2.51, 0.00, 0.00, 0.00, 0.98, 0.00, 3.19, 0.00, 2.43]\n",
            "Epoch 17, batch: 300/5760 (34.31 s), loss: 118.96, kl: [2.02, 2.51, 0.00, 0.00, 0.00, 0.97, 0.00, 3.19, 0.00, 2.43]\n",
            "Epoch 17, batch: 400/5760 (34.04 s), loss: 118.79, kl: [2.03, 2.51, 0.00, 0.00, 0.00, 0.97, 0.00, 3.19, 0.00, 2.42]\n",
            "Epoch 17, batch: 500/5760 (33.93 s), loss: 118.88, kl: [2.04, 2.51, 0.00, 0.00, 0.00, 0.98, 0.00, 3.20, 0.00, 2.43]\n",
            "Epoch 17, batch: 600/5760 (34.16 s), loss: 118.53, kl: [2.03, 2.51, 0.00, 0.00, 0.00, 0.98, 0.00, 3.19, 0.00, 2.44]\n",
            "Epoch 17, batch: 700/5760 (34.43 s), loss: 118.93, kl: [2.03, 2.52, 0.00, 0.00, 0.00, 0.98, 0.00, 3.19, 0.00, 2.44]\n",
            "Epoch 17, batch: 800/5760 (34.13 s), loss: 118.92, kl: [2.04, 2.51, 0.00, 0.00, 0.00, 0.99, 0.00, 3.19, 0.00, 2.43]\n",
            "Epoch 17, batch: 900/5760 (34.16 s), loss: 118.61, kl: [2.02, 2.50, 0.00, 0.00, 0.00, 0.99, 0.00, 3.18, 0.00, 2.43]\n",
            "Epoch 17, batch: 1000/5760 (34.13 s), loss: 118.81, kl: [2.03, 2.50, 0.00, 0.00, 0.00, 0.99, 0.00, 3.19, 0.00, 2.42]\n",
            "Epoch 17, batch: 1100/5760 (34.62 s), loss: 118.51, kl: [2.02, 2.50, 0.00, 0.00, 0.00, 0.98, 0.00, 3.19, 0.00, 2.42]\n",
            "Epoch 17, batch: 1200/5760 (33.80 s), loss: 118.38, kl: [2.02, 2.51, 0.00, 0.00, 0.00, 0.98, 0.00, 3.19, 0.00, 2.42]\n",
            "Epoch 17, batch: 1300/5760 (33.97 s), loss: 118.15, kl: [2.02, 2.50, 0.00, 0.00, 0.00, 0.98, 0.00, 3.19, 0.00, 2.42]\n",
            "Epoch 17, batch: 1400/5760 (34.44 s), loss: 118.55, kl: [2.03, 2.50, 0.00, 0.00, 0.00, 0.99, 0.00, 3.18, 0.00, 2.42]\n",
            "Epoch 17, batch: 1500/5760 (34.58 s), loss: 118.48, kl: [2.03, 2.50, 0.00, 0.00, 0.00, 0.98, 0.00, 3.18, 0.00, 2.42]\n",
            "Epoch 17, batch: 1600/5760 (34.19 s), loss: 118.37, kl: [2.03, 2.51, 0.00, 0.00, 0.00, 0.98, 0.00, 3.18, 0.00, 2.43]\n",
            "Epoch 17, batch: 1700/5760 (34.36 s), loss: 118.68, kl: [2.03, 2.52, 0.00, 0.00, 0.00, 0.98, 0.00, 3.19, 0.00, 2.43]\n",
            "Epoch 17, batch: 1800/5760 (33.88 s), loss: 118.73, kl: [2.03, 2.52, 0.00, 0.00, 0.00, 0.97, 0.00, 3.19, 0.00, 2.42]\n",
            "Epoch 17, batch: 1900/5760 (33.89 s), loss: 118.65, kl: [2.03, 2.51, 0.00, 0.00, 0.00, 0.98, 0.00, 3.19, 0.00, 2.41]\n",
            "Epoch 17, batch: 2000/5760 (34.57 s), loss: 118.15, kl: [2.03, 2.51, 0.00, 0.00, 0.00, 0.98, 0.00, 3.19, 0.00, 2.41]\n",
            "Epoch 17, batch: 2100/5760 (34.54 s), loss: 118.15, kl: [2.04, 2.51, 0.00, 0.00, 0.00, 0.99, 0.00, 3.19, 0.00, 2.42]\n",
            "Epoch 17, batch: 2200/5760 (33.85 s), loss: 118.07, kl: [2.03, 2.51, 0.00, 0.00, 0.00, 0.99, 0.00, 3.19, 0.00, 2.43]\n",
            "Epoch 17, batch: 2300/5760 (33.92 s), loss: 118.16, kl: [2.02, 2.52, 0.00, 0.00, 0.00, 0.99, 0.00, 3.19, 0.00, 2.44]\n",
            "Epoch 17, batch: 2400/5760 (34.48 s), loss: 118.39, kl: [2.03, 2.51, 0.00, 0.00, 0.00, 0.99, 0.00, 3.20, 0.00, 2.44]\n",
            "Epoch 17, batch: 2500/5760 (34.12 s), loss: 118.63, kl: [2.02, 2.52, 0.00, 0.00, 0.00, 0.99, 0.00, 3.20, 0.00, 2.43]\n",
            "Epoch 17, batch: 2600/5760 (34.26 s), loss: 118.51, kl: [2.02, 2.50, 0.00, 0.00, 0.00, 0.99, 0.00, 3.21, 0.00, 2.43]\n",
            "Epoch 17, batch: 2700/5760 (34.11 s), loss: 118.41, kl: [2.03, 2.51, 0.00, 0.00, 0.00, 0.99, 0.00, 3.20, 0.00, 2.43]\n",
            "Epoch 17, batch: 2800/5760 (33.96 s), loss: 118.32, kl: [2.03, 2.51, 0.00, 0.00, 0.00, 0.99, 0.00, 3.19, 0.00, 2.45]\n",
            "Epoch 17, batch: 2900/5760 (34.31 s), loss: 118.38, kl: [2.04, 2.50, 0.00, 0.00, 0.00, 0.98, 0.00, 3.20, 0.00, 2.45]\n",
            "Epoch 17, batch: 3000/5760 (34.24 s), loss: 118.45, kl: [2.04, 2.51, 0.00, 0.00, 0.00, 0.99, 0.00, 3.19, 0.00, 2.44]\n",
            "Epoch 17, batch: 3100/5760 (34.03 s), loss: 118.75, kl: [2.04, 2.50, 0.00, 0.00, 0.00, 0.98, 0.00, 3.20, 0.00, 2.42]\n",
            "Epoch 17, batch: 3200/5760 (33.72 s), loss: 118.40, kl: [2.03, 2.50, 0.00, 0.00, 0.00, 0.99, 0.00, 3.19, 0.00, 2.43]\n",
            "Epoch 17, batch: 3300/5760 (34.43 s), loss: 118.39, kl: [2.03, 2.51, 0.00, 0.00, 0.00, 0.99, 0.00, 3.20, 0.00, 2.43]\n",
            "Epoch 17, batch: 3400/5760 (33.82 s), loss: 118.43, kl: [2.03, 2.50, 0.00, 0.00, 0.00, 0.99, 0.00, 3.20, 0.00, 2.42]\n",
            "Epoch 17, batch: 3500/5760 (33.78 s), loss: 118.56, kl: [2.04, 2.51, 0.00, 0.00, 0.00, 0.99, 0.00, 3.20, 0.00, 2.43]\n",
            "Epoch 17, batch: 3600/5760 (34.14 s), loss: 118.44, kl: [2.03, 2.50, 0.00, 0.00, 0.00, 0.99, 0.00, 3.20, 0.00, 2.44]\n",
            "Epoch 17, batch: 3700/5760 (34.37 s), loss: 118.66, kl: [2.05, 2.50, 0.00, 0.00, 0.00, 0.99, 0.00, 3.20, 0.00, 2.42]\n",
            "Epoch 17, batch: 3800/5760 (34.59 s), loss: 118.57, kl: [2.04, 2.51, 0.00, 0.00, 0.00, 0.99, 0.00, 3.19, 0.00, 2.42]\n",
            "Epoch 17, batch: 3900/5760 (33.99 s), loss: 118.27, kl: [2.04, 2.51, 0.00, 0.00, 0.00, 0.99, 0.00, 3.19, 0.00, 2.42]\n",
            "Epoch 17, batch: 4000/5760 (34.23 s), loss: 118.24, kl: [2.04, 2.50, 0.00, 0.00, 0.00, 0.99, 0.00, 3.20, 0.00, 2.43]\n",
            "Epoch 17, batch: 4100/5760 (34.68 s), loss: 118.26, kl: [2.02, 2.50, 0.00, 0.00, 0.00, 0.99, 0.00, 3.20, 0.00, 2.43]\n",
            "Epoch 17, batch: 4200/5760 (34.99 s), loss: 117.91, kl: [2.02, 2.51, 0.00, 0.00, 0.00, 1.00, 0.00, 3.19, 0.00, 2.42]\n",
            "Epoch 17, batch: 4300/5760 (34.52 s), loss: 118.01, kl: [2.03, 2.51, 0.00, 0.00, 0.00, 1.00, 0.00, 3.18, 0.00, 2.42]\n",
            "Epoch 17, batch: 4400/5760 (34.68 s), loss: 118.24, kl: [2.02, 2.51, 0.00, 0.00, 0.00, 1.00, 0.00, 3.20, 0.00, 2.43]\n",
            "Epoch 17, batch: 4500/5760 (34.62 s), loss: 118.45, kl: [2.02, 2.52, 0.00, 0.00, 0.00, 0.98, 0.00, 3.19, 0.00, 2.43]\n",
            "Epoch 17, batch: 4600/5760 (34.29 s), loss: 118.08, kl: [2.04, 2.52, 0.00, 0.00, 0.00, 0.99, 0.00, 3.19, 0.00, 2.43]\n",
            "Epoch 17, batch: 4700/5760 (35.29 s), loss: 118.39, kl: [2.04, 2.51, 0.00, 0.00, 0.00, 0.99, 0.00, 3.19, 0.00, 2.42]\n",
            "Epoch 17, batch: 4800/5760 (34.69 s), loss: 118.05, kl: [2.03, 2.51, 0.00, 0.00, 0.00, 0.99, 0.00, 3.19, 0.00, 2.43]\n",
            "Epoch 17, batch: 4900/5760 (34.62 s), loss: 118.13, kl: [2.04, 2.51, 0.00, 0.00, 0.00, 0.99, 0.00, 3.19, 0.00, 2.41]\n",
            "Epoch 17, batch: 5000/5760 (34.92 s), loss: 117.99, kl: [2.03, 2.51, 0.00, 0.00, 0.00, 1.00, 0.00, 3.20, 0.00, 2.43]\n",
            "Epoch 17, batch: 5100/5760 (35.44 s), loss: 118.02, kl: [2.03, 2.51, 0.00, 0.00, 0.00, 1.00, 0.00, 3.19, 0.00, 2.44]\n",
            "Epoch 17, batch: 5200/5760 (34.63 s), loss: 118.23, kl: [2.04, 2.51, 0.00, 0.00, 0.00, 1.00, 0.00, 3.19, 0.00, 2.44]\n",
            "Epoch 17, batch: 5300/5760 (34.53 s), loss: 118.05, kl: [2.04, 2.50, 0.00, 0.00, 0.00, 1.00, 0.00, 3.19, 0.00, 2.44]\n",
            "Epoch 17, batch: 5400/5760 (34.70 s), loss: 118.14, kl: [2.06, 2.51, 0.00, 0.00, 0.00, 1.00, 0.00, 3.19, 0.00, 2.44]\n",
            "Epoch 17, batch: 5500/5760 (34.59 s), loss: 118.10, kl: [2.04, 2.50, 0.00, 0.00, 0.00, 1.00, 0.00, 3.20, 0.00, 2.45]\n",
            "Epoch 17, batch: 5600/5760 (34.88 s), loss: 118.23, kl: [2.05, 2.50, 0.00, 0.00, 0.00, 1.00, 0.00, 3.20, 0.00, 2.43]\n",
            "Epoch 17, batch: 5700/5760 (34.55 s), loss: 118.14, kl: [2.04, 2.51, 0.00, 0.00, 0.00, 0.99, 0.00, 3.19, 0.00, 2.43]\n",
            "Epoch 18, batch: 0/5760 (20.85 s), loss: 118.32, kl: [2.03, 2.51, 0.00, 0.00, 0.00, 0.99, 0.00, 3.20, 0.00, 2.43]\n",
            "Epoch 18, batch: 100/5760 (34.75 s), loss: 117.79, kl: [2.03, 2.51, 0.00, 0.00, 0.00, 1.00, 0.00, 3.19, 0.00, 2.42]\n",
            "Epoch 18, batch: 200/5760 (34.63 s), loss: 117.96, kl: [2.03, 2.51, 0.00, 0.00, 0.00, 0.99, 0.00, 3.19, 0.00, 2.43]\n",
            "Epoch 18, batch: 300/5760 (34.61 s), loss: 118.05, kl: [2.03, 2.51, 0.00, 0.00, 0.00, 0.98, 0.00, 3.20, 0.00, 2.43]\n",
            "Epoch 18, batch: 400/5760 (34.78 s), loss: 118.02, kl: [2.04, 2.51, 0.00, 0.00, 0.00, 0.99, 0.00, 3.20, 0.00, 2.43]\n",
            "Epoch 18, batch: 500/5760 (34.61 s), loss: 117.71, kl: [2.04, 2.51, 0.00, 0.00, 0.00, 0.99, 0.00, 3.20, 0.00, 2.44]\n",
            "Epoch 18, batch: 600/5760 (34.54 s), loss: 118.26, kl: [2.05, 2.51, 0.00, 0.00, 0.00, 0.99, 0.00, 3.20, 0.00, 2.43]\n",
            "Epoch 18, batch: 700/5760 (35.11 s), loss: 118.20, kl: [2.02, 2.51, 0.00, 0.00, 0.00, 0.99, 0.00, 3.19, 0.00, 2.44]\n",
            "Epoch 18, batch: 800/5760 (34.65 s), loss: 117.97, kl: [2.02, 2.52, 0.00, 0.00, 0.00, 1.00, 0.00, 3.20, 0.00, 2.43]\n",
            "Epoch 18, batch: 900/5760 (34.56 s), loss: 117.79, kl: [2.03, 2.51, 0.00, 0.00, 0.00, 1.00, 0.00, 3.19, 0.00, 2.43]\n",
            "Epoch 18, batch: 1000/5760 (34.72 s), loss: 118.11, kl: [2.03, 2.51, 0.00, 0.00, 0.00, 0.99, 0.00, 3.19, 0.00, 2.43]\n",
            "Epoch 18, batch: 1100/5760 (35.24 s), loss: 118.03, kl: [2.03, 2.51, 0.00, 0.00, 0.00, 1.01, 0.00, 3.20, 0.00, 2.44]\n",
            "Epoch 18, batch: 1200/5760 (34.48 s), loss: 117.44, kl: [2.03, 2.52, 0.00, 0.00, 0.00, 1.01, 0.00, 3.19, 0.00, 2.43]\n",
            "Epoch 18, batch: 1300/5760 (34.79 s), loss: 117.97, kl: [2.04, 2.52, 0.00, 0.00, 0.00, 1.01, 0.00, 3.19, 0.00, 2.43]\n",
            "Epoch 18, batch: 1400/5760 (34.66 s), loss: 118.33, kl: [2.06, 2.51, 0.00, 0.00, 0.00, 1.01, 0.00, 3.20, 0.00, 2.44]\n",
            "Epoch 18, batch: 1500/5760 (34.61 s), loss: 117.90, kl: [2.05, 2.50, 0.00, 0.00, 0.00, 1.01, 0.00, 3.19, 0.00, 2.44]\n",
            "Epoch 18, batch: 1600/5760 (35.01 s), loss: 117.99, kl: [2.05, 2.51, 0.00, 0.00, 0.00, 1.01, 0.00, 3.20, 0.00, 2.44]\n",
            "Epoch 18, batch: 1700/5760 (34.66 s), loss: 117.92, kl: [2.05, 2.50, 0.00, 0.00, 0.00, 1.01, 0.00, 3.19, 0.00, 2.43]\n",
            "Epoch 18, batch: 1800/5760 (34.66 s), loss: 117.96, kl: [2.04, 2.51, 0.00, 0.00, 0.00, 1.01, 0.00, 3.20, 0.00, 2.43]\n",
            "Epoch 18, batch: 1900/5760 (34.88 s), loss: 117.68, kl: [2.04, 2.51, 0.00, 0.00, 0.00, 1.01, 0.00, 3.19, 0.00, 2.42]\n",
            "Epoch 18, batch: 2000/5760 (35.01 s), loss: 117.82, kl: [2.06, 2.51, 0.00, 0.00, 0.00, 1.01, 0.00, 3.19, 0.00, 2.43]\n",
            "Epoch 18, batch: 2100/5760 (35.19 s), loss: 117.76, kl: [2.05, 2.51, 0.00, 0.00, 0.00, 1.00, 0.00, 3.19, 0.00, 2.43]\n",
            "Epoch 18, batch: 2200/5760 (34.68 s), loss: 117.56, kl: [2.05, 2.50, 0.00, 0.00, 0.00, 1.00, 0.00, 3.20, 0.00, 2.43]\n",
            "Epoch 18, batch: 2300/5760 (34.69 s), loss: 117.69, kl: [2.04, 2.50, 0.00, 0.00, 0.00, 1.00, 0.00, 3.19, 0.00, 2.44]\n",
            "Epoch 18, batch: 2400/5760 (34.70 s), loss: 117.99, kl: [2.05, 2.52, 0.00, 0.00, 0.00, 1.00, 0.00, 3.19, 0.00, 2.43]\n",
            "Epoch 18, batch: 2500/5760 (35.18 s), loss: 117.59, kl: [2.06, 2.52, 0.00, 0.00, 0.00, 1.01, 0.00, 3.19, 0.00, 2.43]\n",
            "Epoch 18, batch: 2600/5760 (34.53 s), loss: 117.75, kl: [2.04, 2.52, 0.00, 0.00, 0.00, 1.01, 0.00, 3.19, 0.00, 2.44]\n",
            "Epoch 18, batch: 2700/5760 (34.95 s), loss: 117.67, kl: [2.04, 2.51, 0.00, 0.00, 0.00, 1.01, 0.00, 3.20, 0.00, 2.43]\n",
            "Epoch 18, batch: 2800/5760 (34.97 s), loss: 117.67, kl: [2.04, 2.51, 0.00, 0.00, 0.00, 1.00, 0.00, 3.20, 0.00, 2.43]\n",
            "Epoch 18, batch: 2900/5760 (35.26 s), loss: 117.73, kl: [2.04, 2.52, 0.00, 0.00, 0.00, 1.01, 0.00, 3.21, 0.00, 2.43]\n",
            "Epoch 18, batch: 3000/5760 (35.11 s), loss: 117.77, kl: [2.04, 2.51, 0.00, 0.00, 0.00, 1.01, 0.00, 3.21, 0.00, 2.45]\n",
            "Epoch 18, batch: 3100/5760 (35.40 s), loss: 117.56, kl: [2.04, 2.50, 0.00, 0.00, 0.00, 1.00, 0.00, 3.20, 0.00, 2.45]\n",
            "Epoch 18, batch: 3200/5760 (34.86 s), loss: 117.45, kl: [2.02, 2.50, 0.00, 0.00, 0.00, 1.01, 0.00, 3.20, 0.00, 2.45]\n",
            "Epoch 18, batch: 3300/5760 (34.78 s), loss: 117.36, kl: [2.03, 2.50, 0.00, 0.00, 0.00, 1.02, 0.00, 3.20, 0.00, 2.45]\n",
            "Epoch 18, batch: 3400/5760 (35.12 s), loss: 117.36, kl: [2.03, 2.50, 0.00, 0.00, 0.00, 1.01, 0.00, 3.20, 0.00, 2.44]\n",
            "Epoch 18, batch: 3500/5760 (34.64 s), loss: 117.80, kl: [2.04, 2.50, 0.00, 0.00, 0.00, 1.01, 0.00, 3.19, 0.00, 2.44]\n",
            "Epoch 18, batch: 3600/5760 (34.52 s), loss: 117.76, kl: [2.03, 2.51, 0.00, 0.00, 0.00, 1.01, 0.00, 3.19, 0.00, 2.45]\n",
            "Epoch 18, batch: 3700/5760 (34.64 s), loss: 117.79, kl: [2.04, 2.51, 0.00, 0.00, 0.00, 1.01, 0.00, 3.20, 0.00, 2.45]\n",
            "Epoch 18, batch: 3800/5760 (34.82 s), loss: 117.60, kl: [2.05, 2.50, 0.00, 0.00, 0.00, 1.01, 0.00, 3.19, 0.00, 2.44]\n",
            "Epoch 18, batch: 3900/5760 (34.36 s), loss: 117.47, kl: [2.06, 2.50, 0.00, 0.00, 0.00, 1.02, 0.00, 3.19, 0.00, 2.44]\n",
            "Epoch 18, batch: 4000/5760 (34.62 s), loss: 117.54, kl: [2.04, 2.50, 0.00, 0.00, 0.00, 1.01, 0.00, 3.19, 0.00, 2.43]\n",
            "Epoch 18, batch: 4100/5760 (34.92 s), loss: 117.75, kl: [2.03, 2.51, 0.00, 0.00, 0.00, 1.01, 0.00, 3.20, 0.00, 2.43]\n",
            "Epoch 18, batch: 4200/5760 (34.58 s), loss: 117.92, kl: [2.04, 2.51, 0.00, 0.00, 0.00, 1.01, 0.00, 3.20, 0.00, 2.45]\n",
            "Epoch 18, batch: 4300/5760 (34.70 s), loss: 117.24, kl: [2.03, 2.51, 0.00, 0.00, 0.00, 1.02, 0.00, 3.19, 0.00, 2.44]\n",
            "Epoch 18, batch: 4400/5760 (34.42 s), loss: 117.72, kl: [2.03, 2.51, 0.00, 0.00, 0.00, 1.01, 0.00, 3.20, 0.00, 2.44]\n",
            "Epoch 18, batch: 4500/5760 (34.31 s), loss: 117.79, kl: [2.04, 2.51, 0.00, 0.00, 0.00, 1.02, 0.00, 3.20, 0.00, 2.44]\n",
            "Epoch 18, batch: 4600/5760 (34.34 s), loss: 117.58, kl: [2.04, 2.51, 0.00, 0.00, 0.00, 1.02, 0.00, 3.20, 0.00, 2.45]\n",
            "Epoch 18, batch: 4700/5760 (34.76 s), loss: 117.66, kl: [2.04, 2.51, 0.00, 0.00, 0.00, 1.01, 0.00, 3.20, 0.00, 2.46]\n",
            "Epoch 18, batch: 4800/5760 (34.63 s), loss: 117.81, kl: [2.05, 2.52, 0.00, 0.00, 0.00, 1.01, 0.00, 3.20, 0.00, 2.45]\n",
            "Epoch 18, batch: 4900/5760 (34.11 s), loss: 117.62, kl: [2.04, 2.51, 0.00, 0.00, 0.00, 1.02, 0.00, 3.20, 0.00, 2.45]\n",
            "Epoch 18, batch: 5000/5760 (34.19 s), loss: 117.55, kl: [2.04, 2.51, 0.00, 0.00, 0.00, 1.02, 0.00, 3.20, 0.00, 2.45]\n",
            "Epoch 18, batch: 5100/5760 (34.53 s), loss: 117.88, kl: [2.05, 2.52, 0.00, 0.00, 0.00, 1.03, 0.00, 3.21, 0.00, 2.44]\n",
            "Epoch 18, batch: 5200/5760 (34.93 s), loss: 117.63, kl: [2.04, 2.50, 0.00, 0.00, 0.00, 1.02, 0.00, 3.20, 0.00, 2.44]\n",
            "Epoch 18, batch: 5300/5760 (34.37 s), loss: 117.53, kl: [2.04, 2.51, 0.00, 0.00, 0.00, 1.03, 0.00, 3.20, 0.00, 2.45]\n",
            "Epoch 18, batch: 5400/5760 (34.09 s), loss: 117.64, kl: [2.04, 2.52, 0.00, 0.00, 0.00, 1.01, 0.00, 3.20, 0.00, 2.44]\n",
            "Epoch 18, batch: 5500/5760 (34.36 s), loss: 117.64, kl: [2.05, 2.51, 0.00, 0.00, 0.00, 1.02, 0.00, 3.20, 0.00, 2.44]\n",
            "Epoch 18, batch: 5600/5760 (34.58 s), loss: 117.74, kl: [2.06, 2.51, 0.00, 0.00, 0.00, 1.03, 0.00, 3.21, 0.00, 2.45]\n",
            "Epoch 18, batch: 5700/5760 (34.37 s), loss: 117.80, kl: [2.07, 2.51, 0.00, 0.00, 0.00, 1.02, 0.00, 3.20, 0.00, 2.44]\n",
            "Epoch 19, batch: 0/5760 (20.60 s), loss: 117.47, kl: [2.07, 2.52, 0.00, 0.00, 0.00, 1.03, 0.00, 3.20, 0.00, 2.45]\n",
            "Epoch 19, batch: 100/5760 (34.58 s), loss: 117.35, kl: [2.07, 2.51, 0.00, 0.00, 0.00, 1.02, 0.00, 3.20, 0.00, 2.45]\n",
            "Epoch 19, batch: 200/5760 (34.14 s), loss: 117.40, kl: [2.07, 2.52, 0.00, 0.00, 0.00, 1.02, 0.00, 3.20, 0.00, 2.45]\n",
            "Epoch 19, batch: 300/5760 (34.59 s), loss: 117.47, kl: [2.05, 2.51, 0.00, 0.00, 0.00, 1.02, 0.00, 3.19, 0.00, 2.45]\n",
            "Epoch 19, batch: 400/5760 (35.37 s), loss: 117.29, kl: [2.05, 2.51, 0.00, 0.00, 0.00, 1.02, 0.00, 3.20, 0.00, 2.45]\n",
            "Epoch 19, batch: 500/5760 (35.29 s), loss: 117.72, kl: [2.05, 2.52, 0.00, 0.00, 0.00, 1.02, 0.00, 3.20, 0.00, 2.46]\n",
            "Epoch 19, batch: 600/5760 (35.03 s), loss: 117.65, kl: [2.05, 2.51, 0.00, 0.00, 0.00, 1.04, 0.00, 3.21, 0.00, 2.45]\n",
            "Epoch 19, batch: 700/5760 (35.51 s), loss: 117.31, kl: [2.05, 2.51, 0.00, 0.00, 0.00, 1.03, 0.00, 3.20, 0.00, 2.45]\n",
            "Epoch 19, batch: 800/5760 (34.82 s), loss: 117.28, kl: [2.04, 2.51, 0.00, 0.00, 0.00, 1.02, 0.00, 3.20, 0.00, 2.45]\n",
            "Epoch 19, batch: 900/5760 (34.57 s), loss: 117.30, kl: [2.04, 2.52, 0.00, 0.00, 0.00, 1.02, 0.00, 3.20, 0.00, 2.45]\n",
            "Epoch 19, batch: 1000/5760 (34.44 s), loss: 117.14, kl: [2.04, 2.51, 0.00, 0.00, 0.00, 1.02, 0.00, 3.21, 0.00, 2.45]\n",
            "Epoch 19, batch: 1100/5760 (34.85 s), loss: 116.87, kl: [2.03, 2.51, 0.00, 0.00, 0.00, 1.03, 0.00, 3.20, 0.00, 2.45]\n",
            "Epoch 19, batch: 1200/5760 (34.69 s), loss: 117.21, kl: [2.03, 2.51, 0.00, 0.00, 0.00, 1.04, 0.00, 3.21, 0.00, 2.45]\n",
            "Epoch 19, batch: 1300/5760 (35.64 s), loss: 117.24, kl: [2.02, 2.52, 0.00, 0.00, 0.00, 1.03, 0.00, 3.22, 0.00, 2.44]\n",
            "Epoch 19, batch: 1400/5760 (35.02 s), loss: 117.15, kl: [2.04, 2.51, 0.00, 0.00, 0.00, 1.02, 0.00, 3.21, 0.00, 2.45]\n",
            "Epoch 19, batch: 1500/5760 (34.89 s), loss: 117.25, kl: [2.07, 2.50, 0.00, 0.00, 0.00, 1.02, 0.00, 3.20, 0.00, 2.45]\n",
            "Epoch 19, batch: 1600/5760 (35.28 s), loss: 117.59, kl: [2.06, 2.51, 0.00, 0.00, 0.00, 1.02, 0.00, 3.20, 0.00, 2.45]\n",
            "Epoch 19, batch: 1700/5760 (34.95 s), loss: 117.47, kl: [2.06, 2.51, 0.00, 0.00, 0.00, 1.03, 0.00, 3.21, 0.00, 2.46]\n",
            "Epoch 19, batch: 1800/5760 (34.91 s), loss: 117.37, kl: [2.05, 2.51, 0.00, 0.00, 0.00, 1.02, 0.00, 3.21, 0.00, 2.46]\n",
            "Epoch 19, batch: 1900/5760 (34.84 s), loss: 117.54, kl: [2.04, 2.51, 0.00, 0.00, 0.00, 1.03, 0.00, 3.20, 0.00, 2.46]\n",
            "Epoch 19, batch: 2000/5760 (34.94 s), loss: 117.39, kl: [2.04, 2.50, 0.00, 0.00, 0.00, 1.03, 0.00, 3.20, 0.00, 2.44]\n",
            "Epoch 19, batch: 2100/5760 (35.55 s), loss: 117.34, kl: [2.04, 2.50, 0.00, 0.00, 0.00, 1.04, 0.00, 3.21, 0.00, 2.44]\n",
            "Epoch 19, batch: 2200/5760 (34.96 s), loss: 117.17, kl: [2.04, 2.51, 0.00, 0.00, 0.00, 1.03, 0.00, 3.20, 0.00, 2.44]\n",
            "Epoch 19, batch: 2300/5760 (34.84 s), loss: 116.96, kl: [2.05, 2.52, 0.00, 0.00, 0.00, 1.03, 0.00, 3.20, 0.00, 2.45]\n",
            "Epoch 19, batch: 2400/5760 (34.85 s), loss: 117.27, kl: [2.04, 2.51, 0.00, 0.00, 0.00, 1.04, 0.00, 3.20, 0.00, 2.45]\n",
            "Epoch 19, batch: 2500/5760 (35.19 s), loss: 117.11, kl: [2.05, 2.51, 0.00, 0.00, 0.00, 1.03, 0.00, 3.20, 0.00, 2.47]\n",
            "Epoch 19, batch: 2600/5760 (34.69 s), loss: 116.96, kl: [2.05, 2.51, 0.00, 0.00, 0.00, 1.03, 0.00, 3.21, 0.00, 2.46]\n",
            "Epoch 19, batch: 2700/5760 (34.27 s), loss: 116.75, kl: [2.03, 2.52, 0.00, 0.00, 0.00, 1.04, 0.00, 3.20, 0.00, 2.45]\n",
            "Epoch 19, batch: 2800/5760 (34.44 s), loss: 117.25, kl: [2.05, 2.52, 0.00, 0.00, 0.00, 1.03, 0.00, 3.20, 0.00, 2.45]\n",
            "Epoch 19, batch: 2900/5760 (34.77 s), loss: 116.83, kl: [2.05, 2.51, 0.00, 0.00, 0.00, 1.04, 0.00, 3.20, 0.00, 2.46]\n",
            "Epoch 19, batch: 3000/5760 (35.13 s), loss: 116.89, kl: [2.05, 2.51, 0.00, 0.00, 0.00, 1.02, 0.00, 3.20, 0.00, 2.43]\n",
            "Epoch 19, batch: 3100/5760 (35.48 s), loss: 117.20, kl: [2.05, 2.52, 0.00, 0.00, 0.00, 1.02, 0.00, 3.20, 0.00, 2.44]\n",
            "Epoch 19, batch: 3200/5760 (35.00 s), loss: 117.44, kl: [2.06, 2.52, 0.00, 0.00, 0.00, 1.04, 0.00, 3.21, 0.00, 2.44]\n",
            "Epoch 19, batch: 3300/5760 (34.77 s), loss: 117.08, kl: [2.04, 2.51, 0.00, 0.00, 0.00, 1.03, 0.00, 3.21, 0.00, 2.45]\n",
            "Epoch 19, batch: 3400/5760 (34.64 s), loss: 117.25, kl: [2.04, 2.52, 0.00, 0.00, 0.00, 1.03, 0.00, 3.21, 0.00, 2.45]\n",
            "Epoch 19, batch: 3500/5760 (34.78 s), loss: 116.88, kl: [2.05, 2.51, 0.00, 0.00, 0.00, 1.04, 0.00, 3.20, 0.00, 2.45]\n",
            "Epoch 19, batch: 3600/5760 (34.32 s), loss: 117.03, kl: [2.05, 2.51, 0.00, 0.00, 0.00, 1.03, 0.00, 3.20, 0.00, 2.45]\n",
            "Epoch 19, batch: 3700/5760 (34.40 s), loss: 117.14, kl: [2.05, 2.51, 0.00, 0.00, 0.00, 1.04, 0.00, 3.21, 0.00, 2.44]\n",
            "Epoch 19, batch: 3800/5760 (34.62 s), loss: 117.29, kl: [2.06, 2.52, 0.00, 0.00, 0.00, 1.04, 0.00, 3.21, 0.00, 2.44]\n",
            "Epoch 19, batch: 3900/5760 (35.26 s), loss: 117.07, kl: [2.06, 2.51, 0.00, 0.00, 0.00, 1.03, 0.00, 3.21, 0.00, 2.45]\n",
            "Epoch 19, batch: 4000/5760 (35.27 s), loss: 117.41, kl: [2.05, 2.52, 0.00, 0.00, 0.00, 1.03, 0.00, 3.21, 0.00, 2.44]\n",
            "Epoch 19, batch: 4100/5760 (35.20 s), loss: 117.67, kl: [2.05, 2.52, 0.00, 0.00, 0.00, 1.04, 0.00, 3.22, 0.00, 2.45]\n",
            "Epoch 19, batch: 4200/5760 (35.08 s), loss: 117.43, kl: [2.05, 2.51, 0.00, 0.00, 0.00, 1.05, 0.00, 3.21, 0.00, 2.46]\n",
            "Epoch 19, batch: 4300/5760 (34.77 s), loss: 116.90, kl: [2.04, 2.51, 0.00, 0.00, 0.00, 1.04, 0.00, 3.20, 0.00, 2.45]\n",
            "Epoch 19, batch: 4400/5760 (34.60 s), loss: 116.88, kl: [2.04, 2.51, 0.00, 0.00, 0.00, 1.05, 0.00, 3.21, 0.00, 2.45]\n",
            "Epoch 19, batch: 4500/5760 (34.51 s), loss: 116.98, kl: [2.05, 2.51, 0.00, 0.00, 0.00, 1.04, 0.00, 3.21, 0.00, 2.46]\n",
            "Epoch 19, batch: 4600/5760 (34.65 s), loss: 116.96, kl: [2.07, 2.51, 0.00, 0.00, 0.00, 1.03, 0.00, 3.20, 0.00, 2.45]\n",
            "Epoch 19, batch: 4700/5760 (34.63 s), loss: 116.90, kl: [2.06, 2.51, 0.00, 0.00, 0.00, 1.03, 0.00, 3.20, 0.00, 2.45]\n",
            "Epoch 19, batch: 4800/5760 (35.07 s), loss: 116.86, kl: [2.05, 2.51, 0.00, 0.00, 0.00, 1.04, 0.00, 3.21, 0.00, 2.45]\n",
            "Epoch 19, batch: 4900/5760 (35.11 s), loss: 116.81, kl: [2.03, 2.51, 0.00, 0.00, 0.00, 1.04, 0.00, 3.21, 0.00, 2.45]\n",
            "Epoch 19, batch: 5000/5760 (34.64 s), loss: 116.99, kl: [2.03, 2.52, 0.00, 0.00, 0.00, 1.04, 0.00, 3.21, 0.00, 2.45]\n",
            "Epoch 19, batch: 5100/5760 (35.01 s), loss: 117.14, kl: [2.04, 2.51, 0.00, 0.00, 0.00, 1.04, 0.00, 3.22, 0.00, 2.46]\n",
            "Epoch 19, batch: 5200/5760 (34.24 s), loss: 116.92, kl: [2.05, 2.52, 0.00, 0.00, 0.00, 1.04, 0.00, 3.21, 0.00, 2.46]\n",
            "Epoch 19, batch: 5300/5760 (34.47 s), loss: 116.86, kl: [2.04, 2.51, 0.00, 0.00, 0.00, 1.04, 0.00, 3.21, 0.00, 2.46]\n",
            "Epoch 19, batch: 5400/5760 (34.51 s), loss: 116.89, kl: [2.06, 2.51, 0.00, 0.00, 0.00, 1.04, 0.00, 3.21, 0.00, 2.45]\n",
            "Epoch 19, batch: 5500/5760 (34.48 s), loss: 116.82, kl: [2.05, 2.51, 0.00, 0.00, 0.00, 1.05, 0.00, 3.21, 0.00, 2.45]\n",
            "Epoch 19, batch: 5600/5760 (34.82 s), loss: 116.77, kl: [2.05, 2.52, 0.00, 0.00, 0.00, 1.05, 0.00, 3.20, 0.00, 2.46]\n",
            "Epoch 19, batch: 5700/5760 (35.15 s), loss: 116.67, kl: [2.05, 2.51, 0.00, 0.00, 0.00, 1.04, 0.00, 3.20, 0.00, 2.45]\n",
            "Epoch 20, batch: 0/5760 (21.02 s), loss: 116.84, kl: [2.04, 2.51, 0.00, 0.00, 0.00, 1.04, 0.00, 3.20, 0.00, 2.45]\n",
            "Epoch 20, batch: 100/5760 (34.96 s), loss: 116.88, kl: [2.05, 2.51, 0.00, 0.00, 0.00, 1.04, 0.00, 3.21, 0.00, 2.46]\n",
            "Epoch 20, batch: 200/5760 (34.81 s), loss: 117.10, kl: [2.05, 2.52, 0.00, 0.00, 0.00, 1.05, 0.00, 3.22, 0.00, 2.48]\n",
            "Epoch 20, batch: 300/5760 (34.62 s), loss: 116.85, kl: [2.05, 2.51, 0.00, 0.00, 0.00, 1.04, 0.00, 3.21, 0.00, 2.47]\n",
            "Epoch 20, batch: 400/5760 (34.60 s), loss: 116.84, kl: [2.06, 2.51, 0.00, 0.00, 0.00, 1.03, 0.00, 3.21, 0.00, 2.46]\n",
            "Epoch 20, batch: 500/5760 (34.64 s), loss: 116.60, kl: [2.05, 2.51, 0.00, 0.00, 0.00, 1.04, 0.00, 3.21, 0.00, 2.46]\n",
            "Epoch 20, batch: 600/5760 (34.44 s), loss: 116.91, kl: [2.04, 2.52, 0.00, 0.00, 0.00, 1.04, 0.00, 3.21, 0.00, 2.46]\n",
            "Epoch 20, batch: 700/5760 (34.78 s), loss: 116.80, kl: [2.04, 2.51, 0.00, 0.00, 0.00, 1.04, 0.00, 3.21, 0.00, 2.46]\n",
            "Epoch 20, batch: 800/5760 (34.46 s), loss: 116.83, kl: [2.04, 2.52, 0.00, 0.00, 0.00, 1.05, 0.00, 3.21, 0.00, 2.45]\n",
            "Epoch 20, batch: 900/5760 (34.81 s), loss: 116.61, kl: [2.05, 2.51, 0.00, 0.00, 0.00, 1.05, 0.00, 3.20, 0.00, 2.45]\n",
            "Epoch 20, batch: 1000/5760 (34.28 s), loss: 116.40, kl: [2.06, 2.52, 0.00, 0.00, 0.00, 1.05, 0.00, 3.20, 0.00, 2.46]\n",
            "Epoch 20, batch: 1100/5760 (34.77 s), loss: 116.35, kl: [2.05, 2.52, 0.00, 0.00, 0.00, 1.06, 0.00, 3.20, 0.00, 2.46]\n",
            "Epoch 20, batch: 1200/5760 (34.35 s), loss: 116.49, kl: [2.04, 2.51, 0.00, 0.00, 0.00, 1.06, 0.00, 3.21, 0.00, 2.46]\n",
            "Epoch 20, batch: 1300/5760 (34.36 s), loss: 116.40, kl: [2.04, 2.51, 0.00, 0.00, 0.00, 1.06, 0.00, 3.21, 0.00, 2.45]\n",
            "Epoch 20, batch: 1400/5760 (34.15 s), loss: 116.68, kl: [2.04, 2.51, 0.00, 0.00, 0.00, 1.06, 0.00, 3.21, 0.00, 2.45]\n",
            "Epoch 20, batch: 1500/5760 (34.59 s), loss: 116.70, kl: [2.05, 2.52, 0.00, 0.00, 0.00, 1.06, 0.00, 3.21, 0.00, 2.46]\n",
            "Epoch 20, batch: 1600/5760 (34.53 s), loss: 116.59, kl: [2.05, 2.51, 0.00, 0.00, 0.00, 1.06, 0.00, 3.21, 0.00, 2.45]\n",
            "Epoch 20, batch: 1700/5760 (34.71 s), loss: 116.98, kl: [2.06, 2.51, 0.00, 0.00, 0.00, 1.06, 0.00, 3.21, 0.00, 2.45]\n",
            "Epoch 20, batch: 1800/5760 (35.25 s), loss: 116.58, kl: [2.05, 2.51, 0.00, 0.00, 0.00, 1.05, 0.00, 3.20, 0.00, 2.45]\n",
            "Epoch 20, batch: 1900/5760 (34.73 s), loss: 116.57, kl: [2.06, 2.51, 0.00, 0.00, 0.00, 1.04, 0.00, 3.19, 0.00, 2.45]\n",
            "Epoch 20, batch: 2000/5760 (34.75 s), loss: 116.61, kl: [2.06, 2.51, 0.00, 0.00, 0.00, 1.06, 0.00, 3.20, 0.00, 2.46]\n",
            "Epoch 20, batch: 2100/5760 (34.75 s), loss: 116.81, kl: [2.06, 2.52, 0.00, 0.00, 0.00, 1.05, 0.00, 3.20, 0.00, 2.47]\n",
            "Epoch 20, batch: 2200/5760 (34.63 s), loss: 116.75, kl: [2.05, 2.53, 0.00, 0.00, 0.00, 1.07, 0.00, 3.21, 0.00, 2.48]\n",
            "Epoch 20, batch: 2300/5760 (34.56 s), loss: 116.76, kl: [2.05, 2.52, 0.00, 0.00, 0.00, 1.07, 0.00, 3.21, 0.00, 2.47]\n",
            "Epoch 20, batch: 2400/5760 (34.62 s), loss: 116.85, kl: [2.04, 2.51, 0.00, 0.00, 0.00, 1.07, 0.00, 3.21, 0.00, 2.46]\n",
            "Epoch 20, batch: 2500/5760 (34.62 s), loss: 116.69, kl: [2.05, 2.52, 0.00, 0.00, 0.00, 1.06, 0.00, 3.21, 0.00, 2.45]\n",
            "Epoch 20, batch: 2600/5760 (34.88 s), loss: 116.53, kl: [2.06, 2.51, 0.00, 0.00, 0.00, 1.06, 0.00, 3.21, 0.00, 2.45]\n",
            "Epoch 20, batch: 2700/5760 (35.12 s), loss: 116.84, kl: [2.06, 2.52, 0.00, 0.00, 0.00, 1.06, 0.00, 3.22, 0.00, 2.47]\n",
            "Epoch 20, batch: 2800/5760 (34.42 s), loss: 116.04, kl: [2.04, 2.51, 0.00, 0.00, 0.00, 1.05, 0.00, 3.21, 0.00, 2.47]\n",
            "Epoch 20, batch: 2900/5760 (34.82 s), loss: 116.69, kl: [2.05, 2.52, 0.00, 0.00, 0.00, 1.05, 0.00, 3.21, 0.00, 2.46]\n",
            "Epoch 20, batch: 3000/5760 (34.38 s), loss: 116.21, kl: [2.04, 2.52, 0.00, 0.00, 0.00, 1.05, 0.00, 3.21, 0.00, 2.45]\n",
            "Epoch 20, batch: 3100/5760 (34.71 s), loss: 116.30, kl: [2.03, 2.51, 0.00, 0.00, 0.00, 1.06, 0.00, 3.20, 0.00, 2.46]\n",
            "Epoch 20, batch: 3200/5760 (34.44 s), loss: 116.26, kl: [2.06, 2.50, 0.00, 0.00, 0.00, 1.06, 0.00, 3.20, 0.00, 2.45]\n",
            "Epoch 20, batch: 3300/5760 (34.66 s), loss: 116.60, kl: [2.05, 2.51, 0.00, 0.00, 0.00, 1.06, 0.00, 3.21, 0.00, 2.47]\n",
            "Epoch 20, batch: 3400/5760 (34.45 s), loss: 116.37, kl: [2.04, 2.52, 0.00, 0.00, 0.00, 1.07, 0.00, 3.21, 0.00, 2.48]\n",
            "Epoch 20, batch: 3500/5760 (34.54 s), loss: 116.36, kl: [2.05, 2.51, 0.00, 0.00, 0.00, 1.06, 0.00, 3.22, 0.00, 2.47]\n",
            "Epoch 20, batch: 3600/5760 (34.98 s), loss: 116.52, kl: [2.07, 2.51, 0.00, 0.00, 0.00, 1.07, 0.00, 3.21, 0.00, 2.46]\n",
            "Epoch 20, batch: 3700/5760 (34.42 s), loss: 116.49, kl: [2.06, 2.50, 0.00, 0.00, 0.00, 1.06, 0.00, 3.21, 0.00, 2.46]\n",
            "Epoch 20, batch: 3800/5760 (34.88 s), loss: 116.50, kl: [2.07, 2.51, 0.00, 0.00, 0.00, 1.06, 0.00, 3.21, 0.00, 2.47]\n",
            "Epoch 20, batch: 3900/5760 (34.37 s), loss: 116.39, kl: [2.06, 2.50, 0.00, 0.00, 0.00, 1.06, 0.00, 3.21, 0.00, 2.47]\n",
            "Epoch 20, batch: 4000/5760 (34.46 s), loss: 116.60, kl: [2.05, 2.51, 0.00, 0.00, 0.00, 1.06, 0.00, 3.21, 0.00, 2.48]\n",
            "Epoch 20, batch: 4100/5760 (34.96 s), loss: 116.14, kl: [2.04, 2.51, 0.00, 0.00, 0.00, 1.06, 0.00, 3.21, 0.00, 2.46]\n",
            "Epoch 20, batch: 4200/5760 (34.75 s), loss: 116.04, kl: [2.04, 2.52, 0.00, 0.00, 0.00, 1.06, 0.00, 3.21, 0.00, 2.46]\n",
            "Epoch 20, batch: 4300/5760 (34.96 s), loss: 116.20, kl: [2.04, 2.52, 0.00, 0.00, 0.00, 1.07, 0.00, 3.22, 0.00, 2.47]\n",
            "Epoch 20, batch: 4400/5760 (34.87 s), loss: 116.35, kl: [2.05, 2.52, 0.00, 0.00, 0.00, 1.07, 0.00, 3.21, 0.00, 2.46]\n",
            "Epoch 20, batch: 4500/5760 (35.21 s), loss: 116.37, kl: [2.06, 2.52, 0.00, 0.00, 0.00, 1.06, 0.00, 3.21, 0.00, 2.46]\n",
            "Epoch 20, batch: 4600/5760 (34.65 s), loss: 116.45, kl: [2.04, 2.52, 0.00, 0.00, 0.00, 1.06, 0.00, 3.20, 0.00, 2.46]\n",
            "Epoch 20, batch: 4700/5760 (34.97 s), loss: 116.06, kl: [2.05, 2.52, 0.00, 0.00, 0.00, 1.06, 0.00, 3.20, 0.00, 2.46]\n",
            "Epoch 20, batch: 4800/5760 (34.64 s), loss: 116.06, kl: [2.05, 2.52, 0.00, 0.00, 0.00, 1.06, 0.00, 3.21, 0.00, 2.46]\n",
            "Epoch 20, batch: 4900/5760 (34.65 s), loss: 116.37, kl: [2.05, 2.51, 0.00, 0.00, 0.00, 1.07, 0.00, 3.21, 0.00, 2.46]\n",
            "Epoch 20, batch: 5000/5760 (34.99 s), loss: 116.05, kl: [2.03, 2.52, 0.00, 0.00, 0.00, 1.07, 0.00, 3.20, 0.00, 2.46]\n",
            "Epoch 20, batch: 5100/5760 (35.07 s), loss: 116.07, kl: [2.05, 2.52, 0.00, 0.00, 0.00, 1.08, 0.00, 3.21, 0.00, 2.47]\n",
            "Epoch 20, batch: 5200/5760 (34.68 s), loss: 116.22, kl: [2.05, 2.51, 0.00, 0.00, 0.00, 1.08, 0.00, 3.21, 0.00, 2.47]\n",
            "Epoch 20, batch: 5300/5760 (34.48 s), loss: 116.27, kl: [2.04, 2.51, 0.00, 0.00, 0.00, 1.07, 0.00, 3.21, 0.00, 2.47]\n",
            "Epoch 20, batch: 5400/5760 (34.88 s), loss: 116.16, kl: [2.05, 2.51, 0.00, 0.00, 0.00, 1.08, 0.00, 3.21, 0.00, 2.46]\n",
            "Epoch 20, batch: 5500/5760 (34.44 s), loss: 116.01, kl: [2.06, 2.50, 0.00, 0.00, 0.00, 1.07, 0.00, 3.21, 0.00, 2.47]\n",
            "Epoch 20, batch: 5600/5760 (34.80 s), loss: 116.18, kl: [2.06, 2.51, 0.00, 0.00, 0.00, 1.08, 0.00, 3.21, 0.00, 2.48]\n",
            "Epoch 20, batch: 5700/5760 (34.37 s), loss: 116.67, kl: [2.06, 2.51, 0.00, 0.00, 0.00, 1.07, 0.00, 3.21, 0.00, 2.48]\n",
            "Epoch 21, batch: 0/5760 (20.74 s), loss: 116.34, kl: [2.05, 2.51, 0.00, 0.00, 0.00, 1.07, 0.00, 3.21, 0.00, 2.47]\n",
            "Epoch 21, batch: 100/5760 (35.20 s), loss: 116.71, kl: [2.07, 2.51, 0.00, 0.00, 0.00, 1.08, 0.00, 3.22, 0.00, 2.48]\n",
            "Epoch 21, batch: 200/5760 (34.45 s), loss: 116.13, kl: [2.05, 2.50, 0.00, 0.00, 0.00, 1.07, 0.00, 3.21, 0.00, 2.47]\n",
            "Epoch 21, batch: 300/5760 (34.61 s), loss: 116.03, kl: [2.04, 2.51, 0.00, 0.00, 0.00, 1.06, 0.00, 3.21, 0.00, 2.48]\n",
            "Epoch 21, batch: 400/5760 (34.45 s), loss: 116.08, kl: [2.04, 2.52, 0.00, 0.00, 0.00, 1.06, 0.00, 3.22, 0.00, 2.49]\n",
            "Epoch 21, batch: 500/5760 (34.88 s), loss: 116.27, kl: [2.05, 2.52, 0.00, 0.00, 0.00, 1.07, 0.00, 3.22, 0.00, 2.49]\n",
            "Epoch 21, batch: 600/5760 (34.46 s), loss: 116.16, kl: [2.05, 2.51, 0.00, 0.00, 0.00, 1.06, 0.00, 3.21, 0.00, 2.48]\n",
            "Epoch 21, batch: 700/5760 (34.73 s), loss: 116.10, kl: [2.06, 2.51, 0.00, 0.00, 0.00, 1.07, 0.00, 3.22, 0.00, 2.48]\n",
            "Epoch 21, batch: 800/5760 (34.42 s), loss: 115.85, kl: [2.05, 2.51, 0.00, 0.00, 0.00, 1.07, 0.00, 3.22, 0.00, 2.46]\n",
            "Epoch 21, batch: 900/5760 (34.46 s), loss: 115.91, kl: [2.06, 2.51, 0.00, 0.00, 0.00, 1.06, 0.00, 3.22, 0.00, 2.47]\n",
            "Epoch 21, batch: 1000/5760 (34.79 s), loss: 116.03, kl: [2.06, 2.51, 0.00, 0.00, 0.00, 1.06, 0.00, 3.21, 0.00, 2.48]\n",
            "Epoch 21, batch: 1100/5760 (35.10 s), loss: 116.37, kl: [2.06, 2.51, 0.00, 0.00, 0.00, 1.06, 0.00, 3.22, 0.00, 2.47]\n",
            "Epoch 21, batch: 1200/5760 (34.53 s), loss: 116.06, kl: [2.06, 2.51, 0.00, 0.00, 0.00, 1.07, 0.00, 3.21, 0.00, 2.47]\n",
            "Epoch 21, batch: 1300/5760 (34.51 s), loss: 116.07, kl: [2.06, 2.51, 0.00, 0.00, 0.00, 1.07, 0.00, 3.21, 0.00, 2.47]\n",
            "Epoch 21, batch: 1400/5760 (34.97 s), loss: 115.74, kl: [2.05, 2.52, 0.00, 0.00, 0.00, 1.08, 0.00, 3.21, 0.00, 2.48]\n",
            "Epoch 21, batch: 1500/5760 (34.46 s), loss: 116.06, kl: [2.06, 2.51, 0.00, 0.00, 0.00, 1.08, 0.00, 3.20, 0.00, 2.47]\n",
            "Epoch 21, batch: 1600/5760 (34.50 s), loss: 116.15, kl: [2.06, 2.51, 0.00, 0.00, 0.00, 1.07, 0.00, 3.21, 0.00, 2.47]\n",
            "Epoch 21, batch: 1700/5760 (34.62 s), loss: 116.43, kl: [2.06, 2.51, 0.00, 0.00, 0.00, 1.08, 0.00, 3.22, 0.00, 2.47]\n",
            "Epoch 21, batch: 1800/5760 (34.76 s), loss: 115.98, kl: [2.06, 2.50, 0.00, 0.00, 0.00, 1.07, 0.00, 3.22, 0.00, 2.46]\n",
            "Epoch 21, batch: 1900/5760 (34.81 s), loss: 115.96, kl: [2.06, 2.52, 0.00, 0.00, 0.00, 1.08, 0.00, 3.22, 0.00, 2.46]\n",
            "Epoch 21, batch: 2000/5760 (34.70 s), loss: 115.97, kl: [2.05, 2.52, 0.00, 0.00, 0.00, 1.08, 0.00, 3.21, 0.00, 2.47]\n",
            "Epoch 21, batch: 2100/5760 (35.02 s), loss: 115.92, kl: [2.05, 2.52, 0.00, 0.00, 0.00, 1.08, 0.00, 3.22, 0.00, 2.48]\n",
            "Epoch 21, batch: 2200/5760 (34.63 s), loss: 115.70, kl: [2.05, 2.52, 0.00, 0.00, 0.00, 1.08, 0.00, 3.21, 0.00, 2.47]\n",
            "Epoch 21, batch: 2300/5760 (35.05 s), loss: 116.09, kl: [2.05, 2.51, 0.00, 0.00, 0.00, 1.08, 0.00, 3.21, 0.00, 2.47]\n",
            "Epoch 21, batch: 2400/5760 (34.73 s), loss: 115.86, kl: [2.07, 2.51, 0.00, 0.00, 0.00, 1.08, 0.00, 3.22, 0.00, 2.48]\n",
            "Epoch 21, batch: 2500/5760 (34.99 s), loss: 115.79, kl: [2.06, 2.51, 0.00, 0.00, 0.00, 1.09, 0.00, 3.21, 0.00, 2.48]\n",
            "Epoch 21, batch: 2600/5760 (34.69 s), loss: 115.67, kl: [2.05, 2.51, 0.00, 0.00, 0.00, 1.09, 0.00, 3.21, 0.00, 2.47]\n",
            "Epoch 21, batch: 2700/5760 (34.67 s), loss: 116.13, kl: [2.06, 2.51, 0.00, 0.00, 0.00, 1.07, 0.00, 3.22, 0.00, 2.46]\n",
            "Epoch 21, batch: 2800/5760 (34.62 s), loss: 116.11, kl: [2.06, 2.51, 0.00, 0.00, 0.00, 1.07, 0.00, 3.23, 0.00, 2.47]\n",
            "Epoch 21, batch: 2900/5760 (34.61 s), loss: 115.71, kl: [2.06, 2.51, 0.00, 0.00, 0.00, 1.08, 0.00, 3.23, 0.00, 2.47]\n",
            "Epoch 21, batch: 3000/5760 (34.47 s), loss: 115.46, kl: [2.05, 2.50, 0.00, 0.00, 0.00, 1.08, 0.00, 3.22, 0.00, 2.47]\n",
            "Epoch 21, batch: 3100/5760 (34.88 s), loss: 115.73, kl: [2.05, 2.51, 0.00, 0.00, 0.00, 1.08, 0.00, 3.22, 0.00, 2.47]\n",
            "Epoch 21, batch: 3200/5760 (34.83 s), loss: 115.93, kl: [2.05, 2.51, 0.00, 0.00, 0.00, 1.08, 0.00, 3.21, 0.00, 2.48]\n",
            "Epoch 21, batch: 3300/5760 (34.71 s), loss: 115.62, kl: [2.06, 2.51, 0.00, 0.00, 0.00, 1.08, 0.00, 3.21, 0.00, 2.47]\n",
            "Epoch 21, batch: 3400/5760 (34.44 s), loss: 115.75, kl: [2.07, 2.52, 0.00, 0.00, 0.00, 1.08, 0.00, 3.21, 0.00, 2.48]\n",
            "Epoch 21, batch: 3500/5760 (34.33 s), loss: 115.78, kl: [2.06, 2.52, 0.00, 0.00, 0.00, 1.09, 0.00, 3.22, 0.00, 2.48]\n",
            "Epoch 21, batch: 3600/5760 (34.75 s), loss: 116.25, kl: [2.08, 2.52, 0.00, 0.00, 0.00, 1.10, 0.00, 3.21, 0.00, 2.48]\n",
            "Epoch 21, batch: 3700/5760 (34.65 s), loss: 116.04, kl: [2.05, 2.51, 0.00, 0.00, 0.00, 1.09, 0.00, 3.22, 0.00, 2.48]\n",
            "Epoch 21, batch: 3800/5760 (34.80 s), loss: 115.78, kl: [2.06, 2.51, 0.00, 0.00, 0.00, 1.08, 0.00, 3.21, 0.00, 2.48]\n",
            "Epoch 21, batch: 3900/5760 (34.58 s), loss: 115.87, kl: [2.05, 2.51, 0.00, 0.00, 0.00, 1.08, 0.00, 3.22, 0.00, 2.46]\n",
            "Epoch 21, batch: 4000/5760 (34.56 s), loss: 115.82, kl: [2.07, 2.51, 0.00, 0.00, 0.00, 1.07, 0.00, 3.23, 0.00, 2.47]\n",
            "Epoch 21, batch: 4100/5760 (35.53 s), loss: 116.08, kl: [2.08, 2.51, 0.00, 0.00, 0.00, 1.07, 0.00, 3.23, 0.00, 2.47]\n",
            "Epoch 21, batch: 4200/5760 (34.76 s), loss: 115.57, kl: [2.06, 2.51, 0.00, 0.00, 0.00, 1.08, 0.00, 3.21, 0.00, 2.48]\n",
            "Epoch 21, batch: 4300/5760 (34.65 s), loss: 115.84, kl: [2.06, 2.51, 0.00, 0.00, 0.00, 1.08, 0.00, 3.21, 0.00, 2.48]\n",
            "Epoch 21, batch: 4400/5760 (34.64 s), loss: 115.56, kl: [2.06, 2.51, 0.00, 0.00, 0.00, 1.10, 0.00, 3.21, 0.00, 2.48]\n",
            "Epoch 21, batch: 4500/5760 (34.81 s), loss: 115.44, kl: [2.05, 2.51, 0.00, 0.00, 0.00, 1.09, 0.00, 3.21, 0.00, 2.49]\n",
            "Epoch 21, batch: 4600/5760 (34.79 s), loss: 115.61, kl: [2.05, 2.51, 0.00, 0.00, 0.00, 1.08, 0.00, 3.21, 0.00, 2.48]\n",
            "Epoch 21, batch: 4700/5760 (34.55 s), loss: 115.64, kl: [2.05, 2.52, 0.00, 0.00, 0.00, 1.08, 0.00, 3.21, 0.00, 2.48]\n",
            "Epoch 21, batch: 4800/5760 (34.48 s), loss: 115.71, kl: [2.06, 2.52, 0.00, 0.00, 0.00, 1.09, 0.00, 3.21, 0.00, 2.49]\n",
            "Epoch 21, batch: 4900/5760 (34.46 s), loss: 115.43, kl: [2.06, 2.52, 0.00, 0.00, 0.00, 1.08, 0.00, 3.21, 0.00, 2.49]\n",
            "Epoch 21, batch: 5000/5760 (34.92 s), loss: 115.70, kl: [2.06, 2.52, 0.00, 0.00, 0.00, 1.09, 0.00, 3.21, 0.00, 2.48]\n",
            "Epoch 21, batch: 5100/5760 (35.26 s), loss: 116.11, kl: [2.05, 2.52, 0.00, 0.00, 0.00, 1.09, 0.00, 3.22, 0.00, 2.47]\n",
            "Epoch 21, batch: 5200/5760 (34.37 s), loss: 115.74, kl: [2.06, 2.51, 0.00, 0.00, 0.00, 1.08, 0.00, 3.22, 0.00, 2.47]\n",
            "Epoch 21, batch: 5300/5760 (34.52 s), loss: 115.18, kl: [2.06, 2.50, 0.00, 0.00, 0.00, 1.08, 0.00, 3.21, 0.00, 2.47]\n",
            "Epoch 21, batch: 5400/5760 (34.31 s), loss: 115.67, kl: [2.06, 2.52, 0.00, 0.00, 0.00, 1.08, 0.00, 3.21, 0.00, 2.47]\n",
            "Epoch 21, batch: 5500/5760 (34.33 s), loss: 115.47, kl: [2.07, 2.51, 0.00, 0.00, 0.00, 1.09, 0.00, 3.22, 0.00, 2.47]\n",
            "Epoch 21, batch: 5600/5760 (34.19 s), loss: 115.42, kl: [2.07, 2.52, 0.00, 0.00, 0.00, 1.09, 0.00, 3.22, 0.00, 2.47]\n",
            "Epoch 21, batch: 5700/5760 (34.35 s), loss: 115.44, kl: [2.07, 2.52, 0.00, 0.00, 0.00, 1.09, 0.00, 3.22, 0.00, 2.48]\n",
            "Epoch 22, batch: 0/5760 (20.55 s), loss: 115.57, kl: [2.07, 2.52, 0.00, 0.00, 0.00, 1.09, 0.00, 3.22, 0.00, 2.47]\n",
            "Epoch 22, batch: 100/5760 (34.40 s), loss: 115.74, kl: [2.07, 2.52, 0.00, 0.00, 0.00, 1.10, 0.00, 3.22, 0.00, 2.49]\n",
            "Epoch 22, batch: 200/5760 (34.81 s), loss: 115.31, kl: [2.07, 2.50, 0.00, 0.00, 0.00, 1.09, 0.00, 3.22, 0.00, 2.48]\n",
            "Epoch 22, batch: 300/5760 (34.53 s), loss: 115.58, kl: [2.07, 2.50, 0.00, 0.00, 0.00, 1.08, 0.00, 3.22, 0.00, 2.48]\n",
            "Epoch 22, batch: 400/5760 (34.24 s), loss: 115.58, kl: [2.07, 2.51, 0.00, 0.00, 0.00, 1.10, 0.00, 3.22, 0.00, 2.48]\n",
            "Epoch 22, batch: 500/5760 (34.24 s), loss: 115.01, kl: [2.07, 2.51, 0.00, 0.00, 0.00, 1.09, 0.00, 3.22, 0.00, 2.48]\n",
            "Epoch 22, batch: 600/5760 (34.35 s), loss: 114.88, kl: [2.07, 2.51, 0.00, 0.00, 0.00, 1.10, 0.00, 3.21, 0.00, 2.48]\n",
            "Epoch 22, batch: 700/5760 (34.30 s), loss: 115.32, kl: [2.06, 2.51, 0.00, 0.00, 0.00, 1.09, 0.00, 3.22, 0.00, 2.48]\n",
            "Epoch 22, batch: 800/5760 (34.48 s), loss: 115.51, kl: [2.06, 2.52, 0.00, 0.00, 0.00, 1.10, 0.00, 3.22, 0.00, 2.48]\n",
            "Epoch 22, batch: 900/5760 (34.39 s), loss: 115.25, kl: [2.06, 2.52, 0.00, 0.00, 0.00, 1.10, 0.00, 3.21, 0.00, 2.48]\n",
            "Epoch 22, batch: 1000/5760 (34.55 s), loss: 115.60, kl: [2.07, 2.51, 0.00, 0.00, 0.00, 1.11, 0.00, 3.23, 0.00, 2.50]\n",
            "Epoch 22, batch: 1100/5760 (36.11 s), loss: 115.82, kl: [2.06, 2.51, 0.00, 0.00, 0.00, 1.10, 0.00, 3.22, 0.00, 2.48]\n",
            "Epoch 22, batch: 1200/5760 (34.83 s), loss: 115.85, kl: [2.07, 2.52, 0.00, 0.00, 0.00, 1.10, 0.00, 3.23, 0.00, 2.48]\n",
            "Epoch 22, batch: 1300/5760 (34.79 s), loss: 115.76, kl: [2.08, 2.51, 0.00, 0.00, 0.00, 1.10, 0.00, 3.22, 0.00, 2.47]\n",
            "Epoch 22, batch: 1400/5760 (34.51 s), loss: 115.19, kl: [2.06, 2.51, 0.00, 0.00, 0.00, 1.10, 0.00, 3.22, 0.00, 2.47]\n",
            "Epoch 22, batch: 1500/5760 (34.66 s), loss: 115.12, kl: [2.06, 2.52, 0.00, 0.00, 0.00, 1.09, 0.00, 3.21, 0.00, 2.48]\n",
            "Epoch 22, batch: 1600/5760 (34.43 s), loss: 115.52, kl: [2.06, 2.52, 0.00, 0.00, 0.00, 1.09, 0.00, 3.22, 0.00, 2.48]\n",
            "Epoch 22, batch: 1700/5760 (34.48 s), loss: 115.54, kl: [2.06, 2.51, 0.00, 0.00, 0.00, 1.10, 0.00, 3.21, 0.00, 2.49]\n",
            "Epoch 22, batch: 1800/5760 (34.75 s), loss: 115.45, kl: [2.07, 2.51, 0.00, 0.00, 0.00, 1.09, 0.00, 3.22, 0.00, 2.48]\n",
            "Epoch 22, batch: 1900/5760 (34.59 s), loss: 115.18, kl: [2.07, 2.51, 0.00, 0.00, 0.00, 1.09, 0.00, 3.21, 0.00, 2.50]\n",
            "Epoch 22, batch: 2000/5760 (35.51 s), loss: 115.14, kl: [2.06, 2.51, 0.00, 0.00, 0.00, 1.09, 0.00, 3.21, 0.00, 2.49]\n",
            "Epoch 22, batch: 2100/5760 (35.29 s), loss: 115.23, kl: [2.06, 2.51, 0.00, 0.00, 0.00, 1.09, 0.00, 3.22, 0.00, 2.49]\n",
            "Epoch 22, batch: 2200/5760 (34.73 s), loss: 114.98, kl: [2.06, 2.52, 0.00, 0.00, 0.00, 1.10, 0.00, 3.22, 0.00, 2.48]\n",
            "Epoch 22, batch: 2300/5760 (34.74 s), loss: 115.33, kl: [2.07, 2.52, 0.00, 0.00, 0.00, 1.11, 0.00, 3.22, 0.00, 2.48]\n",
            "Epoch 22, batch: 2400/5760 (34.80 s), loss: 115.41, kl: [2.06, 2.51, 0.00, 0.00, 0.00, 1.11, 0.00, 3.22, 0.00, 2.49]\n",
            "Epoch 22, batch: 2500/5760 (34.87 s), loss: 115.66, kl: [2.08, 2.52, 0.00, 0.00, 0.00, 1.11, 0.00, 3.22, 0.00, 2.48]\n",
            "Epoch 22, batch: 2600/5760 (34.84 s), loss: 115.61, kl: [2.08, 2.52, 0.00, 0.00, 0.00, 1.10, 0.00, 3.23, 0.00, 2.49]\n",
            "Epoch 22, batch: 2700/5760 (34.93 s), loss: 115.56, kl: [2.08, 2.52, 0.00, 0.00, 0.00, 1.10, 0.00, 3.23, 0.00, 2.49]\n",
            "Epoch 22, batch: 2800/5760 (34.98 s), loss: 115.38, kl: [2.07, 2.51, 0.00, 0.00, 0.00, 1.09, 0.00, 3.22, 0.00, 2.49]\n",
            "Epoch 22, batch: 2900/5760 (35.37 s), loss: 115.25, kl: [2.06, 2.51, 0.00, 0.00, 0.00, 1.09, 0.00, 3.21, 0.00, 2.49]\n",
            "Epoch 22, batch: 3000/5760 (35.02 s), loss: 115.16, kl: [2.06, 2.51, 0.00, 0.00, 0.00, 1.09, 0.00, 3.22, 0.00, 2.48]\n",
            "Epoch 22, batch: 3100/5760 (35.32 s), loss: 115.11, kl: [2.06, 2.52, 0.00, 0.00, 0.00, 1.10, 0.00, 3.22, 0.00, 2.48]\n",
            "Epoch 22, batch: 3200/5760 (34.62 s), loss: 115.05, kl: [2.06, 2.52, 0.00, 0.00, 0.00, 1.09, 0.00, 3.22, 0.00, 2.48]\n",
            "Epoch 22, batch: 3300/5760 (34.51 s), loss: 115.21, kl: [2.06, 2.52, 0.00, 0.00, 0.00, 1.08, 0.00, 3.22, 0.00, 2.47]\n",
            "Epoch 22, batch: 3400/5760 (34.63 s), loss: 114.96, kl: [2.05, 2.52, 0.00, 0.00, 0.00, 1.10, 0.00, 3.22, 0.00, 2.49]\n",
            "Epoch 22, batch: 3500/5760 (34.70 s), loss: 115.44, kl: [2.06, 2.52, 0.00, 0.00, 0.00, 1.11, 0.00, 3.23, 0.00, 2.48]\n",
            "Epoch 22, batch: 3600/5760 (34.48 s), loss: 115.69, kl: [2.07, 2.52, 0.00, 0.00, 0.00, 1.11, 0.00, 3.23, 0.00, 2.47]\n",
            "Epoch 22, batch: 3700/5760 (34.85 s), loss: 115.59, kl: [2.07, 2.52, 0.00, 0.00, 0.00, 1.11, 0.00, 3.22, 0.00, 2.49]\n",
            "Epoch 22, batch: 3800/5760 (35.33 s), loss: 115.40, kl: [2.06, 2.53, 0.00, 0.00, 0.00, 1.11, 0.00, 3.22, 0.00, 2.49]\n",
            "Epoch 22, batch: 3900/5760 (34.82 s), loss: 115.47, kl: [2.06, 2.53, 0.00, 0.00, 0.00, 1.11, 0.00, 3.23, 0.00, 2.49]\n",
            "Epoch 22, batch: 4000/5760 (34.71 s), loss: 115.17, kl: [2.06, 2.53, 0.00, 0.00, 0.00, 1.11, 0.00, 3.22, 0.00, 2.49]\n",
            "Epoch 22, batch: 4100/5760 (35.27 s), loss: 115.19, kl: [2.06, 2.52, 0.00, 0.00, 0.00, 1.10, 0.00, 3.22, 0.00, 2.49]\n",
            "Epoch 22, batch: 4200/5760 (34.72 s), loss: 115.41, kl: [2.05, 2.52, 0.00, 0.00, 0.00, 1.11, 0.00, 3.22, 0.00, 2.49]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lm1GuDeUl99B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%ls results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oLQVijh8nJlD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}