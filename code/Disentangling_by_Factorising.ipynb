{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Disentangling by Factorising.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mchivuku/csb659-project/blob/master/Disentangling_by_Factorising.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "k11rLa0DCLRo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "# Disentagling by Factorizing:\n",
        "Implementation for paper - https://arxiv.org/pdf/1802.05983.pdf\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "e0iEwCGn_Y_a",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install tqdm six\n",
        "\n",
        "\n",
        "!pip install bokeh\n",
        "!pip install tensorboard\n",
        "!pip install livelossplot\n",
        "\n",
        "!pip install tensorboard\n",
        "\n",
        "!pip install tensorboardX\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uYlhi7EcCIpS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Connect to google drive"
      ]
    },
    {
      "metadata": {
        "id": "-QFrp938CHsQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "0e4e9d61-9b78-4823-b727-5695080d542a"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "SZp-FfgVCoFa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%cd drive/My\\ Drive/Masters-DS/CSCI-B659/project/examples/disentangling-fac"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lMVtHxgmCFhv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Import Libraries"
      ]
    },
    {
      "metadata": {
        "id": "srW4DDk1_0Yu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "0365dbca-9c97-4390-e5a0-51ac6251dd9d"
      },
      "cell_type": "code",
      "source": [
        "from glob import glob\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import shutil\n",
        "from torchvision import transforms\n",
        "from torchvision import models\n",
        "import torchvision\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.nn as nn\n",
        "from torch.optim import lr_scheduler\n",
        "from torch import optim\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision.utils import make_grid\n",
        "import time\n",
        "from torchvision import datasets,transforms\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "import os\n",
        "\n",
        "## Plotting library\n",
        "\n",
        "from bokeh.plotting import figure\n",
        "from bokeh.io import show\n",
        "from bokeh.models import LinearAxis, Range1d\n",
        "from livelossplot import PlotLosses\n",
        "\n",
        "\n",
        "plt.style.use('tableau-colorblind10')\n",
        "\n",
        "print('Torch', torch.__version__, 'CUDA', torch.version.cuda)\n",
        "print('Device:', torch.device('cuda:0'))\n",
        "print(torch.cuda.is_available())\n",
        "\n",
        "is_cuda = torch.cuda.is_available()\n",
        "device = torch.device ( \"cuda:0\" if torch.cuda.is_available () else \"cpu\" )"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Torch 1.0.1.post2 CUDA 10.0.130\n",
            "Device: cuda:0\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3L0Rnc7fDko3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "torch.backends.cudnn.enabled = True\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "init_seed = 1\n",
        "torch.manual_seed(init_seed)\n",
        "torch.cuda.manual_seed(init_seed)\n",
        "np.random.seed(init_seed)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "52nnGBhTEBmY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Class Params"
      ]
    },
    {
      "metadata": {
        "id": "ahTBHVGVD6EQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "class Params:\n",
        "  max_iter=1e6\n",
        "  batch_size = 64\n",
        "  z_dim = 20\n",
        "  gamma = 6.4\n",
        "  lr_vae = 1e-4\n",
        "  beta1_vae = 0.9\n",
        "  beta2_vae = 0.999\n",
        "  lr_D = 1e-4\n",
        "  beta1_D = 0.5\n",
        "  beta2_D  = 0.9\n",
        "  image_size = 784\n",
        "  viz_on = True\n",
        "  print_iter = 500\n",
        "  num_workers = 2\n",
        "  output_dir = \"outputs\"\n",
        "  ckpt_save_iter = 100\n",
        "  \n",
        "  \n",
        "  \n",
        "def mkdirs(path):\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "\n",
        "mkdirs(\"logs\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XrxEAwOgE_sU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "outputId": "9622a20e-e48b-4152-c1bf-bfbad8b0038c"
      },
      "cell_type": "code",
      "source": [
        "LOG_DIR = './logs/'\n",
        "get_ipython().system_raw(\n",
        "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
        "    .format(LOG_DIR)\n",
        ")\n",
        "\n",
        "\n",
        "! wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "! unzip ngrok-stable-linux-amd64.zip\n",
        "get_ipython().system_raw('./ngrok http 6006 &')\n",
        "! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-03-25 14:51:36--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 52.203.66.95, 52.200.123.104, 52.204.188.97, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|52.203.66.95|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13584026 (13M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip.1’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]  12.95M  12.7MB/s    in 1.0s    \n",
            "\n",
            "2019-03-25 14:51:37 (12.7 MB/s) - ‘ngrok-stable-linux-amd64.zip.1’ saved [13584026/13584026]\n",
            "\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "replace ngrok? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: ngrok                   \n",
            "https://adbd0abf.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "72YGnNg1FLRx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from tensorboardX import SummaryWriter\n",
        "writer = SummaryWriter()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JaJ0sjVyI7zb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Models"
      ]
    },
    {
      "metadata": {
        "id": "JWpzt5nyIsci",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.init as init"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GODGx2EgJAjU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self,z_dim):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.z_dim= z_dim\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(z_dim, 1000),\n",
        "            nn.LeakyReLU(0.2, True),\n",
        "            nn.Linear(1000, 1000),\n",
        "            nn.LeakyReLU(0.2, True),\n",
        "            nn.Linear(1000, 1000),\n",
        "            nn.LeakyReLU(0.2, True),\n",
        "            nn.Linear(1000, 1000),\n",
        "            nn.LeakyReLU(0.2, True),\n",
        "            nn.Linear(1000, 1000),\n",
        "            nn.LeakyReLU(0.2, True),\n",
        "            nn.Linear(1000, 2),\n",
        "        )\n",
        "        \n",
        "        self.weight_init()\n",
        "\n",
        "\n",
        "    def weight_init(self, mode='normal'):\n",
        "        if mode == 'kaiming':\n",
        "            initializer = kaiming_init\n",
        "        elif mode == 'normal':\n",
        "            initializer = normal_init\n",
        "\n",
        "        for block in self._modules:\n",
        "            for m in self._modules[block]:\n",
        "                initializer(m)\n",
        "\n",
        "    def forward(self, z):\n",
        "        return self.net(z).squeeze()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "J12XBm1AJG0d",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## FactorVAE, Encoder and Decoder Architecture\n",
        "class FactorVAE(nn.Module):\n",
        "  \"\"\"\n",
        "  Encoder and Decoder architecture\n",
        "  \"\"\"\n",
        "  def __init__(self,z_dim = 20):\n",
        "    super(FactorVAE,self).__init__()\n",
        "    self.z_dim= z_dim\n",
        "    \n",
        "    self.mu_ = nn.Sequential(\n",
        "        #28x28->12x12\n",
        "        nn.Conv2d(1,8,5,2,0,bias=False),\n",
        "        nn.BatchNorm2d(8),\n",
        "        nn.ReLU(True),\n",
        "        #12x12->4x4\n",
        "        nn.Conv2d(8,64,5,2,0,bias=False),\n",
        "        nn.BatchNorm2d(64),\n",
        "        nn.ReLU(True),\n",
        "        #4x4->1x1: 20,1,1\n",
        "        nn.Conv2d(64,20,4,1,0,bias=False),\n",
        "        nn.ReLU(True)\n",
        "        )\n",
        "\n",
        "\n",
        "    self.logsigma_ = nn.Sequential(\n",
        "        #28x28->12x12\n",
        "        nn.Conv2d(1,8,5,2,0,bias=False),\n",
        "        nn.BatchNorm2d(8),\n",
        "        nn.ReLU(True),\n",
        "        #12x12->4x4\n",
        "        nn.Conv2d(8,64,5,2,0,bias=False),\n",
        "        nn.BatchNorm2d(64),\n",
        "        nn.ReLU(True),\n",
        "        #4x4->1x1: 20,1,1\n",
        "        nn.Conv2d(64,20,4,1,0,bias=False),\n",
        "        nn.ReLU(True)\n",
        "        )\n",
        "\n",
        "\n",
        "    self.decode = nn.Sequential(\n",
        "        #1x1->4x4\n",
        "        nn.ConvTranspose2d(20,20*8,4,1,0,bias=False),  #(ic,oc,kernel,stride,padding)\n",
        "        nn.BatchNorm2d(20*8), \n",
        "        nn.ReLU(True),\n",
        "        nn.ConvTranspose2d(20*8,20*16,4,2,1,bias=False), #4x4->8x8\n",
        "        nn.BatchNorm2d(20*16),\n",
        "        nn.ReLU(True),\n",
        "        nn.ConvTranspose2d(20*16,20*32,4,2,1,bias=False), #8x8->16x16\n",
        "        nn.BatchNorm2d(20*32),\n",
        "        nn.ReLU(True),\n",
        "        nn.ConvTranspose2d(20*32,1,2,2,2,bias=False), #16x16->28x28\n",
        "        nn.Sigmoid()\n",
        "        )\n",
        "      \n",
        "    self.weight_init()\n",
        "    \n",
        "    \n",
        "  def weight_init(self, mode='normal'):\n",
        "        if mode == 'kaiming':\n",
        "            initializer = kaiming_init\n",
        "        elif mode == 'normal':\n",
        "            initializer = normal_init\n",
        "\n",
        "        for block in self._modules:\n",
        "            for m in self._modules[block]:\n",
        "                initializer(m)\n",
        "    \n",
        "  def reparametrize(self, mu, logvar):\n",
        "        std = logvar.mul(0.5).exp_()\n",
        "        eps = std.data.new(std.size()).normal_()\n",
        "        return eps.mul(std).add_(mu)\n",
        "      \n",
        "      \n",
        "  def encode_new(self,x):\n",
        "        return self.mu_(x), self.logsigma_(x)\n",
        "\n",
        "      \n",
        "  def forward(self, x, no_dec=False):\n",
        "        \n",
        "        mu, logvar = self.encode_new(x)\n",
        "        \n",
        "        z = self.reparametrize(mu, logvar)\n",
        "\n",
        "        if no_dec:\n",
        "            return z.squeeze()\n",
        "        else:\n",
        "            x_recon = self.decode(z).view(x.size())\n",
        "            return x_recon, mu, logvar, z.squeeze()\n",
        "          \n",
        "          \n",
        "          \n",
        "          \n",
        "          \n",
        "## Weight initializations:\n",
        "def kaiming_init(m):\n",
        "    if isinstance(m, (nn.Linear, nn.Conv2d)):\n",
        "        init.kaiming_normal_(m.weight)\n",
        "        if m.bias is not None:\n",
        "            m.bias.data.fill_(0)\n",
        "    elif isinstance(m, (nn.BatchNorm1d, nn.BatchNorm2d)):\n",
        "        m.weight.data.fill_(1)\n",
        "        if m.bias is not None:\n",
        "            m.bias.data.fill_(0)\n",
        "\n",
        "\n",
        "def normal_init(m):\n",
        "    if isinstance(m, (nn.Linear, nn.Conv2d)):\n",
        "        init.normal_(m.weight, 0, 0.02)\n",
        "        if m.bias is not None:\n",
        "            m.bias.data.fill_(0)\n",
        "    elif isinstance(m, (nn.BatchNorm1d, nn.BatchNorm2d)):\n",
        "        m.weight.data.fill_(1)\n",
        "        if m.bias is not None:\n",
        "            m.bias.data.fill_(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RjEDbnCrLS9l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 904
        },
        "outputId": "d22c3802-4e85-4ecf-c33d-0661cd308de6"
      },
      "cell_type": "code",
      "source": [
        "## initialize the model\n",
        "VAE = FactorVAE(Params.z_dim).to(device)\n",
        "D = Discriminator(Params.z_dim).to(device)\n",
        "print(VAE)\n",
        "print()\n",
        "print(D)"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FactorVAE(\n",
            "  (mu_): Sequential(\n",
            "    (0): Conv2d(1, 8, kernel_size=(5, 5), stride=(2, 2), bias=False)\n",
            "    (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace)\n",
            "    (3): Conv2d(8, 64, kernel_size=(5, 5), stride=(2, 2), bias=False)\n",
            "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): ReLU(inplace)\n",
            "    (6): Conv2d(64, 20, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
            "    (7): ReLU(inplace)\n",
            "  )\n",
            "  (logsigma_): Sequential(\n",
            "    (0): Conv2d(1, 8, kernel_size=(5, 5), stride=(2, 2), bias=False)\n",
            "    (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace)\n",
            "    (3): Conv2d(8, 64, kernel_size=(5, 5), stride=(2, 2), bias=False)\n",
            "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): ReLU(inplace)\n",
            "    (6): Conv2d(64, 20, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
            "    (7): ReLU(inplace)\n",
            "  )\n",
            "  (decode): Sequential(\n",
            "    (0): ConvTranspose2d(20, 160, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
            "    (1): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace)\n",
            "    (3): ConvTranspose2d(160, 320, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (4): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (5): ReLU(inplace)\n",
            "    (6): ConvTranspose2d(320, 640, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "    (7): BatchNorm2d(640, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (8): ReLU(inplace)\n",
            "    (9): ConvTranspose2d(640, 1, kernel_size=(2, 2), stride=(2, 2), padding=(2, 2), bias=False)\n",
            "    (10): Sigmoid()\n",
            "  )\n",
            ")\n",
            "\n",
            "Discriminator(\n",
            "  (net): Sequential(\n",
            "    (0): Linear(in_features=20, out_features=1000, bias=True)\n",
            "    (1): LeakyReLU(negative_slope=0.2, inplace)\n",
            "    (2): Linear(in_features=1000, out_features=1000, bias=True)\n",
            "    (3): LeakyReLU(negative_slope=0.2, inplace)\n",
            "    (4): Linear(in_features=1000, out_features=1000, bias=True)\n",
            "    (5): LeakyReLU(negative_slope=0.2, inplace)\n",
            "    (6): Linear(in_features=1000, out_features=1000, bias=True)\n",
            "    (7): LeakyReLU(negative_slope=0.2, inplace)\n",
            "    (8): Linear(in_features=1000, out_features=1000, bias=True)\n",
            "    (9): LeakyReLU(negative_slope=0.2, inplace)\n",
            "    (10): Linear(in_features=1000, out_features=2, bias=True)\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qtdU0IxJL8Uk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "optim_VAE = optim.Adam(VAE.parameters(), lr=Params.lr_vae,betas=(Params.beta1_vae, Params.beta2_vae))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1KC7TaKzMPgM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "optim_D = optim.Adam(D.parameters(), lr=Params.lr_D,\n",
        "                                  betas=(Params.beta1_D, Params.beta2_D))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9uJLKtq9Mogj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "nets = [VAE, D]\n",
        "\n",
        "## Set model to train\n",
        "\n",
        "def net_mode(nets, train):\n",
        "   if not isinstance(train, bool):\n",
        "      raise ValueError(\"Only bool type allowed\")\n",
        "   for net in nets:\n",
        "    if train:\n",
        "      net.train()\n",
        "    else:\n",
        "      net.eval()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Yd-t_jMAMwkh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Train"
      ]
    },
    {
      "metadata": {
        "id": "oHX2b7c8MrLj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KXMMtPY4Nbzn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "1294c536-1a52-422d-eaa7-0693254dbd2d"
      },
      "cell_type": "code",
      "source": [
        "## Save and load checkpoints\n",
        "from tqdm import tqdm\n",
        "\n",
        "ckpt_dir = \"./logs\"\n",
        "pbar = tqdm(total=Params.max_iter)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Save checkpoint - model params, optimizer params, states etc.\n",
        "\"\"\"\n",
        "def save_checkpoint(global_iter,ckptname='last', verbose=True):\n",
        "        model_states = {'D': D.state_dict(),\n",
        "                        'VAE': VAE.state_dict()}\n",
        "    \n",
        "        optim_states = {'optim_D':optim_D.state_dict(),\n",
        "                        'optim_VAE':optim_VAE.state_dict()}\n",
        "      \n",
        "        states = {'iter':global_iter,\n",
        "                  'model_states':model_states,\n",
        "                  'optim_states':optim_states}\n",
        "\n",
        "        filepath = os.path.join(ckpt_dir, str(ckptname))\n",
        "        with open(filepath, 'wb+') as f:\n",
        "            torch.save(states, f)\n",
        "        if verbose:\n",
        "            pbar.write(\"=> saved checkpoint '{}' (iter {})\".format(filepath, global_iter))\n",
        "\n",
        "\"\"\"\n",
        "Load checkoint\n",
        "\"\"\"           \n",
        "def load_checkpoint(global_iter,ckptname='last', verbose=True):\n",
        "    if ckptname == 'last':\n",
        "        ckpts = os.listdir(ckpt_dir)\n",
        "        if not ckpts:\n",
        "            if verbose:\n",
        "                pbar.write(\"=> no checkpoint found\")\n",
        "            return\n",
        "\n",
        "        ckpts = [int(ckpt) for ckpt in ckpts]\n",
        "        ckpts.sort(reverse=True)\n",
        "        ckptname = str(ckpts[0])\n",
        "\n",
        "    filepath = os.path.join(ckpt_dir, ckptname)\n",
        "    if os.path.isfile(filepath):\n",
        "        with open(filepath, 'rb') as f:\n",
        "            checkpoint = torch.load(f)\n",
        "\n",
        "        global_iter = checkpoint['iter']\n",
        "        VAE.load_state_dict(checkpoint['model_states']['VAE'])\n",
        "        D.load_state_dict(checkpoint['model_states']['D'])\n",
        "        optim_VAE.load_state_dict(checkpoint['optim_states']['optim_VAE'])\n",
        "        optim_D.load_state_dict(checkpoint['optim_states']['optim_D'])\n",
        "        pbar.update(global_iter)\n",
        "        \n",
        "        \n",
        "        if verbose:\n",
        "            pbar.write(\"=> loaded checkpoint '{} (iter {})'\".format(filepath, global_iter))\n",
        "    else:\n",
        "        if verbose:\n",
        "            pbar.write(\"=> no checkpoint found at '{}'\".format(filepath))"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "  0%|          | 0/1000000.0 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "EdBGjJKYO0Aq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Dataset Loader"
      ]
    },
    {
      "metadata": {
        "id": "rUbhO4znOhgA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision import transforms\n",
        "import random\n",
        "class CustomTensorDataset(Dataset):\n",
        "    def __init__(self, data_tensor, transform=None):\n",
        "        self.data_tensor = data_tensor\n",
        "        self.transform = transform\n",
        "        self.indices = range(len(self))\n",
        "\n",
        "    def __getitem__(self, index1):\n",
        "        index2 = random.choice(self.indices)\n",
        "\n",
        "        img1 = self.data_tensor[index1]\n",
        "        img2 = self.data_tensor[index2]\n",
        "        if self.transform is not None:\n",
        "            img1 = self.transform(img1)\n",
        "            img2 = self.transform(img2)\n",
        "\n",
        "        return img1, img2\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data_tensor.size(0)\n",
        "\n",
        "class CustomImageFolder(ImageFolder):\n",
        "    def __init__(self, root, transform=None):\n",
        "        super(CustomImageFolder, self).__init__(root, transform)\n",
        "        self.indices = range(len(self))\n",
        "\n",
        "    def __getitem__(self, index1):\n",
        "        index2 = random.choice(self.indices)\n",
        "\n",
        "        path1 = self.imgs[index1][0]\n",
        "        path2 = self.imgs[index2][0]\n",
        "        img1 = self.loader(path1)\n",
        "        img2 = self.loader(path2)\n",
        "        if self.transform is not None:\n",
        "            img1 = self.transform(img1)\n",
        "            img2 = self.transform(img2)\n",
        "\n",
        "        return img1, img2\n",
        "      \n",
        "dset_dir = \"../vae/MNIST/data/MNIST/processed/training.pt\"\n",
        "data, targets = torch.load(dset_dir)\n",
        "train_kwargs = {'data_tensor':data}\n",
        "dset = CustomTensorDataset\n",
        "\n",
        "\n",
        "train_data = dset(**train_kwargs)\n",
        "train_loader = DataLoader(train_data,\n",
        "          batch_size=Params.batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=Params.num_workers,\n",
        "        pin_memory=True,\n",
        " drop_last=True)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jDunGVXmPFCr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Losses\n",
        "\n",
        "1. reconstruction loss - binary cross entropy\n",
        "2. KL divergence \n",
        "3. permute dimz\n"
      ]
    },
    {
      "metadata": {
        "id": "aCt2xjt8O79i",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def recon_loss(x, x_recon):\n",
        "    n = x.size(0)\n",
        "    loss = F.binary_cross_entropy_with_logits(x_recon, x, size_average=False).div(n)\n",
        "    return loss\n",
        "\n",
        "\n",
        "def kl_divergence(mu, logvar):\n",
        "    kld = -0.5*(1+logvar-mu**2-logvar.exp()).sum(1).mean()\n",
        "    return kld\n",
        "\n",
        "\n",
        "def permute_dims(z):\n",
        "    assert z.dim() == 2\n",
        "\n",
        "    B, _ = z.size()\n",
        "    perm_z = []\n",
        "    for z_j in z.split(1, 1):\n",
        "        perm = torch.randperm(B).to(z.device)\n",
        "        perm_z_j = z_j[perm]\n",
        "        perm_z.append(perm_z_j)\n",
        "\n",
        "    return torch.cat(perm_z, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vno_79XqPP0B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "net_mode(nets,True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "m2JT-a1hPaFf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## real outputs\n",
        "ones = torch.ones(Params.batch_size, dtype=torch.long, device=device)\n",
        "# fake outputs\n",
        "zeros = torch.zeros(Params.batch_size, dtype=torch.long, device=device)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YprRTGfIPhU1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "out = False\n",
        "global_iter = 0\n",
        "while not out:\n",
        "  for x_true1, x_true2 in train_loader:\n",
        "    global_iter +=1\n",
        "    pbar.update(1)\n",
        "    \n",
        "    \n",
        "    x_true1 = x_true1.to(device)\n",
        "    \n",
        "    x_recon, mu,logvar, z = VAE(x_true1)\n",
        "    vae_recon_loss  = recon_loss(x_true1, x_recon)\n",
        "    vae_kld  = kl_divergence(mu,logvar)\n",
        "    \n",
        "    ## discriminator \n",
        "    D_z = D(z)\n",
        "    # total correlation\n",
        "    vae_tc_loss = (D_z[:,:1] - D_z[:,1:]).mean()\n",
        "    \n",
        "    vae_loss = vae_recon_loss + vae_kld +\\\n",
        "    Params.gamma*vae_tc_loss\n",
        "    \n",
        "    optim_VAE.zero_grad()\n",
        "    vae_loss.backward(retain_graph= True)\n",
        "    \n",
        "    optim_VAE.step()\n",
        "    \n",
        "    ## Labels\n",
        "    x_true2 = x_true2.to(device)\n",
        "    z_prime = VAE(x_true2, no_dec=True)\n",
        "    \n",
        "    z_pperm = permute_dims(z_prime).detach()\n",
        "    \n",
        "    D_z_pperm = D(z_pperm)\n",
        "    \n",
        "    D_tc_loss = 0.5*(F.cross_entropy(D_z, zeros) + F.cross_entropy(D_z_pperm, ones))\n",
        "    \n",
        "    optim_D.zero_grad()\n",
        "    \n",
        "    D_tc_loss.backward()\n",
        "    \n",
        "    optim_D.step()\n",
        "    \n",
        "    if global_iter%Param.print_iter == 0:\n",
        "      pbar.write('[{}] vae_recon_loss:{:.3f} vae_kld:{:.3f} vae_tc_loss:{:.3f} D_tc_loss:{:.3f}'.format(global_iter, vae_recon_loss.item(), vae_kld.item(), vae_tc_loss.item(), D_tc_loss.item()))\n",
        "      \n",
        "      writer.add_scalars('data/scalar_group', {'vae_loss': vae_loss.item(),\n",
        "                                              'D_tc_loss': D_tc_loss.item()}, global_iter)\n",
        "      \n",
        "      x = utils.make_grid(F.sigmoid(x_recon).data.cpu(), \n",
        "                           normalize=True, scale_each=True)\n",
        "      \n",
        "      writer.add_image('Image', x, global_iter)\n",
        "      \n",
        "      \n",
        "    if  global_iter >= Params.max_iter:\n",
        "      out = True\n",
        "      break\n",
        "          \n",
        "          \n",
        "\n",
        "          \n",
        "        \n",
        "      \n",
        "    if global_iter%ckpt_save_iter == 0:\n",
        "      save_checkpoint(global_iter)\n",
        "      \n",
        "      \n",
        "    \n",
        "      \n",
        "      \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xXFCFW4qSeTW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}