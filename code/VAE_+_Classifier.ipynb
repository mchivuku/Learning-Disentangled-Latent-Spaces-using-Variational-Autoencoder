{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VAE + Classifier.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mchivuku/csb659-project/blob/master/VAE_%2B_Classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "NTSMmyaDT5F4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install tqdm six"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pkb7Xqt0cAbC",
        "colab_type": "code",
        "outputId": "43c79987-9090-4eed-f0ab-cb761b4e6b79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "\n",
        "%cd /content/drive/My\\ Drive/Masters-DS/CSCI-B659/project/examples/vae/\n",
        "%ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n",
            "/content/drive/My Drive/Masters-DS/CSCI-B659/project/examples/vae\n",
            "\u001b[0m\u001b[01;34mMNIST\u001b[0m/  \u001b[01;34mresults\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ovIkb7hJbtnS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Classification Accuracy of VAE\n",
        "\n",
        "adopted - https://github.com/wohlert/semi-supervised-pytorch/blob/master/examples/notebooks/Deep%20Generative%20Model.ipynb\n",
        "\n",
        "https://lirnli.wordpress.com/2017/09/14/latent-layers-beyond-the-variational-autoencoder-vae/\n",
        "\n",
        "http://cs231n.stanford.edu/reports/2017/pdfs/3.pdf"
      ]
    },
    {
      "metadata": {
        "id": "w61zgwWJbwrK",
        "colab_type": "code",
        "outputId": "fac4326e-dbef-4edc-9c0e-74ad3fbd6262",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "import torch\n",
        "import torch.utils.data\n",
        "from torch import nn, optim\n",
        "from torch.nn import functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "\n",
        "params = {\n",
        "    \"batch_size\":128,\n",
        "    \"epochs\" : 10,\n",
        "    \"log_interval\":10\n",
        "    \n",
        "}\n",
        "\n",
        "torch.manual_seed(5)\n",
        "\n",
        "print('Torch', torch.__version__, 'CUDA', torch.version.cuda)\n",
        "print('Device:', torch.device('cuda:0'))\n",
        "print(torch.cuda.is_available())\n",
        "\n",
        "is_cuda = torch.cuda.is_available()\n",
        "device = torch.device ( \"cuda:0\" if torch.cuda.is_available () else \"cpu\" )\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Torch 1.0.1.post2 CUDA 10.0.130\n",
            "Device: cuda:0\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "L1pS8KL-chlZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class RunningAverage ():\n",
        "    \"\"\"A simple class that maintains the running average of a quantity\n",
        "    Example:\n",
        "    ```\n",
        "    loss_avg = RunningAverage()\n",
        "    loss_avg.update(2)\n",
        "    loss_avg.update(4)\n",
        "    loss_avg() = 3\n",
        "    ```\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__( self ):\n",
        "        self.steps = 0\n",
        "        self.total = 0\n",
        "\n",
        "    def update( self, val ):\n",
        "        self.total += val\n",
        "        self.steps += 1\n",
        "    \n",
        "    def reset(self):\n",
        "        self.steps = 0.\n",
        "        self.total = 0.\n",
        "        \n",
        "        \n",
        "        \n",
        "    def __call__( self ):\n",
        "        return self.total / float ( self.steps )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PnmqxKJpeNQ-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Data Loaders\n",
        "kwargs = {'num_workers': 1, 'pin_memory': True}\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('MNIST/data', train=True, download=True,\n",
        "                   transform=transforms.ToTensor()),\n",
        "    batch_size=params.get(\"batch_size\"), shuffle=True, **kwargs)\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('MNIST/data', train=False, transform=transforms.ToTensor()),\n",
        "    batch_size=params.get(\"batch_size\"), shuffle=True, **kwargs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Elui0JjVcJ0I",
        "colab_type": "code",
        "outputId": "67ba5f8e-7cb4-44f5-c41b-f27f72c013c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "cell_type": "code",
      "source": [
        "from torch.nn import init\n",
        "\n",
        "\"\"\"\n",
        "Flatten input\n",
        "\"\"\"\n",
        "def num_flat_features(x):\n",
        "  size = x.size()[1:] # all dimensions except the batch dimension\n",
        "  num_features = 1\n",
        "  for s in size:\n",
        "    num_features *=s\n",
        "  return num_features\n",
        "  \n",
        "## Classifier\n",
        "class Classifier(nn.Module):\n",
        "  def __init__(self,dims):\n",
        "    \"\"\"\n",
        "    Single hidden layer with softmax\n",
        "    \"\"\"\n",
        "    super(Classifier,self).__init__()\n",
        "    [x_dim, h_dim, y_dim] = dims\n",
        "    \n",
        "    self.fc1 = nn.Linear(x_dim, h_dim)\n",
        "    self.fc2 = nn.Linear(h_dim, y_dim)\n",
        "    \n",
        "  def forward(self,x):\n",
        "    ## flatten x input\n",
        "    x = x.view(-1,num_flat_features(x))\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.softmax(self.fc2(x),dim=-1)\n",
        "    \n",
        "    return x\n",
        "  \n",
        "  \n",
        "  \n",
        "  \n",
        "\"\"\"\n",
        "Gaussian Sample Layer\n",
        "\"\"\"\n",
        "class GaussianSample(nn.Module):\n",
        "  def __init__(self,in_features, out_features):\n",
        "    super(GaussianSample, self).__init__()\n",
        "    self.in_features = in_features\n",
        "    self.out_features = out_features\n",
        "    \n",
        "    self.mu = nn.Linear(in_features, out_features)\n",
        "    self.log_var = nn.Linear(in_features, out_features)\n",
        "    \n",
        "  def forward(self,x):\n",
        "    mu = self.mu(x)\n",
        "    log_var = F.softplus(self.log_var(x))\n",
        "    \n",
        "    return self.reparametrize(mu,log_var), mu, log_var\n",
        "    \n",
        "  def reparametrize(self, mu, log_var):\n",
        "    epsilon = Variable(torch.randn(mu.size()), requires_grad=False)\n",
        "\n",
        "    if mu.is_cuda:\n",
        "      epsilon = epsilon.cuda()\n",
        "    # log_std = 0.5 * log_var\n",
        "    # std = exp(log_std)\n",
        "    std = log_var.mul(0.5).exp_()\n",
        "\n",
        "    # z = std * epsilon + mu\n",
        "    z = mu.addcmul(std, epsilon)\n",
        "    return z\n",
        "      \n",
        "\n",
        "\"\"\"\n",
        "VAE Encoder\n",
        "\"\"\"\n",
        "class Encoder(nn.Module):\n",
        "  def __init__(self,dims):\n",
        "    super(Encoder,self).__init__()\n",
        "    [x_dim, h_dim, z_dim] = dims\n",
        "    ##linear layers\n",
        "    \n",
        "    neurons = [x_dim, *h_dim]\n",
        "    print(\"Neurons\",neurons)\n",
        "    linear_layers = [nn.Linear(neurons[i-1],neurons[i]) for i in range(1,len(neurons))]\n",
        "    \n",
        "    self.hidden=nn.ModuleList(linear_layers)\n",
        "    \n",
        "    self.sample = GaussianSample(h_dim[-1],z_dim)\n",
        "    \n",
        "    \n",
        "  def forward(self,x):\n",
        "    \n",
        "    x = x.view(-1,784)\n",
        "    \n",
        "    for layer in self.hidden:\n",
        "      x = F.relu(layer(x))\n",
        "      \n",
        "    \n",
        "    return self.sample(x)\n",
        "  \n",
        "\"\"\"\n",
        "VAE Decoder\n",
        "\"\"\"\n",
        "class Decoder(nn.Module):\n",
        "  def __init__(self,dims):\n",
        "    super(Decoder,self).__init__()\n",
        "    [z_dim, h_dim, x_dim] = dims\n",
        "    ##linear layers\n",
        "    neurons = [z_dim, *h_dim]\n",
        "    \n",
        "    linear_layers = [nn.Linear(neurons[i-1],neurons[i]) for i in range(1,len(neurons))]\n",
        "    \n",
        "    self.hidden=nn.ModuleList(linear_layers)\n",
        "    \n",
        "    self.reconstruction = nn.Linear(h_dim[-1],x_dim)\n",
        "    self.output_activation = torch.sigmoid\n",
        "    \n",
        "    \n",
        "  def forward(self,x):\n",
        "    #x = x.view(-1,num_flat_features(x))\n",
        "    for layer in self.hidden:\n",
        "      x = F.relu(layer(x))\n",
        "    return self.output_activation(self.reconstruction(x))\n",
        "  \n",
        "\n",
        "\n",
        "\"\"\"\n",
        "VAE object\n",
        "\"\"\"\n",
        "class VAE(nn.Module):\n",
        "  def __init__(self,dims):\n",
        "    super(VAE,self).__init__()\n",
        "    \n",
        "    [x_dim,z_dim,h_dim] = dims\n",
        "    self.z_dim = z_dim\n",
        "    self.h_dim = h_dim\n",
        "    self.x_dim = x_dim\n",
        "   \n",
        "    ## xaview initialization for weights\n",
        "    for m in self.modules():\n",
        "      if isinstance(m, nn.Linear):\n",
        "        init.xavier_normal_(m.weight.data)\n",
        "        if m.bias is not None:\n",
        "          m.bias.data.zero_()\n",
        "    \n",
        " \n",
        "  \"\"\"\n",
        "  Forward\n",
        "  \"\"\"\n",
        "  def forward(self,x):\n",
        "    z, z_mu, z_log_var = self.encoder(x)\n",
        "    return self.decoder(z), z_mu, z_log_var\n",
        "  \n",
        "  \n",
        "  \n",
        "\"\"\"\n",
        "Deep Generative Model \n",
        "\"\"\"\n",
        "class DeepGenerativeModel(VAE):\n",
        "    def __init__(self, dims):\n",
        "        \"\"\"\n",
        "        M2 code replication from the paper\n",
        "        'Semi-Supervised Learning with Deep Generative Models'\n",
        "        (Kingma 2014) in PyTorch.\n",
        "        The \"Generative semi-supervised model\" is a probabilistic\n",
        "        model that incorporates label information in both\n",
        "        inference and generation.\n",
        "        Initialise a new generative model\n",
        "        :param dims: dimensions of x, y, z and hidden layers.\n",
        "        \"\"\"\n",
        "        [x_dim, self.y_dim, z_dim, h_dim] = dims\n",
        "        super(DeepGenerativeModel, self).__init__([784, z_dim, h_dim])\n",
        "        \n",
        "        \n",
        "\n",
        "        self.encoder = Encoder([x_dim, h_dim, z_dim])\n",
        "\n",
        "        self.decoder = Decoder([z_dim, list(reversed(h_dim)), x_dim])\n",
        "        self.classifier = Classifier([x_dim, h_dim[0], self.y_dim])\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "              init.xavier_normal_(m.weight.data)\n",
        "              if m.bias is not None:\n",
        "                m.bias.data.zero_()\n",
        "\n",
        "    \"\"\"\n",
        "    Compute KLD on gaussian sample - inference\n",
        "    \"\"\"\n",
        "    def _kld(self,x,q_param):\n",
        "      (mu, log_var) = q_param\n",
        "      log_pdf = - 0.5 * math.log(2 * math.pi) - log_var / 2 - (x - mu)**2 / (2 * torch.exp(log_var))\n",
        "      return torch.sum(log_pdf, dim=-1)\n",
        "      \n",
        "    def forward(self, x, y):\n",
        "        # Add label and data and generate latent variable\n",
        "        \n",
        "        z, z_mu, z_log_var = self.encoder(x)\n",
        "\n",
        "        self.kl_divergence = self._kld(z, (z_mu, z_log_var))\n",
        "\n",
        "        # Reconstruct data point from latent data and label\n",
        "        x_mu = self.decoder(z)\n",
        "\n",
        "        return x_mu\n",
        "\n",
        "    def classify(self, x):\n",
        "        logits = self.classifier(x)\n",
        "        return logits\n",
        "\n",
        "    def sample(self, z, y):\n",
        "        \"\"\"\n",
        "        Samples from the Decoder to generate an x.\n",
        "        :param z: latent normal variable\n",
        "        :param y: label (one-hot encoded)\n",
        "        :return: x\n",
        "        \"\"\"\n",
        "        y = y.float()\n",
        "        x = self.decoder(torch.cat([z, y], dim=1))\n",
        "        return x\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "binary cross entropy loss\n",
        "\"\"\"\n",
        "def binary_cross_entropy(r, x):\n",
        "    return -torch.sum(x * torch.log(r + 1e-8) + (1 - x) * torch.log(1 - r + 1e-8), dim=-1)\n",
        "\n",
        "  \n",
        "y_dim = 10\n",
        "z_dim = 32\n",
        "h_dim = [256, 128]\n",
        "\n",
        "model = DeepGenerativeModel([784, y_dim, z_dim, h_dim]).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=3e-4, betas=(0.9,0.999))\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "model"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Neurons [784, 256, 128]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DeepGenerativeModel(\n",
              "  (encoder): Encoder(\n",
              "    (hidden): ModuleList(\n",
              "      (0): Linear(in_features=784, out_features=256, bias=True)\n",
              "      (1): Linear(in_features=256, out_features=128, bias=True)\n",
              "    )\n",
              "    (sample): GaussianSample(\n",
              "      (mu): Linear(in_features=128, out_features=32, bias=True)\n",
              "      (log_var): Linear(in_features=128, out_features=32, bias=True)\n",
              "    )\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (hidden): ModuleList(\n",
              "      (0): Linear(in_features=32, out_features=128, bias=True)\n",
              "      (1): Linear(in_features=128, out_features=256, bias=True)\n",
              "    )\n",
              "    (reconstruction): Linear(in_features=256, out_features=784, bias=True)\n",
              "  )\n",
              "  (classifier): Classifier(\n",
              "    (fc1): Linear(in_features=784, out_features=256, bias=True)\n",
              "    (fc2): Linear(in_features=256, out_features=10, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 90
        }
      ]
    },
    {
      "metadata": {
        "id": "MFPRh2uISpea",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## Loss function\n",
        "def log_standard_categorical(p):\n",
        "  \"\"\"\n",
        "    Calculates the cross entropy between a (one-hot) categorical vector\n",
        "    and a standard (uniform) categorical distribution.\n",
        "    :param p: one-hot categorical distribution\n",
        "    :return: H(p, u)\n",
        "  \"\"\"\n",
        "  # Uniform prior over y\n",
        "  prior = F.softmax(torch.ones_like(p, dtype=torch.int64))\n",
        "  prior.requires_grad = False\n",
        "  cross_entropy = -torch.sum(p* torch.log(prior + 1e-8), dim=1)\n",
        "  \n",
        "  return cross_entropy\n",
        "\n",
        "\"\"\"\n",
        "LogSumExp an approximation for the sum in log-domain\n",
        "\"\"\"\n",
        "def log_sum_exp(tensor, dim=-1, sum_op = torch.sum):\n",
        "  max, _ = torch.max(tensor, dim = dim, keepdim = True)\n",
        "  return torch.log(sum_op(torch.exp(tensor - max), dim = dim, keepdim = True) + 1e-8) + max\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Importance Weighted Sampler - [Burda 2015] to be used in conjunction to get better estimate of\n",
        "Stochastic variational inference\n",
        "\"\"\"  \n",
        "class ImportanceWeightedSampler(object):\n",
        "  def __init__(self,mc=1,iw=1):\n",
        "    \"\"\"\n",
        "    sampler\n",
        "    :param mc: number of monte carlo samples\n",
        "    : param iw: number of importance weighted samples\n",
        "    \"\"\"\n",
        "    self.mc = mc\n",
        "    self.iw = iw\n",
        "    \n",
        "  def resample(self,x):\n",
        "    return x.repeat(self.mc * self.iw, 1)\n",
        "  \n",
        "  def __call__(self, elbo):\n",
        "    elbo = elbo.view(self.mc, self.iw, -1)\n",
        "    elbo = torch.mean(log_sum_exp(elbo, dim=1, sum_op=torch.mean), dim=0)\n",
        "    return elbo.view(-1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rJdaHuhhcSOI",
        "colab_type": "code",
        "outputId": "fc10030d-c6fb-44bd-adc6-821c474c7601",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Train\n",
        "\"\"\"\n",
        "from itertools import repeat\n",
        "from torch.autograd import Variable\n",
        "import math\n",
        "\n",
        "sampler = ImportanceWeightedSampler(mc=1, iw=1)\n",
        "beta = repeat(1)\n",
        "for epoch in range(10):\n",
        "  model.train()\n",
        "  train_loss = 0\n",
        "  accuracy = 0\n",
        "  for batch_idx, (data, labels) in enumerate(train_loader):\n",
        "    x = data.to(device)\n",
        "    y = labels.to(device)\n",
        "    \n",
        "    \n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    batch_size = data.size(0)\n",
        "    \n",
        "    ## increase sampling dimension\n",
        "    #xs = sampler.resample(x)\n",
        "    #ys = sampler.resample(y)\n",
        "    \n",
        "    reconstruction= model(x, y)\n",
        "    \n",
        "   \n",
        "    #p(x|y,z)\n",
        "    likelihood = -F.binary_cross_entropy(reconstruction, x.view(-1,784))\n",
        "    \n",
        "    \n",
        "    #p(y)\n",
        "    #prior = -log_standard_categorical(y)\n",
        "    \n",
        "    #Equivalent to -L(x,y)\n",
        "    elbo  = likelihood  - next(beta) *model.kl_divergence\n",
        "    L = sampler(elbo)\n",
        "    \n",
        "    loss = torch.mean(L)\n",
        "   \n",
        "    \n",
        "    logits = model.classify(x)\n",
        "    # Regular cross entropy\n",
        "    classification_loss = criterion(logits, y) \n",
        "    total_loss = loss +  classification_loss\n",
        "    total_loss.backward()\n",
        "    \n",
        "    train_loss += total_loss.item()\n",
        "    _, idx = logits.topk(1,dim=1)\n",
        "    accuracy+=  torch.sum(idx.view(batch_size) == y).item()\n",
        "    \n",
        "    optimizer.step()\n",
        "    \n",
        "    \n",
        "    \n",
        "  if epoch % 1 == 0:\n",
        "    m = len(train_loader)\n",
        "    print(\"Epoch: {}\".format(epoch))\n",
        "    print(\"[Train]\\t\\t J_a: {:.2f}, accuracy: {:.2f}\".format(train_loss / m, accuracy / m))\n",
        "\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0\n",
            "[Train]\t\t J_a: 23.80, accuracy: 14.38\n",
            "Epoch: 1\n",
            "[Train]\t\t J_a: 23.79, accuracy: 14.38\n",
            "Epoch: 2\n",
            "[Train]\t\t J_a: 23.79, accuracy: 14.38\n",
            "Epoch: 3\n",
            "[Train]\t\t J_a: 23.79, accuracy: 14.38\n",
            "Epoch: 4\n",
            "[Train]\t\t J_a: 23.82, accuracy: 14.38\n",
            "Epoch: 5\n",
            "[Train]\t\t J_a: 23.81, accuracy: 14.38\n",
            "Epoch: 6\n",
            "[Train]\t\t J_a: 23.78, accuracy: 14.38\n",
            "Epoch: 7\n",
            "[Train]\t\t J_a: 23.79, accuracy: 14.38\n",
            "Epoch: 8\n",
            "[Train]\t\t J_a: 23.80, accuracy: 14.38\n",
            "Epoch: 9\n",
            "[Train]\t\t J_a: 23.83, accuracy: 14.38\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qfQLtE5kdPYY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Test\n",
        "\"\"\"\n",
        "def test(epoch):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    test_accuracy = RunningAverage()\n",
        "    with torch.no_grad():\n",
        "        for i, (data,labels) in enumerate(test_loader):\n",
        "            data = data.to(device)\n",
        "            labels = labels.to(device)\n",
        "            recon_batch, mu, logvar = model(data)\n",
        "            batch_size = data.size(0)\n",
        "            test_loss += loss_function(recon_batch, data, mu, logvar).item()\n",
        "            ## accuracy\n",
        "            _, idx = recon_batch.topk(1,dim=1)\n",
        "            test_accuracy.update(torch.sum(idx.view(batch_size) == labels).item())\n",
        "        \n",
        "            \n",
        "            if i == 0:\n",
        "                n = min(data.size(0), 8)\n",
        "                comparison = torch.cat([data[:n],\n",
        "                                      recon_batch.view(params.get(\"batch_size\"), 1, 28, 28)[:n]])\n",
        "                save_image(comparison.cpu(),\n",
        "                         './results/reconstruction_' + str(epoch) + '.png', nrow=n)\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print('====> Test set loss: {:.4f}, test accuracy: {:.4f}'.format(test_loss,test_accuracy()))\n",
        "    return (test_loss,test_accuracy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mXS9GUuSdyy-",
        "colab_type": "code",
        "outputId": "06d9abc9-95e6-447a-858b-fbc5e3780091",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2244
        }
      },
      "cell_type": "code",
      "source": [
        "train_losses = []\n",
        "test_losses = []\n",
        "train_acc_list = []\n",
        "test_acc_list = []\n",
        "for epoch in range(1, 2):\n",
        "        trainloss,train_acc = train(epoch)\n",
        "        testloss,test_acc = test(epoch)\n",
        "        train_losses.append(trainloss)\n",
        "        test_losses.append(testloss)\n",
        "        train_acc_list.append(train_acc)\n",
        "        test_acc_list.append(test_acc)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            sample = torch.randn(64, 20).to(device)\n",
        "            sample = model.decode(sample).cpu()\n",
        "            save_image(sample.view(64, 1, 28, 28),\n",
        "                       'results/sample_' + str(epoch) + '.png')\n",
        "            \n",
        "            \n",
        "            \n",
        "            \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[407],\n",
            "        [408],\n",
            "        [408],\n",
            "        [407],\n",
            "        [408],\n",
            "        [434],\n",
            "        [407],\n",
            "        [407],\n",
            "        [408],\n",
            "        [407],\n",
            "        [407],\n",
            "        [407],\n",
            "        [407],\n",
            "        [407],\n",
            "        [408],\n",
            "        [434],\n",
            "        [407],\n",
            "        [407],\n",
            "        [408],\n",
            "        [407],\n",
            "        [408],\n",
            "        [407],\n",
            "        [407],\n",
            "        [407],\n",
            "        [407],\n",
            "        [407],\n",
            "        [408],\n",
            "        [407],\n",
            "        [407],\n",
            "        [408],\n",
            "        [408],\n",
            "        [407],\n",
            "        [408],\n",
            "        [407],\n",
            "        [408],\n",
            "        [434],\n",
            "        [407],\n",
            "        [407],\n",
            "        [408],\n",
            "        [408],\n",
            "        [407],\n",
            "        [407],\n",
            "        [407],\n",
            "        [407],\n",
            "        [407],\n",
            "        [407],\n",
            "        [408],\n",
            "        [407],\n",
            "        [407],\n",
            "        [407],\n",
            "        [407],\n",
            "        [408],\n",
            "        [407],\n",
            "        [408],\n",
            "        [407],\n",
            "        [407],\n",
            "        [408],\n",
            "        [407],\n",
            "        [407],\n",
            "        [407],\n",
            "        [407],\n",
            "        [408],\n",
            "        [407],\n",
            "        [407],\n",
            "        [407],\n",
            "        [407],\n",
            "        [407],\n",
            "        [407],\n",
            "        [408],\n",
            "        [407],\n",
            "        [407],\n",
            "        [407],\n",
            "        [408],\n",
            "        [407],\n",
            "        [407],\n",
            "        [407],\n",
            "        [407],\n",
            "        [408],\n",
            "        [407],\n",
            "        [407],\n",
            "        [407],\n",
            "        [407],\n",
            "        [408],\n",
            "        [407],\n",
            "        [408],\n",
            "        [407],\n",
            "        [407],\n",
            "        [434],\n",
            "        [408],\n",
            "        [407],\n",
            "        [407],\n",
            "        [407],\n",
            "        [408],\n",
            "        [408],\n",
            "        [407],\n",
            "        [408],\n",
            "        [407],\n",
            "        [407],\n",
            "        [407],\n",
            "        [408],\n",
            "        [407],\n",
            "        [408],\n",
            "        [407],\n",
            "        [407],\n",
            "        [408],\n",
            "        [407],\n",
            "        [407],\n",
            "        [407],\n",
            "        [407],\n",
            "        [407],\n",
            "        [407],\n",
            "        [408],\n",
            "        [407],\n",
            "        [407],\n",
            "        [408],\n",
            "        [408],\n",
            "        [408],\n",
            "        [407],\n",
            "        [407],\n",
            "        [407],\n",
            "        [407],\n",
            "        [407],\n",
            "        [407],\n",
            "        [408],\n",
            "        [408],\n",
            "        [407],\n",
            "        [407],\n",
            "        [407]], device='cuda:0')\n",
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 214.258728\n",
            "==> Epoch 1, average loss: 0.4571,average accuracy: 0.0000\n",
            "====> Test set loss: 216.8492, test accuracy: 0.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cp8R66yznWrq",
        "colab_type": "code",
        "outputId": "b73d075b-c870-484c-a28e-38f215895c9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "cell_type": "code",
      "source": [
        "F.softmax(torch.ones_like(y,dtype=torch.float))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078,\n",
              "        0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078,\n",
              "        0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078,\n",
              "        0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078,\n",
              "        0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078,\n",
              "        0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078,\n",
              "        0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078,\n",
              "        0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078,\n",
              "        0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078,\n",
              "        0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078,\n",
              "        0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078,\n",
              "        0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078,\n",
              "        0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078,\n",
              "        0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078, 0.0078,\n",
              "        0.0078, 0.0078], device='cuda:0')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 113
        }
      ]
    },
    {
      "metadata": {
        "id": "3x7BxjQGeGSu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.makedirs(\"./results\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iaqMNHkzestE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}